
epochs: 0 train loss: 0.9802783567514  train f1: 0.20952  seed: 0 
epochs: 0 val loss: 1.9855957031249996  val f1:0.27315  seed: 0
epochs: 1 train loss: 0.44033950122434684  train f1: 0.43546  seed: 0 
epochs: 1 val loss: 1.3213106043198526  val f1:0.45193  seed: 0
epochs: 2 train loss: 0.2852605563491139  train f1: 0.62745  seed: 0 
epochs: 2 val loss: 1.0079812442555145  val f1:0.57092  seed: 0
epochs: 3 train loss: 0.1850547221169543  train f1: 0.75179  seed: 0 
epochs: 3 val loss: 0.8799043543198531  val f1:0.64574  seed: 0
epochs: 4 train loss: 0.1199803138846782  train f1: 0.82943  seed: 0 
epochs: 4 val loss: 0.825967227711397  val f1:0.69786  seed: 0
epochs: 5 train loss: 0.0819056816955111  train f1: 0.88637  seed: 0 
epochs: 5 val loss: 0.9713062959558822  val f1:0.68948  seed: 0
epochs: 6 train loss: 0.06187769192368235  train f1: 0.92995  seed: 0 
epochs: 6 val loss: 0.768497242647059  val f1:0.71405  seed: 0
epochs: 7 train loss: 0.057894507450843916  train f1: 0.93405  seed: 0 
epochs: 7 val loss: 0.8754200654871325  val f1:0.72787  seed: 0
epochs: 8 train loss: 0.040672978358482235  train f1: 0.94703  seed: 0 
epochs: 8 val loss: 0.9687607709099263  val f1:0.68649  seed: 0
epochs: 9 train loss: 0.030052651220293183  train f1: 0.95590  seed: 0 
epochs: 9 val loss: 0.8562693876378676  val f1:0.71338  seed: 0
epochs: 10 train loss: 0.04141774284305858  train f1: 0.95505  seed: 0 
epochs: 10 val loss: 0.787427116842831  val f1:0.73774  seed: 0
epochs: 11 train loss: 0.04532747838034561  train f1: 0.95362  seed: 0 
epochs: 11 val loss: 0.9313174977022061  val f1:0.71124  seed: 0
epochs: 12 train loss: 0.0507356978174466  train f1: 0.95425  seed: 0 
epochs: 12 val loss: 0.9317373387953813  val f1:0.72026  seed: 0
epochs: 13 train loss: 0.027644641363798673  train f1: 0.96431  seed: 0 
epochs: 13 val loss: 0.9730655445772055  val f1:0.70223  seed: 0
epochs: 14 train loss: 0.024599365334012635  train f1: 0.96528  seed: 0 
epochs: 14 val loss: 0.898386786965763  val f1:0.74129  seed: 0
epochs: 0 train loss: 0.9402730002332094  train f1: 0.22107  seed: 1 
epochs: 0 val loss: 1.8665268841911762  val f1:0.26190  seed: 1
epochs: 1 train loss: 0.4303597976912314  train f1: 0.46980  seed: 1 
epochs: 1 val loss: 1.5306899126838234  val f1:0.43266  seed: 1
epochs: 2 train loss: 0.27337897001807365  train f1: 0.63362  seed: 1 
epochs: 2 val loss: 1.0714075425091916  val f1:0.59028  seed: 1
epochs: 3 train loss: 0.1758045082661643  train f1: 0.76388  seed: 1 
epochs: 3 val loss: 1.0180951286764703  val f1:0.63439  seed: 1
epochs: 4 train loss: 0.1271190358631647  train f1: 0.83573  seed: 1 
epochs: 4 val loss: 1.0400067497702208  val f1:0.61864  seed: 1
epochs: 5 train loss: 0.08299720821095934  train f1: 0.87765  seed: 1 
epochs: 5 val loss: 0.9712883444393384  val f1:0.66333  seed: 1
epochs: 6 train loss: 0.07973854577363429  train f1: 0.89026  seed: 1 
epochs: 6 val loss: 1.0175440171185666  val f1:0.68359  seed: 1
epochs: 7 train loss: 0.05796062412546644  train f1: 0.92980  seed: 1 
epochs: 7 val loss: 0.8194903205422795  val f1:0.71117  seed: 1
epochs: 8 train loss: 0.036807956980235534  train f1: 0.95752  seed: 1 
epochs: 8 val loss: 0.8049783145680147  val f1:0.73553  seed: 1
epochs: 9 train loss: 0.026176893889014393  train f1: 0.96703  seed: 1 
epochs: 9 val loss: 0.8829794491038601  val f1:0.73485  seed: 1
epochs: 10 train loss: 0.02788456874107247  train f1: 0.97648  seed: 1 
epochs: 10 val loss: 0.8825916963465074  val f1:0.74012  seed: 1
epochs: 11 train loss: 0.024385280573546  train f1: 0.98233  seed: 1 
epochs: 11 val loss: 1.0064051011029411  val f1:0.70196  seed: 1
epochs: 12 train loss: 0.040035827835993985  train f1: 0.96796  seed: 1 
epochs: 12 val loss: 0.9588515337775737  val f1:0.73332  seed: 1
epochs: 0 train loss: 4.5296816914498095  train f1: 0.00956  seed: 0 
epochs: 0 val loss: 4.492130055147058  val f1:0.00263  seed: 0
epochs: 0 train loss: 0.9773167852145521  train f1: 0.22675  seed: 0 
epochs: 0 val loss: 1.594152113970588  val f1:0.29277  seed: 0
epochs: 1 train loss: 0.4515681480293844  train f1: 0.43292  seed: 0 
epochs: 1 val loss: 1.3253461052389703  val f1:0.49174  seed: 0
epochs: 2 train loss: 0.2995042943242771  train f1: 0.59580  seed: 0 
epochs: 2 val loss: 1.1900527056525738  val f1:0.50395  seed: 0
epochs: 0 train loss: 0.9404861677938436  train f1: 0.22853  seed: 0 
epochs: 0 val loss: 1.8178280101102937  val f1:0.27252  seed: 0
epochs: 1 train loss: 0.42110340630830245  train f1: 0.49339  seed: 0 
epochs: 1 val loss: 1.4204460592830879  val f1:0.44375  seed: 0
epochs: 2 train loss: 0.25783140267898785  train f1: 0.65181  seed: 0 
epochs: 2 val loss: 1.0173662971047797  val f1:0.57949  seed: 0
epochs: 3 train loss: 0.16495878305008163  train f1: 0.79590  seed: 0 
epochs: 3 val loss: 0.9545683019301471  val f1:0.64201  seed: 0
epochs: 4 train loss: 0.12466123210850046  train f1: 0.84852  seed: 0 
epochs: 4 val loss: 0.8300386316636027  val f1:0.67549  seed: 0
epochs: 5 train loss: 0.07779944120947987  train f1: 0.88100  seed: 0 
epochs: 5 val loss: 0.8918816061580881  val f1:0.70382  seed: 0
epochs: 6 train loss: 0.057302289934300664  train f1: 0.92972  seed: 0 
epochs: 6 val loss: 0.915118049172794  val f1:0.70449  seed: 0
epochs: 7 train loss: 0.05404589069423391  train f1: 0.93343  seed: 0 
epochs: 7 val loss: 0.9369470932904412  val f1:0.70627  seed: 0
epochs: 8 train loss: 0.03927355381979871  train f1: 0.95072  seed: 0 
epochs: 8 val loss: 0.953390682444853  val f1:0.69703  seed: 0
epochs: 9 train loss: 0.04550382628369689  train f1: 0.95415  seed: 0 
epochs: 9 val loss: 0.9573777142693014  val f1:0.71307  seed: 0
epochs: 10 train loss: 0.036238293149578035  train f1: 0.95719  seed: 0 
epochs: 10 val loss: 0.9874465044806985  val f1:0.74675  seed: 0
epochs: 11 train loss: 0.03991094987783859  train f1: 0.95193  seed: 0 
epochs: 11 val loss: 1.1127606560202206  val f1:0.68263  seed: 0
epochs: 12 train loss: 0.027522939354626103  train f1: 0.96496  seed: 0 
epochs: 12 val loss: 1.057254566865809  val f1:0.72483  seed: 0
epochs: 13 train loss: 0.02169543504714965  train f1: 0.97382  seed: 0 
epochs: 13 val loss: 0.9465978285845585  val f1:0.72644  seed: 0
epochs: 14 train loss: 0.018100363105090698  train f1: 0.97213  seed: 0 
epochs: 14 val loss: 0.9289927763097429  val f1:0.72932  seed: 0
epochs: 15 train loss: 0.02361200816595733  train f1: 0.98440  seed: 0 
epochs: 15 val loss: 0.8843531889073991  val f1:0.74313  seed: 0
epochs: 16 train loss: 0.029103831568760664  train f1: 0.96374  seed: 0 
epochs: 16 val loss: 1.103368422564338  val f1:0.69532  seed: 0
epochs: 17 train loss: 0.03266676860069161  train f1: 0.96182  seed: 0 
epochs: 17 val loss: 0.9805746639476103  val f1:0.72264  seed: 0
epochs: 18 train loss: 0.019662373101533345  train f1: 0.97927  seed: 0 
epochs: 18 val loss: 0.9037834616268379  val f1:0.73678  seed: 0
epochs: 19 train loss: 0.019052054455031213  train f1: 0.98808  seed: 0 
epochs: 19 val loss: 0.9629067813648902  val f1:0.75570  seed: 0
epochs: 20 train loss: 0.030125251456872733  train f1: 0.97261  seed: 0 
epochs: 20 val loss: 1.1494427849264708  val f1:0.71698  seed: 0
epochs: 21 train loss: 0.03304036517641436  train f1: 0.97427  seed: 0 
epochs: 21 val loss: 0.9659387925091909  val f1:0.72344  seed: 0
epochs: 22 train loss: 0.009503774678529198  train f1: 0.99329  seed: 0 
epochs: 22 val loss: 1.042627671185662  val f1:0.75232  seed: 0
epochs: 23 train loss: 0.013463228051342185  train f1: 0.98974  seed: 0 
epochs: 23 val loss: 1.1391278435202206  val f1:0.71082  seed: 0
epochs: 24 train loss: 0.051350731458236896  train f1: 0.95615  seed: 0 
epochs: 24 val loss: 1.196335736443014  val f1:0.70255  seed: 0
epochs: 25 train loss: 0.032022104334475394  train f1: 0.97112  seed: 0 
epochs: 25 val loss: 0.9935841279871322  val f1:0.72777  seed: 0
epochs: 26 train loss: 0.035168198062412774  train f1: 0.96705  seed: 0 
epochs: 26 val loss: 1.1273534438189337  val f1:0.70336  seed: 0
epochs: 27 train loss: 0.027395772400187004  train f1: 0.98148  seed: 0 
epochs: 27 val loss: 1.0158498988432039  val f1:0.74069  seed: 0
epochs: 28 train loss: 0.014506972547787338  train f1: 0.98848  seed: 0 
epochs: 28 val loss: 1.0267253202550553  val f1:0.73379  seed: 0
epochs: 29 train loss: 0.01441534063709316  train f1: 0.98946  seed: 0 
epochs: 29 val loss: 0.9821920955882352  val f1:0.75303  seed: 0
epochs: 0 train loss: 0.9389083634561564  train f1: 0.22375  seed: 1 
epochs: 0 val loss: 1.7838852826286768  val f1:0.27631  seed: 1
epochs: 1 train loss: 0.4669207672574626  train f1: 0.42207  seed: 1 
epochs: 1 val loss: 1.5040139590992647  val f1:0.40034  seed: 1
epochs: 0 train loss: 0.9489381704757464  train f1: 0.22327  seed: 0 
epochs: 0 val loss: 1.8232996323529411  val f1:0.23689  seed: 0
epochs: 1 train loss: 0.45888485125641326  train f1: 0.43544  seed: 0 
epochs: 1 val loss: 1.4659244312959563  val f1:0.38394  seed: 0
epochs: 2 train loss: 0.30681735366138063  train f1: 0.57800  seed: 0 
epochs: 2 val loss: 1.1806856043198528  val f1:0.56742  seed: 0
epochs: 3 train loss: 0.1757539208255598  train f1: 0.77567  seed: 0 
epochs: 3 val loss: 1.2318240894990806  val f1:0.56313  seed: 0
epochs: 4 train loss: 0.12753613315411463  train f1: 0.80834  seed: 0 
epochs: 4 val loss: 1.016899557674633  val f1:0.63960  seed: 0
epochs: 5 train loss: 0.09589778843210706  train f1: 0.88199  seed: 0 
epochs: 5 val loss: 1.0065244786879597  val f1:0.66615  seed: 0
epochs: 6 train loss: 0.06765846707927647  train f1: 0.91697  seed: 0 
epochs: 6 val loss: 1.0319541482364427  val f1:0.63974  seed: 0
epochs: 7 train loss: 0.055299595220765094  train f1: 0.91837  seed: 0 
epochs: 7 val loss: 1.0884516098920036  val f1:0.65467  seed: 0
epochs: 8 train loss: 0.061519957300442366  train f1: 0.93424  seed: 0 
epochs: 8 val loss: 1.4092299517463232  val f1:0.63287  seed: 0
epochs: 9 train loss: 0.04511043206969307  train f1: 0.95479  seed: 0 
epochs: 9 val loss: 1.1415153952205885  val f1:0.69161  seed: 0
epochs: 10 train loss: 0.019611494754677387  train f1: 0.98530  seed: 0 
epochs: 10 val loss: 1.1817178165211397  val f1:0.69884  seed: 0
epochs: 11 train loss: 0.04510359977608297  train f1: 0.94258  seed: 0 
epochs: 11 val loss: 1.4841452205882353  val f1:0.64377  seed: 0
epochs: 12 train loss: 0.016945378104252604  train f1: 0.98546  seed: 0 
epochs: 12 val loss: 1.3842181037454044  val f1:0.66352  seed: 0
epochs: 13 train loss: 0.02715724350801155  train f1: 0.97786  seed: 0 
epochs: 13 val loss: 0.9773460836971509  val f1:0.71304  seed: 0
epochs: 14 train loss: 0.031068179144788148  train f1: 0.96045  seed: 0 
epochs: 14 val loss: 1.1468434053308822  val f1:0.67954  seed: 0
epochs: 15 train loss: 0.026164732762237103  train f1: 0.97133  seed: 0 
epochs: 15 val loss: 1.1754437614889706  val f1:0.68853  seed: 0
epochs: 16 train loss: 0.03146084475873121  train f1: 0.97161  seed: 0 
epochs: 16 val loss: 1.2768123851102942  val f1:0.68541  seed: 0
epochs: 17 train loss: 0.03765961305419012  train f1: 0.96258  seed: 0 
epochs: 17 val loss: 1.1283246208639703  val f1:0.70800  seed: 0
epochs: 18 train loss: 0.024244418784753596  train f1: 0.97765  seed: 0 
epochs: 18 val loss: 1.5821497300091916  val f1:0.68814  seed: 0
epochs: 19 train loss: 0.019650576719597204  train f1: 0.98037  seed: 0 
epochs: 19 val loss: 1.1725912655101103  val f1:0.71604  seed: 0
epochs: 20 train loss: 0.033756492742851606  train f1: 0.97356  seed: 0 
epochs: 20 val loss: 1.1930398380055145  val f1:0.71483  seed: 0
epochs: 21 train loss: 0.03028058738850837  train f1: 0.96379  seed: 0 
epochs: 21 val loss: 1.2151273839613974  val f1:0.70943  seed: 0
epochs: 22 train loss: 0.01759429298230071  train f1: 0.97422  seed: 0 
epochs: 22 val loss: 1.1698716107536766  val f1:0.71241  seed: 0
epochs: 23 train loss: 0.02587211043087404  train f1: 0.97678  seed: 0 
epochs: 23 val loss: 0.9449373133042284  val f1:0.75646  seed: 0
epochs: 24 train loss: 0.02030856662721777  train f1: 0.98135  seed: 0 
epochs: 24 val loss: 1.6281924528234133  val f1:0.70243  seed: 0
epochs: 25 train loss: 0.053824008400760445  train f1: 0.95183  seed: 0 
epochs: 25 val loss: 1.153910328360165  val f1:0.70365  seed: 0
epochs: 26 train loss: 0.0433845947037882  train f1: 0.95744  seed: 0 
epochs: 26 val loss: 1.1263643152573537  val f1:0.72311  seed: 0
epochs: 27 train loss: 0.01504977336570398  train f1: 0.98108  seed: 0 
epochs: 27 val loss: 1.045691995059743  val f1:0.72171  seed: 0
epochs: 28 train loss: 0.025467015024441392  train f1: 0.98401  seed: 0 
epochs: 28 val loss: 1.0446633731617647  val f1:0.72306  seed: 0
epochs: 29 train loss: 0.013987266305667245  train f1: 0.98384  seed: 0 
epochs: 29 val loss: 0.9832135368795956  val f1:0.75389  seed: 0
epochs: 0 train loss: 0.9388773903917902  train f1: 0.20303  seed: 0 
epochs: 0 val loss: 2.0087603400735294  val f1:0.26325  seed: 0
epochs: 1 train loss: 0.453935310022155  train f1: 0.44147  seed: 0 
epochs: 1 val loss: 1.526044060202206  val f1:0.43416  seed: 0
epochs: 2 train loss: 0.3072139683054452  train f1: 0.58126  seed: 0 
epochs: 2 val loss: 1.2055592256433825  val f1:0.50162  seed: 0
epochs: 3 train loss: 0.18426362792057785  train f1: 0.74491  seed: 0 
epochs: 3 val loss: 1.0238898782169115  val f1:0.59270  seed: 0
epochs: 4 train loss: 0.1444344022380772  train f1: 0.80474  seed: 0 
epochs: 4 val loss: 0.9215123793658089  val f1:0.64747  seed: 0
epochs: 5 train loss: 0.10102922524978866  train f1: 0.85961  seed: 0 
epochs: 5 val loss: 0.8774270450367646  val f1:0.67214  seed: 0
epochs: 6 train loss: 0.07105490698743225  train f1: 0.90123  seed: 0 
epochs: 6 val loss: 0.8783300063189339  val f1:0.70238  seed: 0
epochs: 7 train loss: 0.05325826958044252  train f1: 0.93086  seed: 0 
epochs: 7 val loss: 0.8304802389705882  val f1:0.70403  seed: 0
epochs: 8 train loss: 0.05614951119494083  train f1: 0.92843  seed: 0 
epochs: 8 val loss: 0.7867000804227945  val f1:0.74217  seed: 0
epochs: 9 train loss: 0.033601301819530874  train f1: 0.96577  seed: 0 
epochs: 9 val loss: 0.9083350686465993  val f1:0.72801  seed: 0
epochs: 10 train loss: 0.03584451817754489  train f1: 0.95143  seed: 0 
epochs: 10 val loss: 0.8259708180147061  val f1:0.74238  seed: 0
epochs: 11 train loss: 0.016309213282457036  train f1: 0.98408  seed: 0 
epochs: 11 val loss: 0.8936013614430146  val f1:0.73034  seed: 0
epochs: 12 train loss: 0.02559481449981234  train f1: 0.97744  seed: 0 
epochs: 12 val loss: 1.098859001608456  val f1:0.70662  seed: 0
epochs: 13 train loss: 0.02721726360605723  train f1: 0.97594  seed: 0 
epochs: 13 val loss: 0.933401668772978  val f1:0.73589  seed: 0
epochs: 14 train loss: 0.04185248132961898  train f1: 0.94556  seed: 0 
epochs: 14 val loss: 1.0924287683823528  val f1:0.68081  seed: 0
epochs: 15 train loss: 0.039983792091483485  train f1: 0.95624  seed: 0 
epochs: 15 val loss: 0.9478391759535846  val f1:0.69642  seed: 0
epochs: 16 train loss: 0.03380824202921854  train f1: 0.96894  seed: 0 
epochs: 16 val loss: 0.9714822208180147  val f1:0.72353  seed: 0
epochs: 17 train loss: 0.05066917191690472  train f1: 0.94973  seed: 0 
epochs: 17 val loss: 1.066131591796875  val f1:0.68675  seed: 0
epochs: 18 train loss: 0.035866182241866836  train f1: 0.96582  seed: 0 
epochs: 18 val loss: 0.9559900620404411  val f1:0.70034  seed: 0
epochs: 19 train loss: 0.013159428959462182  train f1: 0.98619  seed: 0 
epochs: 19 val loss: 0.881975959329044  val f1:0.73732  seed: 0
epochs: 20 train loss: 0.023969125391832043  train f1: 0.97539  seed: 0 
epochs: 20 val loss: 0.8954503676470585  val f1:0.72175  seed: 0
epochs: 21 train loss: 0.02012851700853946  train f1: 0.98016  seed: 0 
epochs: 21 val loss: 0.9824793198529411  val f1:0.71580  seed: 0
epochs: 22 train loss: 0.022553473266203015  train f1: 0.97689  seed: 0 
epochs: 22 val loss: 1.0227122587316175  val f1:0.71934  seed: 0
epochs: 23 train loss: 0.024399982459509563  train f1: 0.97288  seed: 0 
epochs: 23 val loss: 1.105712890625  val f1:0.69236  seed: 0
epochs: 24 train loss: 0.011107252604925816  train f1: 0.98765  seed: 0 
epochs: 24 val loss: 0.8855608771829047  val f1:0.75539  seed: 0
epochs: 25 train loss: 0.014363450345708361  train f1: 0.98688  seed: 0 
epochs: 25 val loss: 0.9314682904411765  val f1:0.75255  seed: 0
epochs: 26 train loss: 0.023674263882992878  train f1: 0.97729  seed: 0 
epochs: 26 val loss: 1.2414586684283087  val f1:0.71482  seed: 0
epochs: 27 train loss: 0.00859058659468124  train f1: 0.99301  seed: 0 
epochs: 27 val loss: 1.1822078929227937  val f1:0.72999  seed: 0
epochs: 28 train loss: 0.012652034634974463  train f1: 0.98868  seed: 0 
epochs: 28 val loss: 0.8813440659466913  val f1:0.75443  seed: 0
epochs: 29 train loss: 0.04080320650072241  train f1: 0.94728  seed: 0 
epochs: 29 val loss: 0.9281221277573529  val f1:0.72309  seed: 0
epochs: 0 train loss: 0.9513886864505593  train f1: 0.21717  seed: 1 
epochs: 0 val loss: 1.9837574678308825  val f1:0.26614  seed: 1
epochs: 1 train loss: 0.460811330311334  train f1: 0.40503  seed: 1 
epochs: 1 val loss: 1.3381419462316173  val f1:0.41764  seed: 1
epochs: 2 train loss: 0.28363242078183315  train f1: 0.61003  seed: 1 
epochs: 2 val loss: 1.1999942555147058  val f1:0.53719  seed: 1
epochs: 3 train loss: 0.2063596355381296  train f1: 0.68909  seed: 1 
epochs: 3 val loss: 0.8787626378676469  val f1:0.64363  seed: 1
epochs: 4 train loss: 0.1228776618615905  train f1: 0.83569  seed: 1 
epochs: 4 val loss: 0.8842091279871326  val f1:0.64099  seed: 1
epochs: 5 train loss: 0.08627379118506591  train f1: 0.85879  seed: 1 
epochs: 5 val loss: 0.9077363855698533  val f1:0.66540  seed: 1
epochs: 6 train loss: 0.09214324381814075  train f1: 0.88379  seed: 1 
epochs: 6 val loss: 1.1119384765624996  val f1:0.63146  seed: 1
epochs: 7 train loss: 0.05417546229575997  train f1: 0.91207  seed: 1 
epochs: 7 val loss: 0.8367094152113969  val f1:0.69907  seed: 1
epochs: 8 train loss: 0.050758867121454485  train f1: 0.93457  seed: 1 
epochs: 8 val loss: 0.8512887393727021  val f1:0.70018  seed: 1
epochs: 9 train loss: 0.04083710997851925  train f1: 0.94314  seed: 1 
epochs: 9 val loss: 0.9056199017693015  val f1:0.72911  seed: 1
epochs: 10 train loss: 0.022555290763057873  train f1: 0.96924  seed: 1 
epochs: 10 val loss: 1.0164615406709558  val f1:0.68992  seed: 1
epochs: 11 train loss: 0.024903115941517392  train f1: 0.97463  seed: 1 
epochs: 11 val loss: 0.9626321231617643  val f1:0.71737  seed: 1
epochs: 12 train loss: 0.017484647124560906  train f1: 0.97904  seed: 1 
epochs: 12 val loss: 0.8817336138556986  val f1:0.73226  seed: 1
epochs: 13 train loss: 0.042695568568670926  train f1: 0.94990  seed: 1 
epochs: 13 val loss: 1.0958718692555147  val f1:0.70295  seed: 1
epochs: 14 train loss: 0.04258936554638308  train f1: 0.96427  seed: 1 
epochs: 14 val loss: 1.0058333453010109  val f1:0.70791  seed: 1
epochs: 15 train loss: 0.04838411131901528  train f1: 0.94854  seed: 1 
epochs: 15 val loss: 1.064804974724265  val f1:0.70904  seed: 1
epochs: 16 train loss: 0.020812191180328826  train f1: 0.98079  seed: 1 
epochs: 16 val loss: 1.0962811638327201  val f1:0.71818  seed: 1
epochs: 17 train loss: 0.021659966725022045  train f1: 0.96537  seed: 1 
epochs: 17 val loss: 1.0472537769990806  val f1:0.72918  seed: 1
epochs: 18 train loss: 0.021271755446248982  train f1: 0.96936  seed: 1 
epochs: 18 val loss: 1.0999661613913136  val f1:0.72270  seed: 1
epochs: 19 train loss: 0.028694028285012316  train f1: 0.97047  seed: 1 
epochs: 19 val loss: 0.8873237161075366  val f1:0.72349  seed: 1
epochs: 20 train loss: 0.019629447317835117  train f1: 0.97362  seed: 1 
epochs: 20 val loss: 1.147940242991728  val f1:0.69848  seed: 1
epochs: 21 train loss: 0.021650514495906544  train f1: 0.97509  seed: 1 
epochs: 21 val loss: 1.054795209099265  val f1:0.73397  seed: 1
epochs: 22 train loss: 0.019677333867371968  train f1: 0.98534  seed: 1 
epochs: 22 val loss: 0.9781188964843751  val f1:0.74134  seed: 1
epochs: 23 train loss: 0.028472128199107605  train f1: 0.97931  seed: 1 
epochs: 23 val loss: 1.0830114028033087  val f1:0.71620  seed: 1
epochs: 24 train loss: 0.027995689591365086  train f1: 0.97662  seed: 1 
epochs: 24 val loss: 1.0307294060202203  val f1:0.73816  seed: 1
epochs: 25 train loss: 0.016998953783690034  train f1: 0.98459  seed: 1 
epochs: 25 val loss: 1.107646493350758  val f1:0.74851  seed: 1
epochs: 26 train loss: 0.022307417285976135  train f1: 0.98581  seed: 1 
epochs: 26 val loss: 1.0137629789464617  val f1:0.73880  seed: 1
epochs: 27 train loss: 0.014417654987591417  train f1: 0.98944  seed: 1 
epochs: 27 val loss: 1.1075977998621325  val f1:0.74969  seed: 1
epochs: 28 train loss: 0.0073822963593611094  train f1: 0.99194  seed: 1 
epochs: 28 val loss: 1.2373298196231617  val f1:0.73027  seed: 1
epochs: 29 train loss: 0.01331587335956631  train f1: 0.98829  seed: 1 
epochs: 29 val loss: 1.1522755342371325  val f1:0.72775  seed: 1
epochs: 0 train loss: 0.9429931640624999  train f1: 0.22010  seed: 2 
epochs: 0 val loss: 1.8782743566176465  val f1:0.30419  seed: 2
epochs: 1 train loss: 0.4350768131996267  train f1: 0.46752  seed: 2 
epochs: 1 val loss: 1.398918600643382  val f1:0.41972  seed: 2
epochs: 2 train loss: 0.28423981168376855  train f1: 0.58814  seed: 2 
epochs: 2 val loss: 0.9348826688878675  val f1:0.55674  seed: 2
epochs: 3 train loss: 0.18457748640829053  train f1: 0.74163  seed: 2 
epochs: 3 val loss: 0.7557714125689339  val f1:0.64951  seed: 2
epochs: 4 train loss: 0.13575383086702716  train f1: 0.80789  seed: 2 
epochs: 4 val loss: 0.7421372357536763  val f1:0.67454  seed: 2
epochs: 5 train loss: 0.094986275060853  train f1: 0.86389  seed: 2 
epochs: 5 val loss: 0.8013987821691174  val f1:0.70262  seed: 2
epochs: 6 train loss: 0.06814842081781644  train f1: 0.92470  seed: 2 
epochs: 6 val loss: 0.9148059171788833  val f1:0.69538  seed: 2
epochs: 7 train loss: 0.04788229358730032  train f1: 0.93886  seed: 2 
epochs: 7 val loss: 0.9415157542509192  val f1:0.68431  seed: 2
epochs: 8 train loss: 0.057936931723978964  train f1: 0.92863  seed: 2 
epochs: 8 val loss: 0.8782245411592372  val f1:0.71605  seed: 2
epochs: 9 train loss: 0.04799816501674367  train f1: 0.95444  seed: 2 
epochs: 9 val loss: 0.8446134679457722  val f1:0.73017  seed: 2
epochs: 10 train loss: 0.04459984622784518  train f1: 0.95057  seed: 2 
epochs: 10 val loss: 1.003369499655331  val f1:0.70067  seed: 2
epochs: 11 train loss: 0.023950334805161212  train f1: 0.97298  seed: 2 
epochs: 11 val loss: 0.8980317957261029  val f1:0.73498  seed: 2
epochs: 12 train loss: 0.04046540829672742  train f1: 0.96257  seed: 2 
epochs: 12 val loss: 0.9783002068014708  val f1:0.70687  seed: 2
epochs: 13 train loss: 0.037491446110739644  train f1: 0.96455  seed: 2 
epochs: 13 val loss: 0.9377202426686007  val f1:0.71971  seed: 2
epochs: 14 train loss: 0.021911815031250923  train f1: 0.97648  seed: 2 
epochs: 14 val loss: 0.8487378288717831  val f1:0.74468  seed: 2
epochs: 15 train loss: 0.007647978725718034  train f1: 0.99047  seed: 2 
epochs: 15 val loss: 0.9327288234935088  val f1:0.73950  seed: 2
epochs: 16 train loss: 0.021624225289074348  train f1: 0.98413  seed: 2 
epochs: 16 val loss: 1.2632948931525734  val f1:0.68052  seed: 2
epochs: 17 train loss: 0.03814760606680344  train f1: 0.96388  seed: 2 
epochs: 17 val loss: 1.0269741731531479  val f1:0.72390  seed: 2
epochs: 18 train loss: 0.05124948095919479  train f1: 0.94710  seed: 2 
epochs: 18 val loss: 0.9935863719266999  val f1:0.70530  seed: 2
epochs: 19 train loss: 0.018028406509712564  train f1: 0.98770  seed: 2 
epochs: 19 val loss: 0.833142224480124  val f1:0.74018  seed: 2
epochs: 20 train loss: 0.00651468803633505  train f1: 0.99313  seed: 2 
epochs: 20 val loss: 0.7834982591516831  val f1:0.77021  seed: 2
epochs: 21 train loss: 0.013555003191108132  train f1: 0.99034  seed: 2 
epochs: 21 val loss: 0.9547379437614889  val f1:0.74583  seed: 2
epochs: 22 train loss: 0.013226238649282886  train f1: 0.98311  seed: 2 
epochs: 22 val loss: 1.1800133200252756  val f1:0.72559  seed: 2
epochs: 23 train loss: 0.011151347587357707  train f1: 0.97688  seed: 2 
epochs: 23 val loss: 0.9949251062729779  val f1:0.74603  seed: 2
epochs: 24 train loss: 0.010296425267831606  train f1: 0.99472  seed: 2 
epochs: 24 val loss: 1.0231290705063765  val f1:0.74275  seed: 2
epochs: 25 train loss: 0.017552019944831507  train f1: 0.97410  seed: 2 
epochs: 25 val loss: 1.0479682473575371  val f1:0.71913  seed: 2
epochs: 26 train loss: 0.02651102952103118  train f1: 0.97684  seed: 2 
epochs: 26 val loss: 1.2789701573988967  val f1:0.73347  seed: 2
epochs: 27 train loss: 0.06874290153161805  train f1: 0.93675  seed: 2 
epochs: 27 val loss: 1.205702837775735  val f1:0.68381  seed: 2
epochs: 28 train loss: 0.045288477370988095  train f1: 0.94724  seed: 2 
epochs: 28 val loss: 1.4686036951401653  val f1:0.68559  seed: 2
epochs: 29 train loss: 0.023943117305414003  train f1: 0.97345  seed: 2 
epochs: 29 val loss: 1.74856387867647  val f1:0.68441  seed: 2
epochs: 0 train loss: 0.9476846723414178  train f1: 0.21463  seed: 3 
epochs: 0 val loss: 1.7832749310661762  val f1:0.27556  seed: 3
epochs: 1 train loss: 0.4224065524428639  train f1: 0.47014  seed: 3 
epochs: 1 val loss: 1.2983111213235294  val f1:0.45047  seed: 3
epochs: 2 train loss: 0.26958568060576016  train f1: 0.63419  seed: 3 
epochs: 2 val loss: 1.015768612132353  val f1:0.56348  seed: 3
epochs: 3 train loss: 0.17825317382812506  train f1: 0.74374  seed: 3 
epochs: 3 val loss: 0.7677414838005516  val f1:0.66305  seed: 3
epochs: 4 train loss: 0.11370991948825208  train f1: 0.84357  seed: 3 
epochs: 4 val loss: 0.8116347369025737  val f1:0.64954  seed: 3
epochs: 5 train loss: 0.07972370688594992  train f1: 0.90106  seed: 3 
epochs: 5 val loss: 0.7959666532628676  val f1:0.72621  seed: 3
epochs: 6 train loss: 0.053474226994300995  train f1: 0.92973  seed: 3 
epochs: 6 val loss: 0.7181549072265627  val f1:0.74718  seed: 3
epochs: 7 train loss: 0.03721080609221955  train f1: 0.95537  seed: 3 
epochs: 7 val loss: 0.7628137925091911  val f1:0.72790  seed: 3
epochs: 8 train loss: 0.05324320294963778  train f1: 0.93107  seed: 3 
epochs: 8 val loss: 1.0123578239889708  val f1:0.67424  seed: 3
epochs: 9 train loss: 0.04582963772674104  train f1: 0.94644  seed: 3 
epochs: 9 val loss: 0.8498535156250002  val f1:0.72291  seed: 3
epochs: 0 train loss: 0.9230030682070037  train f1: 0.24152  seed: 0 
epochs: 0 val loss: 1.7441119025735294  val f1:0.28119  seed: 0
epochs: 1 train loss: 0.41701188324191035  train f1: 0.46475  seed: 0 
epochs: 1 val loss: 1.2209400850183822  val f1:0.45415  seed: 0
epochs: 2 train loss: 0.25796270708665775  train f1: 0.64732  seed: 0 
epochs: 2 val loss: 1.0938397575827208  val f1:0.55908  seed: 0
epochs: 3 train loss: 0.16630067216589103  train f1: 0.75935  seed: 0 
epochs: 3 val loss: 0.9336763269761025  val f1:0.66458  seed: 0
epochs: 4 train loss: 0.11119815474706336  train f1: 0.84593  seed: 0 
epochs: 4 val loss: 1.0430477366727942  val f1:0.65766  seed: 0
epochs: 5 train loss: 0.07643476445624171  train f1: 0.89850  seed: 0 
epochs: 5 val loss: 1.0330702837775732  val f1:0.66943  seed: 0
epochs: 6 train loss: 0.05511984588406612  train f1: 0.93717  seed: 0 
epochs: 6 val loss: 0.8930664062500001  val f1:0.68770  seed: 0
epochs: 7 train loss: 0.05331163879827405  train f1: 0.94054  seed: 0 
epochs: 7 val loss: 0.8710829790900736  val f1:0.71001  seed: 0
epochs: 8 train loss: 0.06838615566280715  train f1: 0.92673  seed: 0 
epochs: 8 val loss: 1.0531652113970593  val f1:0.69984  seed: 0
epochs: 9 train loss: 0.049308411618496494  train f1: 0.94531  seed: 0 
epochs: 9 val loss: 0.9023922190946689  val f1:0.70810  seed: 0
epochs: 10 train loss: 0.036470656699322646  train f1: 0.96260  seed: 0 
epochs: 10 val loss: 1.0474745806525734  val f1:0.69504  seed: 0
epochs: 11 train loss: 0.04629490223336727  train f1: 0.95830  seed: 0 
epochs: 11 val loss: 1.0762867647058825  val f1:0.69863  seed: 0
epochs: 12 train loss: 0.030221036139954922  train f1: 0.96899  seed: 0 
epochs: 12 val loss: 0.8494639677159925  val f1:0.72921  seed: 0
epochs: 13 train loss: 0.02815963359589273  train f1: 0.97610  seed: 0 
epochs: 13 val loss: 0.8354321648092831  val f1:0.73455  seed: 0
epochs: 14 train loss: 0.029788588801174295  train f1: 0.97171  seed: 0 
epochs: 14 val loss: 0.9067113539751839  val f1:0.73293  seed: 0
epochs: 15 train loss: 0.010796166480855745  train f1: 0.98941  seed: 0 
epochs: 15 val loss: 0.9214764763327203  val f1:0.75266  seed: 0
epochs: 16 train loss: 0.01704687852386042  train f1: 0.98049  seed: 0 
epochs: 16 val loss: 0.9661506204044119  val f1:0.74053  seed: 0
epochs: 17 train loss: 0.03453174973210546  train f1: 0.97034  seed: 0 
epochs: 17 val loss: 0.9773523667279411  val f1:0.73396  seed: 0
epochs: 18 train loss: 0.03441850006157624  train f1: 0.96841  seed: 0 
epochs: 18 val loss: 1.1731962316176467  val f1:0.70807  seed: 0
epochs: 19 train loss: 0.024094855531733083  train f1: 0.97699  seed: 0 
epochs: 19 val loss: 0.935085521024816  val f1:0.72912  seed: 0
epochs: 20 train loss: 0.02119604154681484  train f1: 0.98145  seed: 0 
epochs: 20 val loss: 1.0551389806410845  val f1:0.72863  seed: 0
#################################################################################
2022.04.10 ( batchsize, lr 최적점 알아내깅)
#################################################################################epochs: 0 train loss: 1.2753839314207154  train f1: 0.17268  lr: 0.001 batch: 4 
epochs: 0 val loss: 3.6002786438186445  val f1:0.12860  lr: 0.001 batch: 4
epochs: 1 train loss: 0.7900191299924242  train f1: 0.22776  lr: 0.001 batch: 4 
epochs: 1 val loss: 2.975511455358956  val f1:0.16912  lr: 0.001 batch: 4
epochs: 2 train loss: 0.6370017448168127  train f1: 0.27057  lr: 0.001 batch: 4 
epochs: 2 val loss: 3.1055238932536966  val f1:0.19143  lr: 0.001 batch: 4
epochs: 3 train loss: 0.5941163538100578  train f1: 0.31251  lr: 0.001 batch: 4 
epochs: 3 val loss: 3.198993909336864  val f1:0.20831  lr: 0.001 batch: 4
epochs: 4 train loss: 0.5655832897857779  train f1: 0.33679  lr: 0.001 batch: 4 
epochs: 4 val loss: 2.9669289951642956  val f1:0.21372  lr: 0.001 batch: 4
epochs: 5 train loss: 0.5036334848582514  train f1: 0.38492  lr: 0.001 batch: 4 
epochs: 5 val loss: 3.2922414238245015  val f1:0.25776  lr: 0.001 batch: 4
epochs: 6 train loss: 0.4893907286254653  train f1: 0.41636  lr: 0.001 batch: 4 
epochs: 6 val loss: 2.996524832023096  val f1:0.23667  lr: 0.001 batch: 4
epochs: 7 train loss: 0.44196600949719617  train f1: 0.48877  lr: 0.001 batch: 4 
epochs: 7 val loss: 2.898429771522425  val f1:0.26762  lr: 0.001 batch: 4
epochs: 8 train loss: 0.4198735456788135  train f1: 0.48734  lr: 0.001 batch: 4 
epochs: 8 val loss: 3.3466601256758053  val f1:0.21928  lr: 0.001 batch: 4
epochs: 9 train loss: 0.38099798682923647  train f1: 0.52648  lr: 0.001 batch: 4 
epochs: 9 val loss: 3.5477059599641114  val f1:0.25150  lr: 0.001 batch: 4
epochs: 10 train loss: 0.4033744402146065  train f1: 0.50348  lr: 0.001 batch: 4 
epochs: 10 val loss: 3.1671262610157713  val f1:0.30688  lr: 0.001 batch: 4
epochs: 11 train loss: 0.3515113250593121  train f1: 0.57100  lr: 0.001 batch: 4 
epochs: 11 val loss: 3.732479921740816  val f1:0.30112  lr: 0.001 batch: 4
epochs: 12 train loss: 0.3310982189821392  train f1: 0.60142  lr: 0.001 batch: 4 
epochs: 12 val loss: 3.9311760387526826  val f1:0.29735  lr: 0.001 batch: 4
epochs: 13 train loss: 0.3245849397298551  train f1: 0.60367  lr: 0.001 batch: 4 
epochs: 13 val loss: 4.045230766395469  val f1:0.31143  lr: 0.001 batch: 4
epochs: 14 train loss: 0.31406738516989746  train f1: 0.61712  lr: 0.001 batch: 4 
epochs: 14 val loss: 3.5169527942041743  val f1:0.31429  lr: 0.001 batch: 4
epochs: 15 train loss: 0.2901662804660727  train f1: 0.63067  lr: 0.001 batch: 4 
epochs: 15 val loss: 3.5551290352844354  val f1:0.36125  lr: 0.001 batch: 4
epochs: 16 train loss: 0.2802798051512643  train f1: 0.66167  lr: 0.001 batch: 4 
epochs: 16 val loss: 5.999109979463199  val f1:0.29282  lr: 0.001 batch: 4
epochs: 17 train loss: 0.26992801936824656  train f1: 0.65379  lr: 0.001 batch: 4 
epochs: 17 val loss: 4.933155806475984  val f1:0.33366  lr: 0.001 batch: 4
epochs: 18 train loss: 0.2546404620234885  train f1: 0.68266  lr: 0.001 batch: 4 
epochs: 18 val loss: 4.328745005082113  val f1:0.30425  lr: 0.001 batch: 4
epochs: 19 train loss: 0.23634855175732675  train f1: 0.72249  lr: 0.001 batch: 4 
epochs: 19 val loss: 10.827500946667723  val f1:0.29237  lr: 0.001 batch: 4
epochs: 20 train loss: 0.2200930310545787  train f1: 0.72638  lr: 0.001 batch: 4 
epochs: 20 val loss: 3.5887095569900747  val f1:0.40349  lr: 0.001 batch: 4
epochs: 21 train loss: 0.2123825305633331  train f1: 0.74756  lr: 0.001 batch: 4 
epochs: 21 val loss: 4.717943616169946  val f1:0.34132  lr: 0.001 batch: 4
epochs: 0 train loss: 1.0604185611567698  train f1: 0.20387  lr: 0.0003 batch: 4 
epochs: 0 val loss: 2.7635902588796513  val f1:0.20194  lr: 0.0003 batch: 4
epochs: 1 train loss: 0.5649541486961561  train f1: 0.34662  lr: 0.0003 batch: 4 
epochs: 1 val loss: 1.971888837655089  val f1:0.33590  lr: 0.0003 batch: 4
epochs: 2 train loss: 0.4297915371169759  train f1: 0.48657  lr: 0.0003 batch: 4 
epochs: 2 val loss: 2.1904208868023183  val f1:0.37025  lr: 0.0003 batch: 4
epochs: 3 train loss: 0.35405434651321194  train f1: 0.55900  lr: 0.0003 batch: 4 
epochs: 3 val loss: 2.387780522150098  val f1:0.43646  lr: 0.0003 batch: 4
epochs: 4 train loss: 0.2662331367030123  train f1: 0.64249  lr: 0.0003 batch: 4 
epochs: 4 val loss: 2.5243067201745313  val f1:0.46195  lr: 0.0003 batch: 4
epochs: 5 train loss: 0.23392173748337797  train f1: 0.68777  lr: 0.0003 batch: 4 
epochs: 5 val loss: 2.836071442584601  val f1:0.41967  lr: 0.0003 batch: 4
epochs: 6 train loss: 0.18933991775307346  train f1: 0.75963  lr: 0.0003 batch: 4 
epochs: 6 val loss: 2.8686183317251657  val f1:0.41990  lr: 0.0003 batch: 4
epochs: 7 train loss: 0.1712859353322659  train f1: 0.77314  lr: 0.0003 batch: 4 
epochs: 7 val loss: 2.6414962538098163  val f1:0.49381  lr: 0.0003 batch: 4
epochs: 8 train loss: 0.14811890714623965  train f1: 0.81907  lr: 0.0003 batch: 4 
epochs: 8 val loss: 3.419831700139231  val f1:0.52945  lr: 0.0003 batch: 4
epochs: 9 train loss: 0.12427187373352393  train f1: 0.83125  lr: 0.0003 batch: 4 
epochs: 9 val loss: 2.8514214221090888  val f1:0.54122  lr: 0.0003 batch: 4
epochs: 10 train loss: 0.14949092570315578  train f1: 0.82694  lr: 0.0003 batch: 4 
epochs: 10 val loss: 3.0334480414806353  val f1:0.50365  lr: 0.0003 batch: 4
epochs: 11 train loss: 0.10744319967786033  train f1: 0.86521  lr: 0.0003 batch: 4 
epochs: 11 val loss: 3.322945438864497  val f1:0.51757  lr: 0.0003 batch: 4
epochs: 12 train loss: 0.10369503051600648  train f1: 0.88414  lr: 0.0003 batch: 4 
epochs: 12 val loss: 2.9013303041679284  val f1:0.51845  lr: 0.0003 batch: 4
epochs: 13 train loss: 0.08527778608075694  train f1: 0.89521  lr: 0.0003 batch: 4 
epochs: 13 val loss: 3.804989811883124  val f1:0.52595  lr: 0.0003 batch: 4
epochs: 14 train loss: 0.07698205537563856  train f1: 0.90624  lr: 0.0003 batch: 4 
epochs: 14 val loss: 3.670355073817368  val f1:0.52521  lr: 0.0003 batch: 4
epochs: 15 train loss: 0.08758256497901026  train f1: 0.88948  lr: 0.0003 batch: 4 
epochs: 15 val loss: 3.5479490803433675  val f1:0.55317  lr: 0.0003 batch: 4
epochs: 16 train loss: 0.09044173516137757  train f1: 0.89482  lr: 0.0003 batch: 4 
epochs: 16 val loss: 2.8366224829916176  val f1:0.55059  lr: 0.0003 batch: 4
epochs: 17 train loss: 0.07255033551530449  train f1: 0.90976  lr: 0.0003 batch: 4 
epochs: 17 val loss: 3.592286284646653  val f1:0.50730  lr: 0.0003 batch: 4
epochs: 18 train loss: 0.06581031656667088  train f1: 0.93589  lr: 0.0003 batch: 4 
epochs: 18 val loss: 4.586370898863379  val f1:0.50224  lr: 0.0003 batch: 4
epochs: 19 train loss: 0.07238712220379478  train f1: 0.92839  lr: 0.0003 batch: 4 
epochs: 19 val loss: 4.397010919667349  val f1:0.48810  lr: 0.0003 batch: 4
epochs: 20 train loss: 0.059261569090550296  train f1: 0.93024  lr: 0.0003 batch: 4 
epochs: 20 val loss: 3.8402219385959193  val f1:0.56528  lr: 0.0003 batch: 4
epochs: 21 train loss: 0.051732559105876645  train f1: 0.93525  lr: 0.0003 batch: 4 
epochs: 21 val loss: 4.178322191251674  val f1:0.51718  lr: 0.0003 batch: 4
epochs: 0 train loss: 1.1629135617602648  train f1: 0.17295  lr: 0.0001 batch: 4 
epochs: 0 val loss: 2.7264759368047207  val f1:0.16364  lr: 0.0001 batch: 4
epochs: 1 train loss: 0.6106559524821866  train f1: 0.28831  lr: 0.0001 batch: 4 
epochs: 1 val loss: 2.0243643511204183  val f1:0.27002  lr: 0.0001 batch: 4
epochs: 2 train loss: 0.45536293787009663  train f1: 0.43324  lr: 0.0001 batch: 4 
epochs: 2 val loss: 2.037633694169251  val f1:0.35546  lr: 0.0001 batch: 4
epochs: 3 train loss: 0.34327248598305954  train f1: 0.54203  lr: 0.0001 batch: 4 
epochs: 3 val loss: 1.951457977294922  val f1:0.47185  lr: 0.0001 batch: 4
epochs: 4 train loss: 0.2555772347396681  train f1: 0.66263  lr: 0.0001 batch: 4 
epochs: 4 val loss: 1.7804154235047183  val f1:0.49213  lr: 0.0001 batch: 4
epochs: 5 train loss: 0.19823476102914714  train f1: 0.74466  lr: 0.0001 batch: 4 
epochs: 5 val loss: 2.1354417075473866  val f1:0.52857  lr: 0.0001 batch: 4
epochs: 6 train loss: 0.1557831103435618  train f1: 0.79870  lr: 0.0001 batch: 4 
epochs: 6 val loss: 2.214020766221083  val f1:0.52999  lr: 0.0001 batch: 4
epochs: 7 train loss: 0.12662412410371765  train f1: 0.83921  lr: 0.0001 batch: 4 
epochs: 7 val loss: 2.184120041098798  val f1:0.54158  lr: 0.0001 batch: 4
epochs: 8 train loss: 0.10091672854477095  train f1: 0.87690  lr: 0.0001 batch: 4 
epochs: 8 val loss: 2.549471418136566  val f1:0.54253  lr: 0.0001 batch: 4
epochs: 9 train loss: 0.08367789238133233  train f1: 0.88570  lr: 0.0001 batch: 4 
epochs: 9 val loss: 2.6674215501668477  val f1:0.59192  lr: 0.0001 batch: 4
epochs: 10 train loss: 0.07001009142577423  train f1: 0.91151  lr: 0.0001 batch: 4 
epochs: 10 val loss: 2.356236959211459  val f1:0.59023  lr: 0.0001 batch: 4
epochs: 11 train loss: 0.055216956004667854  train f1: 0.92371  lr: 0.0001 batch: 4 
epochs: 11 val loss: 2.704494651925365  val f1:0.56741  lr: 0.0001 batch: 4
epochs: 12 train loss: 0.06306182874945673  train f1: 0.93277  lr: 0.0001 batch: 4 
epochs: 12 val loss: 2.487034541556475  val f1:0.57826  lr: 0.0001 batch: 4
epochs: 13 train loss: 0.0378040719902917  train f1: 0.96144  lr: 0.0001 batch: 4 
epochs: 13 val loss: 2.997634186483711  val f1:0.60744  lr: 0.0001 batch: 4
epochs: 14 train loss: 0.039235688215784395  train f1: 0.95864  lr: 0.0001 batch: 4 
epochs: 14 val loss: 2.8047199054640153  val f1:0.57843  lr: 0.0001 batch: 4
epochs: 15 train loss: 0.045931503120879506  train f1: 0.95299  lr: 0.0001 batch: 4 
epochs: 15 val loss: 2.9400546569549952  val f1:0.60619  lr: 0.0001 batch: 4
epochs: 16 train loss: 0.03980251231443568  train f1: 0.94706  lr: 0.0001 batch: 4 
epochs: 16 val loss: 3.023058445865019  val f1:0.61766  lr: 0.0001 batch: 4
epochs: 17 train loss: 0.02865813739514082  train f1: 0.98216  lr: 0.0001 batch: 4 
epochs: 17 val loss: 3.0841730961330733  val f1:0.61506  lr: 0.0001 batch: 4
epochs: 18 train loss: 0.03860503401649135  train f1: 0.96702  lr: 0.0001 batch: 4 
epochs: 18 val loss: 3.101684563456307  val f1:0.56569  lr: 0.0001 batch: 4
epochs: 19 train loss: 0.033782089805781595  train f1: 0.97231  lr: 0.0001 batch: 4 
epochs: 19 val loss: 3.301567330873521  val f1:0.60784  lr: 0.0001 batch: 4
epochs: 20 train loss: 0.03473599394608974  train f1: 0.96172  lr: 0.0001 batch: 4 
epochs: 20 val loss: 3.5575264390412884  val f1:0.52526  lr: 0.0001 batch: 4
epochs: 21 train loss: 0.02464604070793824  train f1: 0.97651  lr: 0.0001 batch: 4 
epochs: 21 val loss: 3.1408541077585532  val f1:0.58570  lr: 0.0001 batch: 4
epochs: 0 train loss: 1.1347237633408667  train f1: 0.19995  lr: 0.001 batch: 8 
epochs: 0 val loss: 2.7125890661168968  val f1:0.16099  lr: 0.001 batch: 8
epochs: 1 train loss: 0.68184464701106  train f1: 0.27925  lr: 0.001 batch: 8 
epochs: 1 val loss: 2.9988713299786625  val f1:0.20679  lr: 0.001 batch: 8
epochs: 2 train loss: 0.5310458779781498  train f1: 0.37376  lr: 0.001 batch: 8 
epochs: 2 val loss: 2.4829512984664364  val f1:0.26707  lr: 0.001 batch: 8
epochs: 3 train loss: 0.49973171391290694  train f1: 0.40034  lr: 0.001 batch: 8 
epochs: 3 val loss: 2.613202808521413  val f1:0.29189  lr: 0.001 batch: 8
epochs: 4 train loss: 0.42871420928154996  train f1: 0.49838  lr: 0.001 batch: 8 
epochs: 4 val loss: 2.4090928819444453  val f1:0.29809  lr: 0.001 batch: 8
epochs: 5 train loss: 0.3994037971067965  train f1: 0.51395  lr: 0.001 batch: 8 
epochs: 5 val loss: 2.503883644386573  val f1:0.30922  lr: 0.001 batch: 8
epochs: 6 train loss: 0.39410759000742523  train f1: 0.51050  lr: 0.001 batch: 8 
epochs: 6 val loss: 2.928658944589122  val f1:0.32165  lr: 0.001 batch: 8
epochs: 7 train loss: 0.33093112059746793  train f1: 0.60227  lr: 0.001 batch: 8 
epochs: 7 val loss: 4.200553385416668  val f1:0.27200  lr: 0.001 batch: 8
epochs: 8 train loss: 0.3387208681428035  train f1: 0.58578  lr: 0.001 batch: 8 
epochs: 8 val loss: 4.854121907552085  val f1:0.34237  lr: 0.001 batch: 8
epochs: 9 train loss: 0.2942337114712716  train f1: 0.63602  lr: 0.001 batch: 8 
epochs: 9 val loss: 2.5544230143229165  val f1:0.41953  lr: 0.001 batch: 8
epochs: 10 train loss: 0.28899102443166425  train f1: 0.63433  lr: 0.001 batch: 8 
epochs: 10 val loss: 2.467627179181135  val f1:0.37196  lr: 0.001 batch: 8
epochs: 11 train loss: 0.2605308772026374  train f1: 0.66847  lr: 0.001 batch: 8 
epochs: 11 val loss: 2.9957691333912067  val f1:0.40535  lr: 0.001 batch: 8
epochs: 12 train loss: 0.23885657992702292  train f1: 0.70124  lr: 0.001 batch: 8 
epochs: 12 val loss: 3.020594391999422  val f1:0.36549  lr: 0.001 batch: 8
epochs: 13 train loss: 0.21539945280953757  train f1: 0.74105  lr: 0.001 batch: 8 
epochs: 13 val loss: 3.586649802879052  val f1:0.42293  lr: 0.001 batch: 8
epochs: 14 train loss: 0.20858831977129855  train f1: 0.74510  lr: 0.001 batch: 8 
epochs: 14 val loss: 3.95385470920139  val f1:0.42253  lr: 0.001 batch: 8
epochs: 15 train loss: 0.20752050456929277  train f1: 0.73786  lr: 0.001 batch: 8 
epochs: 15 val loss: 3.449048473216867  val f1:0.42227  lr: 0.001 batch: 8
epochs: 16 train loss: 0.18630365307411448  train f1: 0.76753  lr: 0.001 batch: 8 
epochs: 16 val loss: 3.4203823513454834  val f1:0.48032  lr: 0.001 batch: 8
epochs: 17 train loss: 0.17776861351527526  train f1: 0.77827  lr: 0.001 batch: 8 
epochs: 17 val loss: 5.108517908166955  val f1:0.40229  lr: 0.001 batch: 8
epochs: 18 train loss: 0.18785853064462046  train f1: 0.78101  lr: 0.001 batch: 8 
epochs: 18 val loss: 3.61552259657118  val f1:0.49172  lr: 0.001 batch: 8
epochs: 19 train loss: 0.14441065216778806  train f1: 0.82498  lr: 0.001 batch: 8 
epochs: 19 val loss: 4.378082049334485  val f1:0.43903  lr: 0.001 batch: 8
epochs: 20 train loss: 0.17810040288203666  train f1: 0.79220  lr: 0.001 batch: 8 
epochs: 20 val loss: 3.402727254231771  val f1:0.49015  lr: 0.001 batch: 8
epochs: 21 train loss: 0.15173334575324457  train f1: 0.80620  lr: 0.001 batch: 8 
epochs: 21 val loss: 3.831318155924478  val f1:0.45333  lr: 0.001 batch: 8
epochs: 0 train loss: 0.972379520144802  train f1: 0.22754  lr: 0.0003 batch: 8 
epochs: 0 val loss: 2.415322084780093  val f1:0.23423  lr: 0.0003 batch: 8
epochs: 1 train loss: 0.47658392373988634  train f1: 0.45131  lr: 0.0003 batch: 8 
epochs: 1 val loss: 1.9027162905092587  val f1:0.41175  lr: 0.0003 batch: 8
epochs: 2 train loss: 0.3099741310662545  train f1: 0.56269  lr: 0.0003 batch: 8 
epochs: 2 val loss: 1.924581344039353  val f1:0.47701  lr: 0.0003 batch: 8
epochs: 3 train loss: 0.232838482445992  train f1: 0.69433  lr: 0.0003 batch: 8 
epochs: 3 val loss: 2.1152151884856036  val f1:0.51285  lr: 0.0003 batch: 8
epochs: 4 train loss: 0.18641852707452086  train f1: 0.74535  lr: 0.0003 batch: 8 
epochs: 4 val loss: 2.1826653939706304  val f1:0.54783  lr: 0.0003 batch: 8
epochs: 5 train loss: 0.1513607448406436  train f1: 0.80813  lr: 0.0003 batch: 8 
epochs: 5 val loss: 2.255731727458813  val f1:0.56237  lr: 0.0003 batch: 8
epochs: 6 train loss: 0.1289311982272716  train f1: 0.84011  lr: 0.0003 batch: 8 
epochs: 6 val loss: 2.2483641730414496  val f1:0.57980  lr: 0.0003 batch: 8
epochs: 7 train loss: 0.13085982839712942  train f1: 0.83049  lr: 0.0003 batch: 8 
epochs: 7 val loss: 2.5188867074471926  val f1:0.58049  lr: 0.0003 batch: 8
epochs: 8 train loss: 0.10632560583536106  train f1: 0.85712  lr: 0.0003 batch: 8 
epochs: 8 val loss: 2.4977788571958177  val f1:0.53505  lr: 0.0003 batch: 8
epochs: 9 train loss: 0.07779637994837668  train f1: 0.90512  lr: 0.0003 batch: 8 
epochs: 9 val loss: 2.5899523134584768  val f1:0.60035  lr: 0.0003 batch: 8
epochs: 10 train loss: 0.0620013399517045  train f1: 0.91827  lr: 0.0003 batch: 8 
epochs: 10 val loss: 2.4251515706380222  val f1:0.58389  lr: 0.0003 batch: 8
epochs: 11 train loss: 0.08868740448790985  train f1: 0.91179  lr: 0.0003 batch: 8 
epochs: 11 val loss: 2.8755425029330772  val f1:0.54733  lr: 0.0003 batch: 8
epochs: 12 train loss: 0.06408557932028607  train f1: 0.92157  lr: 0.0003 batch: 8 
epochs: 12 val loss: 2.9249798916004313  val f1:0.55873  lr: 0.0003 batch: 8
epochs: 13 train loss: 0.07875923572408129  train f1: 0.90310  lr: 0.0003 batch: 8 
epochs: 13 val loss: 3.303738890753853  val f1:0.56190  lr: 0.0003 batch: 8
epochs: 14 train loss: 0.0667999933721421  train f1: 0.93445  lr: 0.0003 batch: 8 
epochs: 14 val loss: 3.1276061305293323  val f1:0.57952  lr: 0.0003 batch: 8
epochs: 15 train loss: 0.042851104271992325  train f1: 0.95373  lr: 0.0003 batch: 8 
epochs: 15 val loss: 3.19008481061017  val f1:0.56745  lr: 0.0003 batch: 8
epochs: 16 train loss: 0.07286604267827569  train f1: 0.92029  lr: 0.0003 batch: 8 
epochs: 16 val loss: 2.8495793377911607  val f1:0.58915  lr: 0.0003 batch: 8
epochs: 17 train loss: 0.050153889459617164  train f1: 0.94860  lr: 0.0003 batch: 8 
epochs: 17 val loss: 2.876345948819762  val f1:0.58985  lr: 0.0003 batch: 8
epochs: 18 train loss: 0.02689776259861637  train f1: 0.96737  lr: 0.0003 batch: 8 
epochs: 18 val loss: 3.166059266196354  val f1:0.60109  lr: 0.0003 batch: 8
epochs: 19 train loss: 0.033194536015335535  train f1: 0.96963  lr: 0.0003 batch: 8 
epochs: 19 val loss: 3.160867473814225  val f1:0.59888  lr: 0.0003 batch: 8
epochs: 20 train loss: 0.06877543294474421  train f1: 0.94173  lr: 0.0003 batch: 8 
epochs: 20 val loss: 2.8014236547328815  val f1:0.61048  lr: 0.0003 batch: 8
epochs: 21 train loss: 0.05610440208224318  train f1: 0.93913  lr: 0.0003 batch: 8 
epochs: 21 val loss: 3.229848225028428  val f1:0.58279  lr: 0.0003 batch: 8
epochs: 0 train loss: 1.1028441239832054  train f1: 0.16728  lr: 0.0001 batch: 8 
epochs: 0 val loss: 2.2647316261574066  val f1:0.14930  lr: 0.0001 batch: 8
epochs: 1 train loss: 0.5925306113025219  train f1: 0.29401  lr: 0.0001 batch: 8 
epochs: 1 val loss: 1.9188340928819454  val f1:0.28593  lr: 0.0001 batch: 8
epochs: 2 train loss: 0.41970675179127914  train f1: 0.44871  lr: 0.0001 batch: 8 
epochs: 2 val loss: 1.709383138020833  val f1:0.39240  lr: 0.0001 batch: 8
epochs: 3 train loss: 0.29562317208850863  train f1: 0.58791  lr: 0.0001 batch: 8 
epochs: 3 val loss: 1.8561654550057858  val f1:0.48580  lr: 0.0001 batch: 8
epochs: 4 train loss: 0.21588553739397703  train f1: 0.71882  lr: 0.0001 batch: 8 
epochs: 4 val loss: 1.8669738769531252  val f1:0.51517  lr: 0.0001 batch: 8
epochs: 5 train loss: 0.16698115773861777  train f1: 0.77153  lr: 0.0001 batch: 8 
epochs: 5 val loss: 1.7526987711588535  val f1:0.55264  lr: 0.0001 batch: 8
epochs: 6 train loss: 0.12433404422431404  train f1: 0.83770  lr: 0.0001 batch: 8 
epochs: 6 val loss: 1.944590194137009  val f1:0.56830  lr: 0.0001 batch: 8
epochs: 7 train loss: 0.08842142735527693  train f1: 0.88010  lr: 0.0001 batch: 8 
epochs: 7 val loss: 1.9495296407628941  val f1:0.59732  lr: 0.0001 batch: 8
epochs: 8 train loss: 0.07427596778012392  train f1: 0.90128  lr: 0.0001 batch: 8 
epochs: 8 val loss: 2.1740720113118512  val f1:0.62015  lr: 0.0001 batch: 8
epochs: 9 train loss: 0.05563946341753901  train f1: 0.93955  lr: 0.0001 batch: 8 
epochs: 9 val loss: 2.200395887869376  val f1:0.59896  lr: 0.0001 batch: 8
epochs: 10 train loss: 0.053174412875586205  train f1: 0.93844  lr: 0.0001 batch: 8 
epochs: 10 val loss: 2.241827841158267  val f1:0.61933  lr: 0.0001 batch: 8
epochs: 11 train loss: 0.05472963869794923  train f1: 0.93530  lr: 0.0001 batch: 8 
epochs: 11 val loss: 2.345444032880996  val f1:0.60778  lr: 0.0001 batch: 8
epochs: 12 train loss: 0.05378610130106466  train f1: 0.93752  lr: 0.0001 batch: 8 
epochs: 12 val loss: 2.3288410151446315  val f1:0.61720  lr: 0.0001 batch: 8
epochs: 13 train loss: 0.023905556300159718  train f1: 0.97084  lr: 0.0001 batch: 8 
epochs: 13 val loss: 2.5353258433165387  val f1:0.61672  lr: 0.0001 batch: 8
epochs: 14 train loss: 0.029956007383289417  train f1: 0.97095  lr: 0.0001 batch: 8 
epochs: 14 val loss: 2.439559196542811  val f1:0.61809  lr: 0.0001 batch: 8
epochs: 15 train loss: 0.03076140666275882  train f1: 0.96873  lr: 0.0001 batch: 8 
epochs: 15 val loss: 2.5737942448368774  val f1:0.63386  lr: 0.0001 batch: 8
epochs: 16 train loss: 0.02953316600581679  train f1: 0.96932  lr: 0.0001 batch: 8 
epochs: 16 val loss: 2.5155662518960473  val f1:0.61647  lr: 0.0001 batch: 8
epochs: 17 train loss: 0.028737906771206228  train f1: 0.97755  lr: 0.0001 batch: 8 
epochs: 17 val loss: 2.8258362134297714  val f1:0.63170  lr: 0.0001 batch: 8
epochs: 18 train loss: 0.019400433319784725  train f1: 0.97380  lr: 0.0001 batch: 8 
epochs: 18 val loss: 2.829469199754574  val f1:0.60743  lr: 0.0001 batch: 8
epochs: 19 train loss: 0.028148941891023706  train f1: 0.97417  lr: 0.0001 batch: 8 
epochs: 19 val loss: 2.851326190100775  val f1:0.64060  lr: 0.0001 batch: 8
epochs: 20 train loss: 0.018738376960325775  train f1: 0.97953  lr: 0.0001 batch: 8 
epochs: 20 val loss: 2.8871818816220323  val f1:0.63916  lr: 0.0001 batch: 8
epochs: 21 train loss: 0.020484447032771302  train f1: 0.98407  lr: 0.0001 batch: 8 
epochs: 21 val loss: 2.9441469322752054  val f1:0.62931  lr: 0.0001 batch: 8
epochs: 0 train loss: 1.0514327488588482  train f1: 0.21056  lr: 0.001 batch: 16 
epochs: 0 val loss: 2.4003399884259258  val f1:0.20647  lr: 0.001 batch: 16
epochs: 1 train loss: 0.6226900365022238  train f1: 0.31996  lr: 0.001 batch: 16 
epochs: 1 val loss: 2.417100694444445  val f1:0.26986  lr: 0.001 batch: 16
epochs: 2 train loss: 0.480860335103581  train f1: 0.43436  lr: 0.001 batch: 16 
epochs: 2 val loss: 2.3615523726851855  val f1:0.33164  lr: 0.001 batch: 16
epochs: 3 train loss: 0.36560727237315654  train f1: 0.52873  lr: 0.001 batch: 16 
epochs: 3 val loss: 2.207295283564815  val f1:0.40530  lr: 0.001 batch: 16
epochs: 4 train loss: 0.3060174434819026  train f1: 0.59504  lr: 0.001 batch: 16 
epochs: 4 val loss: 2.1322735821759267  val f1:0.41353  lr: 0.001 batch: 16
epochs: 5 train loss: 0.2972557410765231  train f1: 0.60700  lr: 0.001 batch: 16 
epochs: 5 val loss: 2.0579463252314816  val f1:0.45136  lr: 0.001 batch: 16
epochs: 6 train loss: 0.251212716549077  train f1: 0.66429  lr: 0.001 batch: 16 
epochs: 6 val loss: 1.9586697048611106  val f1:0.45347  lr: 0.001 batch: 16
epochs: 7 train loss: 0.2095382204662993  train f1: 0.73146  lr: 0.001 batch: 16 
epochs: 7 val loss: 2.202528211805556  val f1:0.47636  lr: 0.001 batch: 16
epochs: 8 train loss: 0.20197844058833317  train f1: 0.74249  lr: 0.001 batch: 16 
epochs: 8 val loss: 2.3754267939814815  val f1:0.48902  lr: 0.001 batch: 16
epochs: 9 train loss: 0.17645432261491995  train f1: 0.78600  lr: 0.001 batch: 16 
epochs: 9 val loss: 2.3702293113425927  val f1:0.48020  lr: 0.001 batch: 16
epochs: 10 train loss: 0.22050036026743913  train f1: 0.74360  lr: 0.001 batch: 16 
epochs: 10 val loss: 2.714245153356481  val f1:0.46037  lr: 0.001 batch: 16
epochs: 11 train loss: 0.19348047377911398  train f1: 0.75826  lr: 0.001 batch: 16 
epochs: 11 val loss: 2.673753978587964  val f1:0.53523  lr: 0.001 batch: 16
epochs: 12 train loss: 0.1445397830634528  train f1: 0.83021  lr: 0.001 batch: 16 
epochs: 12 val loss: 2.610767505787038  val f1:0.50393  lr: 0.001 batch: 16
epochs: 13 train loss: 0.10980591434664487  train f1: 0.85711  lr: 0.001 batch: 16 
epochs: 13 val loss: 2.219303385416666  val f1:0.53190  lr: 0.001 batch: 16
epochs: 14 train loss: 0.11036027058233484  train f1: 0.85745  lr: 0.001 batch: 16 
epochs: 14 val loss: 2.573864293981481  val f1:0.47181  lr: 0.001 batch: 16
epochs: 15 train loss: 0.13583525468347682  train f1: 0.80872  lr: 0.001 batch: 16 
epochs: 15 val loss: 2.892838541666666  val f1:0.47595  lr: 0.001 batch: 16
epochs: 16 train loss: 0.12524069561047504  train f1: 0.84519  lr: 0.001 batch: 16 
epochs: 16 val loss: 2.432738353587963  val f1:0.48829  lr: 0.001 batch: 16
epochs: 17 train loss: 0.13316786155272067  train f1: 0.84854  lr: 0.001 batch: 16 
epochs: 17 val loss: 2.7754123263888886  val f1:0.51379  lr: 0.001 batch: 16
epochs: 18 train loss: 0.11157360684112659  train f1: 0.85333  lr: 0.001 batch: 16 
epochs: 18 val loss: 3.193945312499999  val f1:0.46643  lr: 0.001 batch: 16
epochs: 19 train loss: 0.11540927601217774  train f1: 0.87342  lr: 0.001 batch: 16 
epochs: 19 val loss: 2.641574435763889  val f1:0.54594  lr: 0.001 batch: 16
epochs: 20 train loss: 0.07585234588451595  train f1: 0.90429  lr: 0.001 batch: 16 
epochs: 20 val loss: 2.806083622685187  val f1:0.49969  lr: 0.001 batch: 16
epochs: 21 train loss: 0.10002064615599678  train f1: 0.87993  lr: 0.001 batch: 16 
epochs: 21 val loss: 2.8550998263888903  val f1:0.53112  lr: 0.001 batch: 16
epochs: 0 train loss: 0.9309924407844679  train f1: 0.22695  lr: 0.0003 batch: 16 
epochs: 0 val loss: 2.286715133101852  val f1:0.27320  lr: 0.0003 batch: 16
epochs: 1 train loss: 0.42224969756737196  train f1: 0.48373  lr: 0.0003 batch: 16 
epochs: 1 val loss: 1.933604600694444  val f1:0.38287  lr: 0.0003 batch: 16
epochs: 2 train loss: 0.2917385315627195  train f1: 0.60368  lr: 0.0003 batch: 16 
epochs: 2 val loss: 1.9933322482638893  val f1:0.45528  lr: 0.0003 batch: 16
epochs: 3 train loss: 0.19497081313686856  train f1: 0.74037  lr: 0.0003 batch: 16 
epochs: 3 val loss: 1.7894314236111113  val f1:0.51679  lr: 0.0003 batch: 16
epochs: 4 train loss: 0.13600672318247806  train f1: 0.83284  lr: 0.0003 batch: 16 
epochs: 4 val loss: 1.9974862557870374  val f1:0.52087  lr: 0.0003 batch: 16
epochs: 5 train loss: 0.09062894542565507  train f1: 0.87221  lr: 0.0003 batch: 16 
epochs: 5 val loss: 1.9405888310185189  val f1:0.59756  lr: 0.0003 batch: 16
epochs: 6 train loss: 0.08458210912983068  train f1: 0.88434  lr: 0.0003 batch: 16 
epochs: 6 val loss: 2.2257197627314813  val f1:0.59144  lr: 0.0003 batch: 16
epochs: 7 train loss: 0.07855891109852313  train f1: 0.90923  lr: 0.0003 batch: 16 
epochs: 7 val loss: 2.3271484375  val f1:0.56195  lr: 0.0003 batch: 16
epochs: 8 train loss: 0.06077207072397293  train f1: 0.92662  lr: 0.0003 batch: 16 
epochs: 8 val loss: 2.1575068721064814  val f1:0.59698  lr: 0.0003 batch: 16
epochs: 9 train loss: 0.04588675766848446  train f1: 0.94867  lr: 0.0003 batch: 16 
epochs: 9 val loss: 2.240838396990741  val f1:0.60231  lr: 0.0003 batch: 16
epochs: 10 train loss: 0.05477092551827878  train f1: 0.95018  lr: 0.0003 batch: 16 
epochs: 10 val loss: 2.6017686631944428  val f1:0.56969  lr: 0.0003 batch: 16
epochs: 11 train loss: 0.0507685968491915  train f1: 0.94234  lr: 0.0003 batch: 16 
epochs: 11 val loss: 2.3740143952546306  val f1:0.56530  lr: 0.0003 batch: 16
epochs: 12 train loss: 0.05492455771799838  train f1: 0.94264  lr: 0.0003 batch: 16 
epochs: 12 val loss: 2.5145417390046294  val f1:0.59796  lr: 0.0003 batch: 16
epochs: 13 train loss: 0.03441905841398773  train f1: 0.96749  lr: 0.0003 batch: 16 
epochs: 13 val loss: 2.6205765335648143  val f1:0.59158  lr: 0.0003 batch: 16
epochs: 14 train loss: 0.03475171171324083  train f1: 0.95549  lr: 0.0003 batch: 16 
epochs: 14 val loss: 2.7536458333333345  val f1:0.61489  lr: 0.0003 batch: 16
epochs: 15 train loss: 0.029081755809569634  train f1: 0.96375  lr: 0.0003 batch: 16 
epochs: 15 val loss: 2.7245121708622704  val f1:0.60014  lr: 0.0003 batch: 16
epochs: 16 train loss: 0.050531572170471904  train f1: 0.96037  lr: 0.0003 batch: 16 
epochs: 16 val loss: 2.7906901041666674  val f1:0.60235  lr: 0.0003 batch: 16
epochs: 17 train loss: 0.05478252810931838  train f1: 0.94757  lr: 0.0003 batch: 16 
epochs: 17 val loss: 2.6789523654513894  val f1:0.59600  lr: 0.0003 batch: 16
epochs: 18 train loss: 0.018717968062068643  train f1: 0.97876  lr: 0.0003 batch: 16 
epochs: 18 val loss: 2.7315637659143515  val f1:0.61734  lr: 0.0003 batch: 16
epochs: 19 train loss: 0.02232134409165115  train f1: 0.97287  lr: 0.0003 batch: 16 
epochs: 19 val loss: 2.9094708478009275  val f1:0.61499  lr: 0.0003 batch: 16
epochs: 20 train loss: 0.02381486571236942  train f1: 0.98416  lr: 0.0003 batch: 16 
epochs: 20 val loss: 2.6912624782986114  val f1:0.62436  lr: 0.0003 batch: 16
epochs: 21 train loss: 0.03468255090356318  train f1: 0.98101  lr: 0.0003 batch: 16 
epochs: 21 val loss: 3.0366048177083327  val f1:0.60875  lr: 0.0003 batch: 16
epochs: 0 train loss: 1.0865064756700606  train f1: 0.16335  lr: 0.0001 batch: 16 
epochs: 0 val loss: 2.3814236111111113  val f1:0.15015  lr: 0.0001 batch: 16
epochs: 1 train loss: 0.5693547966774932  train f1: 0.30971  lr: 0.0001 batch: 16 
epochs: 1 val loss: 2.0430627893518514  val f1:0.24604  lr: 0.0001 batch: 16
epochs: 2 train loss: 0.4090134695674596  train f1: 0.43214  lr: 0.0001 batch: 16 
epochs: 2 val loss: 1.7740776909722227  val f1:0.34575  lr: 0.0001 batch: 16
epochs: 3 train loss: 0.3079836270335908  train f1: 0.59095  lr: 0.0001 batch: 16 
epochs: 3 val loss: 1.6561993634259267  val f1:0.46067  lr: 0.0001 batch: 16
epochs: 4 train loss: 0.22124222601844132  train f1: 0.69658  lr: 0.0001 batch: 16 
epochs: 4 val loss: 1.5903284143518523  val f1:0.50099  lr: 0.0001 batch: 16
epochs: 5 train loss: 0.15892433137929404  train f1: 0.79447  lr: 0.0001 batch: 16 
epochs: 5 val loss: 1.800526258680555  val f1:0.52226  lr: 0.0001 batch: 16
epochs: 6 train loss: 0.11396005180444611  train f1: 0.84392  lr: 0.0001 batch: 16 
epochs: 6 val loss: 1.6147822627314818  val f1:0.56430  lr: 0.0001 batch: 16
epochs: 7 train loss: 0.08770086167010474  train f1: 0.90353  lr: 0.0001 batch: 16 
epochs: 7 val loss: 1.9552915219907412  val f1:0.59656  lr: 0.0001 batch: 16
epochs: 8 train loss: 0.06628758362616495  train f1: 0.91889  lr: 0.0001 batch: 16 
epochs: 8 val loss: 1.8163963035300932  val f1:0.57521  lr: 0.0001 batch: 16
epochs: 9 train loss: 0.04080326994706628  train f1: 0.95633  lr: 0.0001 batch: 16 
epochs: 9 val loss: 1.9386483651620372  val f1:0.58408  lr: 0.0001 batch: 16
epochs: 10 train loss: 0.041705560148432005  train f1: 0.95569  lr: 0.0001 batch: 16 
epochs: 10 val loss: 1.708201316550925  val f1:0.60204  lr: 0.0001 batch: 16
epochs: 11 train loss: 0.02795230315419172  train f1: 0.96882  lr: 0.0001 batch: 16 
epochs: 11 val loss: 1.961277488425926  val f1:0.61933  lr: 0.0001 batch: 16
epochs: 12 train loss: 0.034974797388141056  train f1: 0.95990  lr: 0.0001 batch: 16 
epochs: 12 val loss: 1.9175509982638885  val f1:0.61021  lr: 0.0001 batch: 16
epochs: 13 train loss: 0.029297484887226697  train f1: 0.96833  lr: 0.0001 batch: 16 
epochs: 13 val loss: 1.8339454933449075  val f1:0.60282  lr: 0.0001 batch: 16
epochs: 14 train loss: 0.02571043896764406  train f1: 0.98428  lr: 0.0001 batch: 16 
epochs: 14 val loss: 2.031794343171296  val f1:0.60563  lr: 0.0001 batch: 16
epochs: 15 train loss: 0.0254276254203882  train f1: 0.97582  lr: 0.0001 batch: 16 
epochs: 15 val loss: 1.9826397931134259  val f1:0.62034  lr: 0.0001 batch: 16
epochs: 16 train loss: 0.009341454684511107  train f1: 0.99496  lr: 0.0001 batch: 16 
epochs: 16 val loss: 1.9917629665798606  val f1:0.63806  lr: 0.0001 batch: 16
epochs: 17 train loss: 0.019045754541618554  train f1: 0.98727  lr: 0.0001 batch: 16 
epochs: 17 val loss: 2.195753761574073  val f1:0.61272  lr: 0.0001 batch: 16
epochs: 18 train loss: 0.018431996138354806  train f1: 0.98437  lr: 0.0001 batch: 16 
epochs: 18 val loss: 2.1870352285879635  val f1:0.62724  lr: 0.0001 batch: 16
epochs: 19 train loss: 0.015569738680950261  train f1: 0.98323  lr: 0.0001 batch: 16 
epochs: 19 val loss: 2.0181455258969914  val f1:0.63174  lr: 0.0001 batch: 16
epochs: 20 train loss: 0.017705945709671427  train f1: 0.97919  lr: 0.0001 batch: 16 
epochs: 20 val loss: 2.182411928530094  val f1:0.62631  lr: 0.0001 batch: 16
epochs: 21 train loss: 0.02162024568529166  train f1: 0.97719  lr: 0.0001 batch: 16 
epochs: 21 val loss: 2.189422381365741  val f1:0.63009  lr: 0.0001 batch: 16
epochs: 0 train loss: 0.9957539572644589  train f1: 0.22981  lr: 0.001 batch: 32 
epochs: 0 val loss: 2.496596392463236  val f1:0.18972  lr: 0.001 batch: 32
epochs: 1 train loss: 0.5763158086520522  train f1: 0.35671  lr: 0.001 batch: 32 
epochs: 1 val loss: 2.059699563419118  val f1:0.30892  lr: 0.001 batch: 32
epochs: 2 train loss: 0.39679649694642  train f1: 0.50196  lr: 0.001 batch: 32 
epochs: 2 val loss: 2.1043485753676467  val f1:0.37404  lr: 0.001 batch: 32
epochs: 3 train loss: 0.3148216133687033  train f1: 0.59113  lr: 0.001 batch: 32 
epochs: 3 val loss: 2.488913143382353  val f1:0.40484  lr: 0.001 batch: 32
epochs: 4 train loss: 0.25270319696682614  train f1: 0.65391  lr: 0.001 batch: 32 
epochs: 4 val loss: 2.0324994255514697  val f1:0.47474  lr: 0.001 batch: 32
epochs: 5 train loss: 0.20614248247288952  train f1: 0.70951  lr: 0.001 batch: 32 
epochs: 5 val loss: 2.0023408777573533  val f1:0.43204  lr: 0.001 batch: 32
epochs: 6 train loss: 0.17566635359579058  train f1: 0.76355  lr: 0.001 batch: 32 
epochs: 6 val loss: 2.0773638556985294  val f1:0.48625  lr: 0.001 batch: 32
epochs: 7 train loss: 0.13969034223414184  train f1: 0.80566  lr: 0.001 batch: 32 
epochs: 7 val loss: 2.1155000574448537  val f1:0.50461  lr: 0.001 batch: 32
epochs: 8 train loss: 0.13718528178200795  train f1: 0.80115  lr: 0.001 batch: 32 
epochs: 8 val loss: 2.135964786305147  val f1:0.52119  lr: 0.001 batch: 32
epochs: 9 train loss: 0.13488439303725513  train f1: 0.83662  lr: 0.001 batch: 32 
epochs: 9 val loss: 2.2453038832720584  val f1:0.45397  lr: 0.001 batch: 32
epochs: 10 train loss: 0.11896512045789119  train f1: 0.83151  lr: 0.001 batch: 32 
epochs: 10 val loss: 2.4560834099264697  val f1:0.49028  lr: 0.001 batch: 32
epochs: 11 train loss: 0.10947862311975279  train f1: 0.86781  lr: 0.001 batch: 32 
epochs: 11 val loss: 2.403191061580882  val f1:0.52412  lr: 0.001 batch: 32
epochs: 12 train loss: 0.08813001148736302  train f1: 0.88256  lr: 0.001 batch: 32 
epochs: 12 val loss: 2.5326573988970575  val f1:0.55390  lr: 0.001 batch: 32
epochs: 13 train loss: 0.09759508673824477  train f1: 0.90246  lr: 0.001 batch: 32 
epochs: 13 val loss: 3.2054515165441178  val f1:0.50930  lr: 0.001 batch: 32
epochs: 14 train loss: 0.14400171877732923  train f1: 0.83654  lr: 0.001 batch: 32 
epochs: 14 val loss: 2.6626335592830883  val f1:0.53369  lr: 0.001 batch: 32
epochs: 15 train loss: 0.10190236390526614  train f1: 0.87759  lr: 0.001 batch: 32 
epochs: 15 val loss: 2.705451516544118  val f1:0.56763  lr: 0.001 batch: 32
epochs: 16 train loss: 0.05684742998720995  train f1: 0.93496  lr: 0.001 batch: 32 
epochs: 16 val loss: 2.904217888327207  val f1:0.55353  lr: 0.001 batch: 32
epochs: 17 train loss: 0.04396802809701037  train f1: 0.95095  lr: 0.001 batch: 32 
epochs: 17 val loss: 2.6223790785845584  val f1:0.59256  lr: 0.001 batch: 32
epochs: 18 train loss: 0.0576952286620638  train f1: 0.93704  lr: 0.001 batch: 32 
epochs: 18 val loss: 2.3985595703125004  val f1:0.58579  lr: 0.001 batch: 32
epochs: 19 train loss: 0.060770337261370745  train f1: 0.93033  lr: 0.001 batch: 32 
epochs: 19 val loss: 2.831356272977942  val f1:0.54687  lr: 0.001 batch: 32
epochs: 20 train loss: 0.0531465864893216  train f1: 0.93416  lr: 0.001 batch: 32 
epochs: 20 val loss: 2.9057617187499996  val f1:0.55626  lr: 0.001 batch: 32
epochs: 21 train loss: 0.04849900950246783  train f1: 0.94306  lr: 0.001 batch: 32 
epochs: 21 val loss: 2.608972886029411  val f1:0.55548  lr: 0.001 batch: 32
epochs: 0 train loss: 0.9284631529850748  train f1: 0.22385  lr: 0.0003 batch: 32 
epochs: 0 val loss: 2.438519646139707  val f1:0.19230  lr: 0.0003 batch: 32
epochs: 1 train loss: 0.43542298274253743  train f1: 0.44268  lr: 0.0003 batch: 32 
epochs: 1 val loss: 2.1117733226102935  val f1:0.35315  lr: 0.0003 batch: 32
epochs: 2 train loss: 0.287110058229361  train f1: 0.62343  lr: 0.0003 batch: 32 
epochs: 2 val loss: 1.8443747127757355  val f1:0.44913  lr: 0.0003 batch: 32
epochs: 3 train loss: 0.17881387739039178  train f1: 0.73958  lr: 0.0003 batch: 32 
epochs: 3 val loss: 1.7302461511948524  val f1:0.50728  lr: 0.0003 batch: 32
epochs: 4 train loss: 0.12071077147526524  train f1: 0.81698  lr: 0.0003 batch: 32 
epochs: 4 val loss: 1.7198845358455888  val f1:0.52409  lr: 0.0003 batch: 32
epochs: 5 train loss: 0.08287639048562116  train f1: 0.88842  lr: 0.0003 batch: 32 
epochs: 5 val loss: 1.6939122817095587  val f1:0.56459  lr: 0.0003 batch: 32
epochs: 6 train loss: 0.05930613048041047  train f1: 0.91494  lr: 0.0003 batch: 32 
epochs: 6 val loss: 1.810281192555146  val f1:0.57252  lr: 0.0003 batch: 32
epochs: 7 train loss: 0.05489972812026295  train f1: 0.92187  lr: 0.0003 batch: 32 
epochs: 7 val loss: 1.8552389705882353  val f1:0.58609  lr: 0.0003 batch: 32
epochs: 8 train loss: 0.04236428773225243  train f1: 0.94401  lr: 0.0003 batch: 32 
epochs: 8 val loss: 1.928251378676471  val f1:0.59652  lr: 0.0003 batch: 32
epochs: 9 train loss: 0.0363005880099624  train f1: 0.95382  lr: 0.0003 batch: 32 
epochs: 9 val loss: 1.986888212316176  val f1:0.60128  lr: 0.0003 batch: 32
epochs: 10 train loss: 0.0383241852717613  train f1: 0.95800  lr: 0.0003 batch: 32 
epochs: 10 val loss: 1.987534466911764  val f1:0.61708  lr: 0.0003 batch: 32
epochs: 11 train loss: 0.013067794379903309  train f1: 0.98755  lr: 0.0003 batch: 32 
epochs: 11 val loss: 1.9725198184742652  val f1:0.62780  lr: 0.0003 batch: 32
epochs: 12 train loss: 0.019010746656958743  train f1: 0.97345  lr: 0.0003 batch: 32 
epochs: 12 val loss: 2.078196806066177  val f1:0.63023  lr: 0.0003 batch: 32
epochs: 13 train loss: 0.02814288103758399  train f1: 0.97129  lr: 0.0003 batch: 32 
epochs: 13 val loss: 1.9039881089154416  val f1:0.61471  lr: 0.0003 batch: 32
epochs: 14 train loss: 0.026265726160647266  train f1: 0.97693  lr: 0.0003 batch: 32 
epochs: 14 val loss: 2.258121266084559  val f1:0.61440  lr: 0.0003 batch: 32
epochs: 15 train loss: 0.03006679620315781  train f1: 0.96798  lr: 0.0003 batch: 32 
epochs: 15 val loss: 2.073881261488971  val f1:0.60604  lr: 0.0003 batch: 32
epochs: 16 train loss: 0.0458120499084245  train f1: 0.95672  lr: 0.0003 batch: 32 
epochs: 16 val loss: 2.2372328814338234  val f1:0.59024  lr: 0.0003 batch: 32
epochs: 17 train loss: 0.055481373374141905  train f1: 0.93989  lr: 0.0003 batch: 32 
epochs: 17 val loss: 2.0778664981617645  val f1:0.61435  lr: 0.0003 batch: 32
epochs: 18 train loss: 0.02004702767329429  train f1: 0.97973  lr: 0.0003 batch: 32 
epochs: 18 val loss: 1.980066636029412  val f1:0.61153  lr: 0.0003 batch: 32
epochs: 19 train loss: 0.013461242860822531  train f1: 0.98141  lr: 0.0003 batch: 32 
epochs: 19 val loss: 2.174582088694852  val f1:0.61035  lr: 0.0003 batch: 32
epochs: 20 train loss: 0.00685663854897912  train f1: 0.99166  lr: 0.0003 batch: 32 
epochs: 20 val loss: 2.109813017003676  val f1:0.64796  lr: 0.0003 batch: 32
epochs: 21 train loss: 0.019762735758254782  train f1: 0.98203  lr: 0.0003 batch: 32 
epochs: 21 val loss: 2.387494255514706  val f1:0.60284  lr: 0.0003 batch: 32
epochs: 0 train loss: 1.164750284223414  train f1: 0.16840  lr: 0.0001 batch: 32 
epochs: 0 val loss: 2.444637522977942  val f1:0.13236  lr: 0.0001 batch: 32
epochs: 1 train loss: 0.565508486619636  train f1: 0.25801  lr: 0.0001 batch: 32 
epochs: 1 val loss: 2.0835535386029416  val f1:0.22843  lr: 0.0001 batch: 32
epochs: 2 train loss: 0.40713091038945903  train f1: 0.41700  lr: 0.0001 batch: 32 
epochs: 2 val loss: 1.8056784237132355  val f1:0.29762  lr: 0.0001 batch: 32
epochs: 3 train loss: 0.30368497478428186  train f1: 0.53621  lr: 0.0001 batch: 32 
epochs: 3 val loss: 1.9561408547794121  val f1:0.34620  lr: 0.0001 batch: 32
epochs: 4 train loss: 0.2423134419455457  train f1: 0.65721  lr: 0.0001 batch: 32 
epochs: 4 val loss: 1.8862017463235292  val f1:0.41824  lr: 0.0001 batch: 32
epochs: 5 train loss: 0.16805107913800135  train f1: 0.79466  lr: 0.0001 batch: 32 
epochs: 5 val loss: 1.697150735294117  val f1:0.45970  lr: 0.0001 batch: 32
epochs: 6 train loss: 0.12328327235890857  train f1: 0.85123  lr: 0.0001 batch: 32 
epochs: 6 val loss: 1.9110969094669126  val f1:0.49014  lr: 0.0001 batch: 32
epochs: 7 train loss: 0.10615348815917965  train f1: 0.88574  lr: 0.0001 batch: 32 
epochs: 7 val loss: 1.8943804572610288  val f1:0.52346  lr: 0.0001 batch: 32
epochs: 8 train loss: 0.07281548229616083  train f1: 0.90553  lr: 0.0001 batch: 32 
epochs: 8 val loss: 1.8716179342830888  val f1:0.53441  lr: 0.0001 batch: 32
epochs: 9 train loss: 0.05669704835806319  train f1: 0.93966  lr: 0.0001 batch: 32 
epochs: 9 val loss: 1.7822481043198533  val f1:0.55664  lr: 0.0001 batch: 32
epochs: 10 train loss: 0.04130762014816057  train f1: 0.95573  lr: 0.0001 batch: 32 
epochs: 10 val loss: 1.9335219439338236  val f1:0.55174  lr: 0.0001 batch: 32
epochs: 11 train loss: 0.024686870290272247  train f1: 0.97871  lr: 0.0001 batch: 32 
epochs: 11 val loss: 1.9816032858455883  val f1:0.56468  lr: 0.0001 batch: 32
epochs: 12 train loss: 0.03406284104532271  train f1: 0.96564  lr: 0.0001 batch: 32 
epochs: 12 val loss: 1.89431583180147  val f1:0.57372  lr: 0.0001 batch: 32
epochs: 13 train loss: 0.0234518762844712  train f1: 0.98636  lr: 0.0001 batch: 32 
epochs: 13 val loss: 1.9466983570772065  val f1:0.57135  lr: 0.0001 batch: 32
epochs: 14 train loss: 0.02683384382902686  train f1: 0.97572  lr: 0.0001 batch: 32 
epochs: 14 val loss: 1.9139691521139715  val f1:0.58295  lr: 0.0001 batch: 32
epochs: 15 train loss: 0.01719119655552195  train f1: 0.98851  lr: 0.0001 batch: 32 
epochs: 15 val loss: 1.8903162339154416  val f1:0.59272  lr: 0.0001 batch: 32
epochs: 16 train loss: 0.013521190899521566  train f1: 0.98636  lr: 0.0001 batch: 32 
epochs: 16 val loss: 1.8492934283088236  val f1:0.59040  lr: 0.0001 batch: 32
epochs: 17 train loss: 0.011343612599728712  train f1: 0.98907  lr: 0.0001 batch: 32 
epochs: 17 val loss: 1.9535127527573528  val f1:0.59281  lr: 0.0001 batch: 32
epochs: 18 train loss: 0.015223090328387359  train f1: 0.98374  lr: 0.0001 batch: 32 
epochs: 18 val loss: 1.9744154986213236  val f1:0.58662  lr: 0.0001 batch: 32
epochs: 19 train loss: 0.015461153058863392  train f1: 0.99129  lr: 0.0001 batch: 32 
epochs: 19 val loss: 1.8639490464154407  val f1:0.60619  lr: 0.0001 batch: 32
epochs: 20 train loss: 0.015633851734559927  train f1: 0.98504  lr: 0.0001 batch: 32 
epochs: 20 val loss: 1.936286477481617  val f1:0.59171  lr: 0.0001 batch: 32
epochs: 21 train loss: 0.007456181654289589  train f1: 0.99583  lr: 0.0001 batch: 32 
epochs: 21 val loss: 1.9631993910845593  val f1:0.60426  lr: 0.0001 batch: 32
epochs: 0 train loss: 1.0497873802756554  train f1: 0.19474  lr: 0.0003 batch: 4 
epochs: 0 val loss: 2.602376254900931  val f1:0.19669  lr: 0.0003 batch: 4
epochs: 1 train loss: 0.5936339517657666  train f1: 0.32009  lr: 0.0003 batch: 4 
epochs: 1 val loss: 2.294867917204167  val f1:0.27973  lr: 0.0003 batch: 4
epochs: 2 train loss: 0.4480009480808557  train f1: 0.47353  lr: 0.0003 batch: 4 
epochs: 2 val loss: 2.2183679357751624  val f1:0.36825  lr: 0.0003 batch: 4
epochs: 3 train loss: 0.3627807718984189  train f1: 0.55090  lr: 0.0003 batch: 4 
epochs: 3 val loss: 2.3614226431483902  val f1:0.38611  lr: 0.0003 batch: 4
epochs: 4 train loss: 0.2983920578653001  train f1: 0.63137  lr: 0.0003 batch: 4 
epochs: 4 val loss: 2.439486340821785  val f1:0.47288  lr: 0.0003 batch: 4
epochs: 5 train loss: 0.24428810408052887  train f1: 0.68152  lr: 0.0003 batch: 4 
epochs: 5 val loss: 2.091665593502914  val f1:0.47963  lr: 0.0003 batch: 4
epochs: 6 train loss: 0.21846459041373983  train f1: 0.72394  lr: 0.0003 batch: 4 
epochs: 6 val loss: 3.1101489412098955  val f1:0.43082  lr: 0.0003 batch: 4
epochs: 7 train loss: 0.1924920173620016  train f1: 0.75948  lr: 0.0003 batch: 4 
epochs: 7 val loss: 2.5614496896350984  val f1:0.50201  lr: 0.0003 batch: 4
epochs: 8 train loss: 0.15447590553135465  train f1: 0.78837  lr: 0.0003 batch: 4 
epochs: 8 val loss: 2.9097675684431743  val f1:0.53056  lr: 0.0003 batch: 4
epochs: 9 train loss: 0.1410708495516903  train f1: 0.84270  lr: 0.0003 batch: 4 
epochs: 9 val loss: 2.9544495919197513  val f1:0.47753  lr: 0.0003 batch: 4
epochs: 10 train loss: 0.12841370199503518  train f1: 0.85436  lr: 0.0003 batch: 4 
epochs: 10 val loss: 3.442008476531571  val f1:0.52146  lr: 0.0003 batch: 4
epochs: 11 train loss: 0.11711031711949847  train f1: 0.84761  lr: 0.0003 batch: 4 
epochs: 11 val loss: 3.7506112156639726  val f1:0.52457  lr: 0.0003 batch: 4
epochs: 12 train loss: 0.10097636410806048  train f1: 0.85004  lr: 0.0003 batch: 4 
epochs: 12 val loss: 3.5949482336115115  val f1:0.49159  lr: 0.0003 batch: 4
epochs: 13 train loss: 0.11164092319958215  train f1: 0.87048  lr: 0.0003 batch: 4 
epochs: 13 val loss: 3.7114208115054  val f1:0.49098  lr: 0.0003 batch: 4
epochs: 14 train loss: 0.08977830700213539  train f1: 0.88133  lr: 0.0003 batch: 4 
epochs: 14 val loss: 3.788680364116889  val f1:0.52320  lr: 0.0003 batch: 4
epochs: 15 train loss: 0.07086154913411169  train f1: 0.90787  lr: 0.0003 batch: 4 
epochs: 15 val loss: 4.048988336092466  val f1:0.51921  lr: 0.0003 batch: 4
epochs: 16 train loss: 0.07660523821575364  train f1: 0.89627  lr: 0.0003 batch: 4 
epochs: 16 val loss: 3.81562503790811  val f1:0.55180  lr: 0.0003 batch: 4
epochs: 17 train loss: 0.06749247974447536  train f1: 0.92007  lr: 0.0003 batch: 4 
epochs: 17 val loss: 3.9689858952789465  val f1:0.53589  lr: 0.0003 batch: 4
epochs: 18 train loss: 0.07078208297156224  train f1: 0.92570  lr: 0.0003 batch: 4 
epochs: 18 val loss: 4.741200777064448  val f1:0.53657  lr: 0.0003 batch: 4
epochs: 19 train loss: 0.06290206999591226  train f1: 0.92877  lr: 0.0003 batch: 4 
epochs: 19 val loss: 3.9385106824545772  val f1:0.54487  lr: 0.0003 batch: 4
epochs: 20 train loss: 0.06856498317548847  train f1: 0.93235  lr: 0.0003 batch: 4 
epochs: 20 val loss: 4.656897594844697  val f1:0.54794  lr: 0.0003 batch: 4
epochs: 21 train loss: 0.06916752463199674  train f1: 0.92783  lr: 0.0003 batch: 4 
epochs: 21 val loss: 4.129898244352642  val f1:0.53050  lr: 0.0003 batch: 4
epochs: 0 train loss: 1.1508761345223992  train f1: 0.16275  lr: 0.0001 batch: 4 
epochs: 0 val loss: 2.679467321548038  val f1:0.16584  lr: 0.0001 batch: 4
epochs: 1 train loss: 0.6378054243794979  train f1: 0.27009  lr: 0.0001 batch: 4 
epochs: 1 val loss: 2.16816201855831  val f1:0.25996  lr: 0.0001 batch: 4
epochs: 2 train loss: 0.4651463255007162  train f1: 0.40888  lr: 0.0001 batch: 4 
epochs: 2 val loss: 1.731726948980498  val f1:0.36584  lr: 0.0001 batch: 4
epochs: 3 train loss: 0.3579399433921789  train f1: 0.51166  lr: 0.0001 batch: 4 
epochs: 3 val loss: 1.7234858723429884  val f1:0.44250  lr: 0.0001 batch: 4
epochs: 4 train loss: 0.2635851736818807  train f1: 0.64132  lr: 0.0001 batch: 4 
epochs: 4 val loss: 1.8999227123932836  val f1:0.48590  lr: 0.0001 batch: 4
epochs: 5 train loss: 0.22003425551710926  train f1: 0.70854  lr: 0.0001 batch: 4 
epochs: 5 val loss: 2.0040845464023445  val f1:0.52186  lr: 0.0001 batch: 4
epochs: 6 train loss: 0.15844955716686734  train f1: 0.79108  lr: 0.0001 batch: 4 
epochs: 6 val loss: 2.2314487874839646  val f1:0.52012  lr: 0.0001 batch: 4
epochs: 7 train loss: 0.13346779279494544  train f1: 0.83055  lr: 0.0001 batch: 4 
epochs: 7 val loss: 2.118015248611819  val f1:0.56383  lr: 0.0001 batch: 4
epochs: 8 train loss: 0.10440934362929433  train f1: 0.87003  lr: 0.0001 batch: 4 
epochs: 8 val loss: 2.4061263248960243  val f1:0.55893  lr: 0.0001 batch: 4
epochs: 9 train loss: 0.08374130312869615  train f1: 0.90794  lr: 0.0001 batch: 4 
epochs: 9 val loss: 2.3279508005046656  val f1:0.53396  lr: 0.0001 batch: 4
epochs: 10 train loss: 0.09330895799822564  train f1: 0.89546  lr: 0.0001 batch: 4 
epochs: 10 val loss: 2.508694447480241  val f1:0.58971  lr: 0.0001 batch: 4
epochs: 11 train loss: 0.06809695757060455  train f1: 0.91446  lr: 0.0001 batch: 4 
epochs: 11 val loss: 2.5834147217985874  val f1:0.57009  lr: 0.0001 batch: 4
epochs: 12 train loss: 0.05585191379325662  train f1: 0.93824  lr: 0.0001 batch: 4 
epochs: 12 val loss: 2.59697712883215  val f1:0.62130  lr: 0.0001 batch: 4
epochs: 13 train loss: 0.04537100180258016  train f1: 0.95325  lr: 0.0001 batch: 4 
epochs: 13 val loss: 2.9501760125381358  val f1:0.59761  lr: 0.0001 batch: 4
epochs: 14 train loss: 0.051264359430873804  train f1: 0.93909  lr: 0.0001 batch: 4 
epochs: 14 val loss: 2.8420925781766657  val f1:0.59531  lr: 0.0001 batch: 4
epochs: 15 train loss: 0.03952773573916502  train f1: 0.96076  lr: 0.0001 batch: 4 
epochs: 15 val loss: 2.856720099312032  val f1:0.59721  lr: 0.0001 batch: 4
epochs: 16 train loss: 0.03882734762148908  train f1: 0.95612  lr: 0.0001 batch: 4 
epochs: 16 val loss: 2.9116348315700735  val f1:0.58832  lr: 0.0001 batch: 4
epochs: 17 train loss: 0.031597131981831825  train f1: 0.95841  lr: 0.0001 batch: 4 
epochs: 17 val loss: 3.240382300900614  val f1:0.59335  lr: 0.0001 batch: 4
epochs: 18 train loss: 0.029708931117915022  train f1: 0.96761  lr: 0.0001 batch: 4 
epochs: 18 val loss: 3.5955222163881606  val f1:0.56678  lr: 0.0001 batch: 4
epochs: 19 train loss: 0.023736791385247043  train f1: 0.97017  lr: 0.0001 batch: 4 
epochs: 19 val loss: 3.1556691583764342  val f1:0.59814  lr: 0.0001 batch: 4
epochs: 20 train loss: 0.044927513945415234  train f1: 0.95581  lr: 0.0001 batch: 4 
epochs: 20 val loss: 3.2029838281128957  val f1:0.59155  lr: 0.0001 batch: 4
epochs: 0 train loss: 0.9370809526585823  train f1: 0.23915  lr: 0.0003 batch: 32 
epochs: 0 val loss: 2.3006232766544117  val f1:0.24175  lr: 0.0003 batch: 32
epochs: 1 train loss: 0.4267947068854946  train f1: 0.43515  lr: 0.0003 batch: 32 
epochs: 1 val loss: 2.10671817555147  val f1:0.35923  lr: 0.0003 batch: 32
epochs: 2 train loss: 0.26888024629052004  train f1: 0.61223  lr: 0.0003 batch: 32 
epochs: 2 val loss: 1.7737103630514706  val f1:0.45778  lr: 0.0003 batch: 32
epochs: 3 train loss: 0.17074858252681896  train f1: 0.75654  lr: 0.0003 batch: 32 
epochs: 3 val loss: 1.6049373851102937  val f1:0.56547  lr: 0.0003 batch: 32
epochs: 4 train loss: 0.12134719962504377  train f1: 0.82459  lr: 0.0003 batch: 32 
epochs: 4 val loss: 1.7640165441176465  val f1:0.55444  lr: 0.0003 batch: 32
epochs: 5 train loss: 0.1066550069780492  train f1: 0.87050  lr: 0.0003 batch: 32 
epochs: 5 val loss: 1.7554644416360297  val f1:0.54355  lr: 0.0003 batch: 32
epochs: 6 train loss: 0.06432762430674993  train f1: 0.91369  lr: 0.0003 batch: 32 
epochs: 6 val loss: 1.7483197380514706  val f1:0.60448  lr: 0.0003 batch: 32
epochs: 7 train loss: 0.0582980611431065  train f1: 0.93547  lr: 0.0003 batch: 32 
epochs: 7 val loss: 1.9175666360294124  val f1:0.60897  lr: 0.0003 batch: 32
epochs: 8 train loss: 0.05125587377975238  train f1: 0.93021  lr: 0.0003 batch: 32 
epochs: 8 val loss: 2.077722886029412  val f1:0.59539  lr: 0.0003 batch: 32
epochs: 9 train loss: 0.023501937069110013  train f1: 0.96827  lr: 0.0003 batch: 32 
epochs: 9 val loss: 2.0409007352941178  val f1:0.59506  lr: 0.0003 batch: 32
epochs: 10 train loss: 0.028695590460478388  train f1: 0.96692  lr: 0.0003 batch: 32 
epochs: 10 val loss: 2.2898092830882355  val f1:0.60061  lr: 0.0003 batch: 32
epochs: 11 train loss: 0.03664601738773178  train f1: 0.96901  lr: 0.0003 batch: 32 
epochs: 11 val loss: 1.9593936695772065  val f1:0.63802  lr: 0.0003 batch: 32
epochs: 12 train loss: 0.03212591783324285  train f1: 0.95874  lr: 0.0003 batch: 32 
epochs: 12 val loss: 2.1458237591911757  val f1:0.58125  lr: 0.0003 batch: 32
epochs: 13 train loss: 0.03226869141877587  train f1: 0.95766  lr: 0.0003 batch: 32 
epochs: 13 val loss: 2.0868494370404416  val f1:0.62615  lr: 0.0003 batch: 32
epochs: 14 train loss: 0.027764692235348835  train f1: 0.97350  lr: 0.0003 batch: 32 
epochs: 14 val loss: 2.0645249310661766  val f1:0.61481  lr: 0.0003 batch: 32
epochs: 15 train loss: 0.01533317743842282  train f1: 0.98105  lr: 0.0003 batch: 32 
epochs: 15 val loss: 2.2384894875919112  val f1:0.60821  lr: 0.0003 batch: 32
epochs: 16 train loss: 0.02046476015404089  train f1: 0.97592  lr: 0.0003 batch: 32 
epochs: 16 val loss: 2.1796731387867645  val f1:0.63077  lr: 0.0003 batch: 32
epochs: 17 train loss: 0.046349187395465924  train f1: 0.96174  lr: 0.0003 batch: 32 
epochs: 17 val loss: 2.2286017922794126  val f1:0.60256  lr: 0.0003 batch: 32
epochs: 18 train loss: 0.038456135721349004  train f1: 0.95578  lr: 0.0003 batch: 32 
epochs: 18 val loss: 2.3467514935661757  val f1:0.60162  lr: 0.0003 batch: 32
epochs: 19 train loss: 0.0489051733443986  train f1: 0.95113  lr: 0.0003 batch: 32 
epochs: 19 val loss: 2.540225758272059  val f1:0.58566  lr: 0.0003 batch: 32
epochs: 20 train loss: 0.04177032299895785  train f1: 0.94829  lr: 0.0003 batch: 32 
epochs: 20 val loss: 2.428294462316177  val f1:0.58628  lr: 0.0003 batch: 32
epochs: 21 train loss: 0.04020381151740231  train f1: 0.95472  lr: 0.0003 batch: 32 
epochs: 21 val loss: 2.4530101102941173  val f1:0.61298  lr: 0.0003 batch: 32
epochs: 0 train loss: 0.9386787983908585  train f1: 0.22010  lr: 0.0003 batch: 32 
epochs: 0 val loss: 1.892506318933824  val f1:0.25977  lr: 0.0003 batch: 32
epochs: 0 train loss: 6.546875  train f1: 0.00000  lr: 0.0003 batch: 32 
epochs: 0 val loss: 6.3515625  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 1 train loss: 5.80859375  train f1: 0.00000  lr: 0.0003 batch: 32 
epochs: 1 val loss: 6.3515625  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 2 train loss: 7.05859375  train f1: 0.00000  lr: 0.0003 batch: 32 
epochs: 2 val loss: 6.3515625  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 3 train loss: 5.96484375  train f1: 0.00000  lr: 0.0003 batch: 32 
epochs: 3 val loss: 6.3515625  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 4 train loss: 5.87890625  train f1: 0.00000  lr: 0.0003 batch: 32 
epochs: 4 val loss: 13.9375  val f1:0.06667  lr: 0.0003 batch: 32
epochs: 5 train loss: 2.3046875  train f1: 0.22222  lr: 0.0003 batch: 32 
epochs: 5 val loss: 16.453125  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 6 train loss: 6.8828125  train f1: 0.50000  lr: 0.0003 batch: 32 
epochs: 6 val loss: 10.34375  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 7 train loss: 3.5703125  train f1: 0.16667  lr: 0.0003 batch: 32 
epochs: 7 val loss: 10.5546875  val f1:0.06667  lr: 0.0003 batch: 32
epochs: 8 train loss: 0.83984375  train f1: 0.60000  lr: 0.0003 batch: 32 
epochs: 8 val loss: 10.125  val f1:0.08333  lr: 0.0003 batch: 32
epochs: 9 train loss: 0.00083160400390625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 9 val loss: 9.46875  val f1:0.08333  lr: 0.0003 batch: 32
epochs: 10 train loss: 0.0004069805145263672  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 10 val loss: 9.03125  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 11 train loss: 0.0003848075866699219  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 11 val loss: 9.046875  val f1:0.16667  lr: 0.0003 batch: 32
epochs: 12 train loss: 0.00041937828063964844  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 12 val loss: 9.9921875  val f1:0.16667  lr: 0.0003 batch: 32
epochs: 13 train loss: 0.0008440017700195312  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 13 val loss: 10.9296875  val f1:0.16667  lr: 0.0003 batch: 32
epochs: 14 train loss: 1.263671875  train f1: 0.77778  lr: 0.0003 batch: 32 
epochs: 14 val loss: 13.4765625  val f1:0.06667  lr: 0.0003 batch: 32
epochs: 0 train loss: 4.5703125  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.77734375  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 1 train loss: 4.66796875  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 1 val loss: 4.8125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 2 train loss: 2.072265625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 2 val loss: 4.76953125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 3 train loss: 1.126953125  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 3 val loss: 4.80859375  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 4 train loss: 0.322021484375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 4 val loss: 4.8125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 5 train loss: 0.10198974609375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 5 val loss: 4.796875  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 6 train loss: 0.0229034423828125  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 6 val loss: 4.86328125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 7 train loss: 0.0158538818359375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 7 val loss: 4.734375  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 8 train loss: 0.00637054443359375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 8 val loss: 4.60546875  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 9 train loss: 0.006900787353515625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 9 val loss: 4.5625  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 10 train loss: 0.0037746429443359375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 10 val loss: 4.546875  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 11 train loss: 0.002445220947265625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 11 val loss: 4.51171875  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 12 train loss: 0.0015010833740234375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 12 val loss: 4.34765625  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 13 train loss: 0.0011987686157226562  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 13 val loss: 4.29296875  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 14 train loss: 0.0013036727905273438  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 14 val loss: 4.3203125  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 15 train loss: 0.0009393692016601562  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 15 val loss: 4.3515625  val f1:0.09524  lr: 0.0003 batch: 16
epochs: 16 train loss: 0.0007634162902832031  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 16 val loss: 4.53125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 17 train loss: 0.0005555152893066406  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 17 val loss: 4.48046875  val f1:0.08333  lr: 0.0003 batch: 16
epochs: 18 train loss: 0.00043582916259765625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 18 val loss: 4.23828125  val f1:0.08333  lr: 0.0003 batch: 16
epochs: 19 train loss: 0.0005102157592773438  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 19 val loss: 3.943359375  val f1:0.08333  lr: 0.0003 batch: 16
epochs: 20 train loss: 0.0004031658172607422  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 20 val loss: 3.876953125  val f1:0.08333  lr: 0.0003 batch: 16
epochs: 21 train loss: 0.0003218650817871094  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 21 val loss: 3.787109375  val f1:0.08333  lr: 0.0003 batch: 16
epochs: 22 train loss: 0.00030159950256347656  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 22 val loss: 3.705078125  val f1:0.08333  lr: 0.0003 batch: 16
epochs: 0 train loss: 1.5599036813896394  train f1: 0.12328  lr: 0.0003 batch: 4 
epochs: 0 val loss: 2.7962611946856173  val f1:0.09519  lr: 0.0003 batch: 4
epochs: 1 train loss: 0.9855993431305223  train f1: 0.15837  lr: 0.0003 batch: 4 
epochs: 1 val loss: 3.3320108105831125  val f1:0.07592  lr: 0.0003 batch: 4
epochs: 2 train loss: 0.88354734616859  train f1: 0.16786  lr: 0.0003 batch: 4 
epochs: 2 val loss: 3.3095123913823348  val f1:0.10439  lr: 0.0003 batch: 4
epochs: 0 train loss: 4.02734375  train f1: 0.00000  lr: 0.0003 batch: 4 
epochs: 0 val loss: 4.5703125  val f1:0.00000  lr: 0.0003 batch: 4
epochs: 1 train loss: 4.16015625  train f1: 0.00000  lr: 0.0003 batch: 4 
epochs: 1 val loss: 4.7734375  val f1:0.00000  lr: 0.0003 batch: 4
epochs: 2 train loss: 2.234375  train f1: 1.00000  lr: 0.0003 batch: 4 
epochs: 2 val loss: 4.92578125  val f1:0.00000  lr: 0.0003 batch: 4
epochs: 3 train loss: 1.189453125  train f1: 1.00000  lr: 0.0003 batch: 4 
epochs: 3 val loss: 5.00390625  val f1:0.00000  lr: 0.0003 batch: 4
epochs: 4 train loss: 0.435546875  train f1: 1.00000  lr: 0.0003 batch: 4 
epochs: 4 val loss: 5.052734375  val f1:0.00000  lr: 0.0003 batch: 4
epochs: 5 train loss: 0.11181640625  train f1: 1.00000  lr: 0.0003 batch: 4 
epochs: 5 val loss: 5.1171875  val f1:0.00000  lr: 0.0003 batch: 4
epochs: 6 train loss: 0.060791015625  train f1: 1.00000  lr: 0.0003 batch: 4 
epochs: 6 val loss: 5.203125  val f1:0.00000  lr: 0.0003 batch: 4
epochs: 7 train loss: 0.027923583984375  train f1: 1.00000  lr: 0.0003 batch: 4 
epochs: 7 val loss: 5.283203125  val f1:0.00000  lr: 0.0003 batch: 4
epochs: 8 train loss: 0.0192718505859375  train f1: 1.00000  lr: 0.0003 batch: 4 
epochs: 8 val loss: 5.373046875  val f1:0.00000  lr: 0.0003 batch: 4
epochs: 9 train loss: 0.0165252685546875  train f1: 1.00000  lr: 0.0003 batch: 4 
epochs: 9 val loss: 5.36328125  val f1:0.00000  lr: 0.0003 batch: 4
epochs: 10 train loss: 0.013427734375  train f1: 1.00000  lr: 0.0003 batch: 4 
epochs: 10 val loss: 5.341796875  val f1:0.00000  lr: 0.0003 batch: 4
epochs: 11 train loss: 0.00617218017578125  train f1: 1.00000  lr: 0.0003 batch: 4 
epochs: 11 val loss: 5.294921875  val f1:0.14286  lr: 0.0003 batch: 4
epochs: 12 train loss: 0.007659912109375  train f1: 1.00000  lr: 0.0003 batch: 4 
epochs: 12 val loss: 5.28125  val f1:0.14286  lr: 0.0003 batch: 4
epochs: 13 train loss: 0.005405426025390625  train f1: 1.00000  lr: 0.0003 batch: 4 
epochs: 13 val loss: 5.234375  val f1:0.00000  lr: 0.0003 batch: 4
epochs: 14 train loss: 0.003253936767578125  train f1: 1.00000  lr: 0.0003 batch: 4 
epochs: 14 val loss: 5.16015625  val f1:0.07143  lr: 0.0003 batch: 4
epochs: 15 train loss: 0.0022869110107421875  train f1: 1.00000  lr: 0.0003 batch: 4 
epochs: 15 val loss: 5.1640625  val f1:0.00000  lr: 0.0003 batch: 4
epochs: 16 train loss: 0.0015268325805664062  train f1: 1.00000  lr: 0.0003 batch: 4 
epochs: 16 val loss: 5.287109375  val f1:0.00000  lr: 0.0003 batch: 4
epochs: 17 train loss: 0.001033782958984375  train f1: 1.00000  lr: 0.0003 batch: 4 
epochs: 17 val loss: 5.275390625  val f1:0.00000  lr: 0.0003 batch: 4
epochs: 18 train loss: 0.0006265640258789062  train f1: 1.00000  lr: 0.0003 batch: 4 
epochs: 18 val loss: 5.21875  val f1:0.00000  lr: 0.0003 batch: 4
epochs: 19 train loss: 0.0005726814270019531  train f1: 1.00000  lr: 0.0003 batch: 4 
epochs: 19 val loss: 5.15234375  val f1:0.00000  lr: 0.0003 batch: 4
epochs: 20 train loss: 0.0005993843078613281  train f1: 1.00000  lr: 0.0003 batch: 4 
epochs: 20 val loss: 5.14453125  val f1:0.00000  lr: 0.0003 batch: 4
epochs: 21 train loss: 0.0004298686981201172  train f1: 1.00000  lr: 0.0003 batch: 4 
epochs: 21 val loss: 5.107421875  val f1:0.00000  lr: 0.0003 batch: 4
epochs: 22 train loss: 0.00031828880310058594  train f1: 1.00000  lr: 0.0003 batch: 4 
epochs: 22 val loss: 5.064453125  val f1:0.00000  lr: 0.0003 batch: 4
epochs: 23 train loss: 0.00022339820861816406  train f1: 1.00000  lr: 0.0003 batch: 4 
epochs: 23 val loss: 5.01953125  val f1:0.09524  lr: 0.0003 batch: 4
epochs: 24 train loss: 0.0002288818359375  train f1: 1.00000  lr: 0.0003 batch: 4 
epochs: 24 val loss: 4.7392578125  val f1:0.09524  lr: 0.0003 batch: 4
epochs: 25 train loss: 0.0002434253692626953  train f1: 1.00000  lr: 0.0003 batch: 4 
epochs: 25 val loss: 4.6025390625  val f1:0.09524  lr: 0.0003 batch: 4

########### augmentation train_with_labelwithOut_metalnut_pill_toothbrush_aug #################
epochs: 0 train loss: 0.7426189672167054  train f1: 0.31667  lr: 0.0003 batch: 16 
epochs: 0 val loss: 1.146787290219907  val f1:0.46860  lr: 0.0003 batch: 16
epochs: 1 train loss: 0.2758141704808884  train f1: 0.63318  lr: 0.0003 batch: 16 
epochs: 1 val loss: 0.8122422960069443  val f1:0.59841  lr: 0.0003 batch: 16
epochs: 2 train loss: 0.1687566775027835  train f1: 0.77006  lr: 0.0003 batch: 16 
epochs: 2 val loss: 0.7794652868200238  val f1:0.69173  lr: 0.0003 batch: 16
epochs: 3 train loss: 0.1084519680415358  train f1: 0.85574  lr: 0.0003 batch: 16 
epochs: 3 val loss: 0.8887739393446172  val f1:0.68060  lr: 0.0003 batch: 16
epochs: 4 train loss: 0.08597733863046235  train f1: 0.89514  lr: 0.0003 batch: 16 
epochs: 4 val loss: 0.7094391999421298  val f1:0.76174  lr: 0.0003 batch: 16
epochs: 5 train loss: 0.06534824371337897  train f1: 0.91603  lr: 0.0003 batch: 16 
epochs: 5 val loss: 0.684400544343171  val f1:0.78422  lr: 0.0003 batch: 16
epochs: 6 train loss: 0.07330755429847213  train f1: 0.92319  lr: 0.0003 batch: 16 
epochs: 6 val loss: 0.7639503973501697  val f1:0.76220  lr: 0.0003 batch: 16
epochs: 7 train loss: 0.04473781763950247  train f1: 0.94963  lr: 0.0003 batch: 16 
epochs: 7 val loss: 0.7152439117431635  val f1:0.80172  lr: 0.0003 batch: 16
epochs: 8 train loss: 0.04592859187972877  train f1: 0.95082  lr: 0.0003 batch: 16 
epochs: 8 val loss: 0.8235767929642288  val f1:0.76096  lr: 0.0003 batch: 16
epochs: 9 train loss: 0.04693539499122399  train f1: 0.94260  lr: 0.0003 batch: 16 
epochs: 9 val loss: 1.025586841724537  val f1:0.71547  lr: 0.0003 batch: 16
epochs: 10 train loss: 0.049258921525188716  train f1: 0.94734  lr: 0.0003 batch: 16 
epochs: 10 val loss: 0.920575685854311  val f1:0.75435  lr: 0.0003 batch: 16
epochs: 11 train loss: 0.051799240067740455  train f1: 0.93962  lr: 0.0003 batch: 16 
epochs: 11 val loss: 0.9693947120949077  val f1:0.74495  lr: 0.0003 batch: 16
epochs: 12 train loss: 0.03590923157807825  train f1: 0.95729  lr: 0.0003 batch: 16 
epochs: 12 val loss: 0.75542879457827  val f1:0.79581  lr: 0.0003 batch: 16
epochs: 13 train loss: 0.024902431652924724  train f1: 0.98101  lr: 0.0003 batch: 16 
epochs: 13 val loss: 0.9582926291006579  val f1:0.77209  lr: 0.0003 batch: 16
epochs: 14 train loss: 0.034730818895536  train f1: 0.96676  lr: 0.0003 batch: 16 
epochs: 14 val loss: 0.9035884751213923  val f1:0.78182  lr: 0.0003 batch: 16
epochs: 15 train loss: 0.03376554752064644  train f1: 0.96757  lr: 0.0003 batch: 16 
epochs: 15 val loss: 0.9497984585938629  val f1:0.76933  lr: 0.0003 batch: 16
epochs: 16 train loss: 0.03213288338384892  train f1: 0.96817  lr: 0.0003 batch: 16 
epochs: 16 val loss: 0.9135829501681854  val f1:0.78824  lr: 0.0003 batch: 16
epochs: 17 train loss: 0.026759671273632563  train f1: 0.97158  lr: 0.0003 batch: 16 
epochs: 17 val loss: 1.0374767374109342  val f1:0.76691  lr: 0.0003 batch: 16
epochs: 18 train loss: 0.02829478299506357  train f1: 0.96292  lr: 0.0003 batch: 16 
epochs: 18 val loss: 0.8371459678367331  val f1:0.77909  lr: 0.0003 batch: 16
epochs: 19 train loss: 0.03768770371642064  train f1: 0.96702  lr: 0.0003 batch: 16 
epochs: 19 val loss: 0.8427574440285011  val f1:0.78404  lr: 0.0003 batch: 16
epochs: 20 train loss: 0.016024260097574974  train f1: 0.98262  lr: 0.0003 batch: 16 
epochs: 20 val loss: 1.0155143596507887  val f1:0.78732  lr: 0.0003 batch: 16
epochs: 0 train loss: 0.741341889794193  train f1: 0.32585  lr: 0.0003 batch: 32 
epochs: 0 val loss: 1.273236443014706  val f1:0.46227  lr: 0.0003 batch: 32
epochs: 1 train loss: 0.27596197555314256  train f1: 0.64203  lr: 0.0003 batch: 32 
epochs: 1 val loss: 0.8366986443014706  val f1:0.62037  lr: 0.0003 batch: 32
epochs: 2 train loss: 0.14450420550446025  train f1: 0.79684  lr: 0.0003 batch: 32 
epochs: 2 val loss: 0.6646943933823531  val f1:0.70607  lr: 0.0003 batch: 32
epochs: 3 train loss: 0.09595416908833516  train f1: 0.87546  lr: 0.0003 batch: 32 
epochs: 3 val loss: 0.8733251235064341  val f1:0.69335  lr: 0.0003 batch: 32
epochs: 4 train loss: 0.0707050223848713  train f1: 0.91283  lr: 0.0003 batch: 32 
epochs: 4 val loss: 0.7264743131749771  val f1:0.74991  lr: 0.0003 batch: 32
epochs: 5 train loss: 0.050882962212633706  train f1: 0.93365  lr: 0.0003 batch: 32 
epochs: 5 val loss: 0.7481020759133729  val f1:0.76068  lr: 0.0003 batch: 32
epochs: 6 train loss: 0.03107498652899443  train f1: 0.95296  lr: 0.0003 batch: 32 
epochs: 6 val loss: 0.6897145439596738  val f1:0.78651  lr: 0.0003 batch: 32
epochs: 7 train loss: 0.03216657175946592  train f1: 0.96404  lr: 0.0003 batch: 32 
epochs: 7 val loss: 0.8030987907858455  val f1:0.74634  lr: 0.0003 batch: 32
epochs: 8 train loss: 0.04125507198162932  train f1: 0.95373  lr: 0.0003 batch: 32 
epochs: 8 val loss: 0.6506872738108916  val f1:0.78174  lr: 0.0003 batch: 32
epochs: 9 train loss: 0.029336970243880992  train f1: 0.97005  lr: 0.0003 batch: 32 
epochs: 9 val loss: 0.7829953361960021  val f1:0.77450  lr: 0.0003 batch: 32
epochs: 10 train loss: 0.03012492585538037  train f1: 0.97012  lr: 0.0003 batch: 32 
epochs: 10 val loss: 0.6656541263355928  val f1:0.80003  lr: 0.0003 batch: 32
epochs: 11 train loss: 0.03147942717395612  train f1: 0.97340  lr: 0.0003 batch: 32 
epochs: 11 val loss: 0.6749662511488971  val f1:0.79335  lr: 0.0003 batch: 32
epochs: 12 train loss: 0.03608590542380493  train f1: 0.96146  lr: 0.0003 batch: 32 
epochs: 12 val loss: 0.7439305922564341  val f1:0.78543  lr: 0.0003 batch: 32
epochs: 13 train loss: 0.03741591412629654  train f1: 0.96034  lr: 0.0003 batch: 32 
epochs: 13 val loss: 0.7952303044936235  val f1:0.74828  lr: 0.0003 batch: 32
epochs: 14 train loss: 0.028718447507317387  train f1: 0.96569  lr: 0.0003 batch: 32 
epochs: 14 val loss: 0.8392902823055495  val f1:0.77756  lr: 0.0003 batch: 32
epochs: 15 train loss: 0.019148608197027177  train f1: 0.98172  lr: 0.0003 batch: 32 
epochs: 15 val loss: 0.8211617434726037  val f1:0.80156  lr: 0.0003 batch: 32
epochs: 16 train loss: 0.012776784932435456  train f1: 0.99197  lr: 0.0003 batch: 32 
epochs: 16 val loss: 0.849616092794082  val f1:0.79114  lr: 0.0003 batch: 32
epochs: 17 train loss: 0.024738330449630967  train f1: 0.97925  lr: 0.0003 batch: 32 
epochs: 17 val loss: 1.0231628417968746  val f1:0.74646  lr: 0.0003 batch: 32
epochs: 18 train loss: 0.028629007624156436  train f1: 0.97467  lr: 0.0003 batch: 32 
epochs: 18 val loss: 0.8684647504021137  val f1:0.77960  lr: 0.0003 batch: 32
epochs: 19 train loss: 0.03443318761106747  train f1: 0.96315  lr: 0.0003 batch: 32 
epochs: 19 val loss: 0.8431728587431063  val f1:0.78238  lr: 0.0003 batch: 32
epochs: 20 train loss: 0.011669656011595652  train f1: 0.98800  lr: 0.0003 batch: 32 
epochs: 20 val loss: 0.8651302562040443  val f1:0.77550  lr: 0.0003 batch: 32
epochs: 0 train loss: 0.7629532002957063  train f1: 0.31091  lr: 0.0003 batch: 16 
epochs: 0 val loss: 1.2748300057870368  val f1:0.42775  lr: 0.0003 batch: 16
epochs: 1 train loss: 0.29008661430572813  train f1: 0.60754  lr: 0.0003 batch: 16 
epochs: 0 train loss: 4.96875  train f1: 0.00000  lr: 0.0003 batch: 32 
epochs: 0 val loss: 4.3671875  val f1:0.06667  lr: 0.0003 batch: 32
epochs: 1 train loss: 5.0078125  train f1: 0.00000  lr: 0.0003 batch: 32 
epochs: 1 val loss: 4.0703125  val f1:0.07143  lr: 0.0003 batch: 32
epochs: 2 train loss: 2.705078125  train f1: 0.66667  lr: 0.0003 batch: 32 
epochs: 2 val loss: 4.0234375  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 3 train loss: 1.2119140625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 3 val loss: 4.078125  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 4 train loss: 0.387939453125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 4 val loss: 4.13671875  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 5 train loss: 0.200927734375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 5 val loss: 4.171875  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 6 train loss: 0.046630859375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 6 val loss: 4.26953125  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 7 train loss: 0.019683837890625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 7 val loss: 4.31640625  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 8 train loss: 0.01824951171875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 8 val loss: 4.44921875  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 9 train loss: 0.0059051513671875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 9 val loss: 4.59765625  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 10 train loss: 0.006683349609375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 10 val loss: 4.70703125  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 0 train loss: 0.7421614234127206  train f1: 0.34633  lr: 0.0003 batch: 32 
epochs: 0 val loss: 1.0861241957720587  val f1:0.48971  lr: 0.0003 batch: 32
epochs: 1 train loss: 0.2852889103675957  train f1: 0.61995  lr: 0.0003 batch: 32 
epochs: 1 val loss: 0.8846291934742646  val f1:0.63299  lr: 0.0003 batch: 32
epochs: 2 train loss: 0.158851424259926  train f1: 0.79629  lr: 0.0003 batch: 32 
epochs: 2 val loss: 0.7444458007812501  val f1:0.68246  lr: 0.0003 batch: 32
epochs: 3 train loss: 0.09739141321893947  train f1: 0.86734  lr: 0.0003 batch: 32 
epochs: 3 val loss: 0.6136743882123157  val f1:0.76779  lr: 0.0003 batch: 32
epochs: 4 train loss: 0.05996074249495319  train f1: 0.91657  lr: 0.0003 batch: 32 
epochs: 4 val loss: 0.7726781508501841  val f1:0.73038  lr: 0.0003 batch: 32
epochs: 5 train loss: 0.055432177301663055  train f1: 0.92531  lr: 0.0003 batch: 32 
epochs: 5 val loss: 0.791622386259191  val f1:0.72972  lr: 0.0003 batch: 32
epochs: 6 train loss: 0.043760849468743625  train f1: 0.94912  lr: 0.0003 batch: 32 
epochs: 6 val loss: 0.6803122127757354  val f1:0.78268  lr: 0.0003 batch: 32
epochs: 7 train loss: 0.037980768217969314  train f1: 0.95907  lr: 0.0003 batch: 32 
epochs: 7 val loss: 0.8004204245174631  val f1:0.77220  lr: 0.0003 batch: 32
epochs: 8 train loss: 0.047660222694055336  train f1: 0.94421  lr: 0.0003 batch: 32 
epochs: 8 val loss: 0.8503579532398895  val f1:0.75906  lr: 0.0003 batch: 32
epochs: 9 train loss: 0.03931666754964574  train f1: 0.95659  lr: 0.0003 batch: 32 
epochs: 9 val loss: 0.680071662454044  val f1:0.80207  lr: 0.0003 batch: 32
epochs: 10 train loss: 0.026279930748156658  train f1: 0.97526  lr: 0.0003 batch: 32 
epochs: 10 val loss: 0.8555881275850182  val f1:0.77316  lr: 0.0003 batch: 32
epochs: 0 train loss: 0.5935665710905842  train f1: 0.43225  lr: 0.0003 batch: 16 
epochs: 0 val loss: 0.9109879105179404  val f1:0.58286  lr: 0.0003 batch: 16
epochs: 1 train loss: 0.1831508538966759  train f1: 0.76267  lr: 0.0003 batch: 16 
epochs: 1 val loss: 0.7932938187210653  val f1:0.67964  lr: 0.0003 batch: 16
epochs: 2 train loss: 0.10919328163984118  train f1: 0.85258  lr: 0.0003 batch: 16 
epochs: 2 val loss: 0.866852767379196  val f1:0.72315  lr: 0.0003 batch: 16
epochs: 3 train loss: 0.07943437343226409  train f1: 0.89761  lr: 0.0003 batch: 16 
epochs: 3 val loss: 0.730817441587095  val f1:0.76813  lr: 0.0003 batch: 16
epochs: 4 train loss: 0.05541016692829834  train f1: 0.93275  lr: 0.0003 batch: 16 
epochs: 4 val loss: 0.8031697873716004  val f1:0.75932  lr: 0.0003 batch: 16
epochs: 5 train loss: 0.05832175587180848  train f1: 0.93165  lr: 0.0003 batch: 16 
epochs: 5 val loss: 0.827067396375868  val f1:0.75027  lr: 0.0003 batch: 16
epochs: 6 train loss: 0.04795238145271739  train f1: 0.94136  lr: 0.0003 batch: 16 
epochs: 6 val loss: 0.8617149776882593  val f1:0.76082  lr: 0.0003 batch: 16
epochs: 7 train loss: 0.039771255635263895  train f1: 0.96331  lr: 0.0003 batch: 16 
epochs: 7 val loss: 0.960614098442925  val f1:0.76634  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.2734375  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.53125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 1 train loss: 4.25  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 1 val loss: 4.61328125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 2 train loss: 2.06640625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 2 val loss: 4.5  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 3 train loss: 0.85888671875  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 3 val loss: 4.4921875  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 4 train loss: 0.28271484375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 4 val loss: 4.49609375  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 5 train loss: 0.0504150390625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 5 val loss: 4.5625  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 6 train loss: 0.059417724609375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 6 val loss: 4.6015625  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 7 train loss: 0.01983642578125  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 7 val loss: 4.6015625  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 8 train loss: 0.007678985595703125  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 8 val loss: 4.63671875  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 9 train loss: 0.005474090576171875  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 9 val loss: 4.67578125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 10 train loss: 0.0025005340576171875  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 10 val loss: 4.671875  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 11 train loss: 0.00214385986328125  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 11 val loss: 4.66015625  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 12 train loss: 0.0018396377563476562  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 12 val loss: 4.578125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 13 train loss: 0.0011272430419921875  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 13 val loss: 4.61328125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 14 train loss: 0.0008902549743652344  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 14 val loss: 4.265625  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 15 train loss: 0.0005669593811035156  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 15 val loss: 4.0546875  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.2734375  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.53125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 1 train loss: 4.25  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 1 val loss: 4.61328125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 2 train loss: 2.06640625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 2 val loss: 4.5  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 3 train loss: 0.85888671875  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 3 val loss: 4.4921875  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 4 train loss: 0.28271484375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 4 val loss: 4.49609375  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.2734375  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.53125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 1 train loss: 4.25  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 1 val loss: 4.61328125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 2 train loss: 2.06640625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 2 val loss: 4.5  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 3 train loss: 0.85888671875  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 3 val loss: 4.4921875  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 4 train loss: 0.28271484375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 4 val loss: 4.49609375  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.2734375  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.53125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 1 train loss: 4.25  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 1 val loss: 4.61328125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 2 train loss: 2.06640625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 2 val loss: 4.5  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 3 train loss: 0.85888671875  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 3 val loss: 4.4921875  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 4 train loss: 0.28271484375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 4 val loss: 4.49609375  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.2734375  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.53125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 1 train loss: 4.25  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 1 val loss: 4.61328125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 2 train loss: 2.06640625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 2 val loss: 4.5  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 3 train loss: 0.85888671875  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 3 val loss: 4.4921875  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 4 train loss: 0.28271484375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 4 val loss: 4.49609375  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.2734375  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.53125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 1 train loss: 4.25  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 1 val loss: 4.61328125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 2 train loss: 2.06640625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 2 val loss: 4.5  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 3 train loss: 0.85888671875  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 3 val loss: 4.4921875  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 0 train loss: 0.6077506476804204  train f1: 0.42073  lr: 0.0003 batch: 16 
epochs: 0 val loss: 0.9189552589699072  val f1:0.56419  lr: 0.0003 batch: 16
epochs: 1 train loss: 0.18867265791667082  train f1: 0.74041  lr: 0.0003 batch: 16 
epochs: 1 val loss: 0.8559425636574075  val f1:0.67752  lr: 0.0003 batch: 16
epochs: 2 train loss: 0.1068231351952302  train f1: 0.87426  lr: 0.0003 batch: 16 
epochs: 2 val loss: 0.9627536349826391  val f1:0.71637  lr: 0.0003 batch: 16
epochs: 3 train loss: 0.08648594002473982  train f1: 0.90013  lr: 0.0003 batch: 16 
epochs: 3 val loss: 0.7318610297309027  val f1:0.77347  lr: 0.0003 batch: 16
epochs: 4 train loss: 0.056507652537186474  train f1: 0.92462  lr: 0.0003 batch: 16 
epochs: 4 val loss: 0.7570922427707247  val f1:0.78459  lr: 0.0003 batch: 16
epochs: 5 train loss: 0.04720457102592449  train f1: 0.94432  lr: 0.0003 batch: 16 
epochs: 5 val loss: 0.795888378002025  val f1:0.76970  lr: 0.0003 batch: 16
epochs: 6 train loss: 0.042780755166698284  train f1: 0.95143  lr: 0.0003 batch: 16 
epochs: 6 val loss: 0.8986799452039926  val f1:0.75460  lr: 0.0003 batch: 16
epochs: 7 train loss: 0.04558106952177321  train f1: 0.95575  lr: 0.0003 batch: 16 
epochs: 7 val loss: 0.855011607982494  val f1:0.77590  lr: 0.0003 batch: 16
epochs: 8 train loss: 0.03452207515959133  train f1: 0.96447  lr: 0.0003 batch: 16 
epochs: 8 val loss: 0.976939731174045  val f1:0.76550  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.171875  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.51171875  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 1 train loss: 4.2578125  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 1 val loss: 4.59375  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 2 train loss: 2.6640625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 2 val loss: 4.48046875  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 3 train loss: 0.83154296875  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 3 val loss: 4.3984375  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 4 train loss: 0.36767578125  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 4 val loss: 4.41015625  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 0 train loss: 0.5925660739813069  train f1: 0.43497  lr: 0.0003 batch: 16 
epochs: 0 val loss: 1.0249439380787035  val f1:0.56280  lr: 0.0003 batch: 16
epochs: 1 train loss: 0.18950446407099322  train f1: 0.74845  lr: 0.0003 batch: 16 
epochs: 1 val loss: 0.6961042616102427  val f1:0.72565  lr: 0.0003 batch: 16
epochs: 2 train loss: 0.1074034008301999  train f1: 0.85882  lr: 0.0003 batch: 16 
epochs: 2 val loss: 0.8630642361111109  val f1:0.69179  lr: 0.0003 batch: 16
epochs: 3 train loss: 0.07596058084483152  train f1: 0.90875  lr: 0.0003 batch: 16 
epochs: 3 val loss: 0.6985352410210505  val f1:0.76587  lr: 0.0003 batch: 16
epochs: 4 train loss: 0.06185402284536579  train f1: 0.92878  lr: 0.0003 batch: 16 
epochs: 4 val loss: 0.7763913472493488  val f1:0.76666  lr: 0.0003 batch: 16
epochs: 5 train loss: 0.05289173423500726  train f1: 0.94114  lr: 0.0003 batch: 16 
epochs: 5 val loss: 0.8530366685655377  val f1:0.76799  lr: 0.0003 batch: 16
epochs: 6 train loss: 0.04497406815650159  train f1: 0.95033  lr: 0.0003 batch: 16 
epochs: 6 val loss: 0.939899154945656  val f1:0.73993  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.37109375  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.58984375  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 1 train loss: 4.3984375  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 1 val loss: 4.546875  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 2 train loss: 3.69921875  train f1: 0.50000  lr: 0.0003 batch: 16 
epochs: 2 val loss: 4.52734375  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 3 train loss: 2.83984375  train f1: 0.77778  lr: 0.0003 batch: 16 
epochs: 3 val loss: 4.50390625  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 4 train loss: 1.97265625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 4 val loss: 4.4765625  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 5 train loss: 0.77197265625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 5 val loss: 4.4609375  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 6 train loss: 0.421142578125  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 6 val loss: 4.421875  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 7 train loss: 0.2099609375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 7 val loss: 4.36328125  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 8 train loss: 0.0311737060546875  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 8 val loss: 4.2890625  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 9 train loss: 0.016143798828125  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 9 val loss: 4.21484375  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 10 train loss: 0.0038547515869140625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 10 val loss: 4.140625  val f1:0.14286  lr: 0.0003 batch: 16
epochs: 0 train loss: 0.5229104974323371  train f1: 0.49571  lr: 0.0003 batch: 8 
epochs: 0 val loss: 0.8274096594916449  val f1:0.63622  lr: 0.0003 batch: 8
epochs: 1 train loss: 0.14428013934756173  train f1: 0.80773  lr: 0.0003 batch: 8 
epochs: 1 val loss: 0.6924269499602147  val f1:0.73598  lr: 0.0003 batch: 8
epochs: 2 train loss: 0.08950027600488167  train f1: 0.88931  lr: 0.0003 batch: 8 
epochs: 2 val loss: 0.7084198792775472  val f1:0.76666  lr: 0.0003 batch: 8
epochs: 3 train loss: 0.055237184661879456  train f1: 0.93509  lr: 0.0003 batch: 8 
epochs: 3 val loss: 0.7334385024176704  val f1:0.77845  lr: 0.0003 batch: 8
epochs: 4 train loss: 0.051866286301850736  train f1: 0.94117  lr: 0.0003 batch: 8 
epochs: 4 val loss: 0.939376217347604  val f1:0.76498  lr: 0.0003 batch: 8
epochs: 5 train loss: 0.03325721242481333  train f1: 0.96528  lr: 0.0003 batch: 8 
epochs: 5 val loss: 0.8029389142990111  val f1:0.80035  lr: 0.0003 batch: 8
epochs: 6 train loss: 0.031578920725872366  train f1: 0.97044  lr: 0.0003 batch: 8 
epochs: 6 val loss: 0.9715855059800321  val f1:0.77385  lr: 0.0003 batch: 8
epochs: 0 train loss: 0.6001118115356151  train f1: 0.42592  lr: 0.0003 batch: 16 
epochs: 0 val loss: 1.0125144675925921  val f1:0.56025  lr: 0.0003 batch: 16
epochs: 1 train loss: 0.1940827167539526  train f1: 0.74151  lr: 0.0003 batch: 16 
epochs: 1 val loss: 0.7339251482928238  val f1:0.69116  lr: 0.0003 batch: 16
epochs: 2 train loss: 0.11832599449633374  train f1: 0.84655  lr: 0.0003 batch: 16 
epochs: 2 val loss: 0.7760041413483794  val f1:0.73861  lr: 0.0003 batch: 16
epochs: 3 train loss: 0.07618955186477627  train f1: 0.89980  lr: 0.0003 batch: 16 
epochs: 3 val loss: 0.742197757297092  val f1:0.75000  lr: 0.0003 batch: 16
epochs: 4 train loss: 0.06542371127968111  train f1: 0.92713  lr: 0.0003 batch: 16 
epochs: 4 val loss: 0.8031689396610968  val f1:0.77385  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.22265625  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.5390625  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 1 train loss: 4.22265625  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 1 val loss: 4.6328125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 2 train loss: 4.30859375  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 2 val loss: 4.5390625  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 3 train loss: 2.0234375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 3 val loss: 4.50390625  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 4 train loss: 0.43115234375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 4 val loss: 4.4921875  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 5 train loss: 0.325439453125  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 5 val loss: 4.54296875  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 6 train loss: 0.1552734375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 6 val loss: 4.65625  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.22265625  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.5390625  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 1 train loss: 4.22265625  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 1 val loss: 4.6328125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 2 train loss: 4.30859375  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 2 val loss: 4.5390625  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 3 train loss: 2.0234375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 3 val loss: 4.50390625  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 4 train loss: 0.43115234375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 4 val loss: 4.4921875  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.22265625  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.5390625  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 1 train loss: 4.22265625  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 1 val loss: 4.6328125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 2 train loss: 4.30859375  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 2 val loss: 4.5390625  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 3 train loss: 2.0234375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 3 val loss: 4.50390625  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 4 train loss: 0.43115234375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 4 val loss: 4.4921875  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 5 train loss: 0.325439453125  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 5 val loss: 4.54296875  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.30859375  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.4453125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 1 train loss: 4.26171875  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 1 val loss: 4.53515625  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 2 train loss: 2.6328125  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 2 val loss: 4.4375  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 3 train loss: 1.263671875  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 3 val loss: 4.40625  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 4 train loss: 0.349365234375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 4 val loss: 4.37109375  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 5 train loss: 0.144287109375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 5 val loss: 4.3359375  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 6 train loss: 0.0491943359375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 6 val loss: 4.34765625  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 7 train loss: 0.019378662109375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 7 val loss: 4.3984375  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.30859375  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.4453125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 1 train loss: 4.26171875  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 1 val loss: 4.53515625  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 2 train loss: 2.6328125  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 2 val loss: 4.4375  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 3 train loss: 1.263671875  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 3 val loss: 4.40625  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.3203125  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.8671875  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 1 train loss: 4.296875  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 1 val loss: 4.8671875  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 2 train loss: 2.681640625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 2 val loss: 4.8046875  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 3 train loss: 1.291015625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 3 val loss: 4.64453125  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 4 train loss: 0.355224609375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 4 val loss: 4.5  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.3203125  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.8671875  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 1 train loss: 4.296875  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 1 val loss: 4.8671875  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 2 train loss: 2.681640625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 2 val loss: 4.8046875  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 3 train loss: 1.291015625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 3 val loss: 4.64453125  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.2734375  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.83203125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 1 train loss: 4.296875  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 1 val loss: 4.82421875  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.2421875  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.83984375  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 1 train loss: 4.25  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 1 val loss: 4.79296875  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 2 train loss: 2.81640625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 2 val loss: 4.703125  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.265625  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.84375  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 1 train loss: 4.25  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 1 val loss: 4.79296875  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 2 train loss: 2.7578125  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 2 val loss: 4.69921875  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.265625  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.83984375  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 1 train loss: 4.25  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 1 val loss: 4.79296875  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.27734375  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.8359375  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 1 train loss: 4.25  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 1 val loss: 4.7890625  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.23828125  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.83984375  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 1 train loss: 4.25  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 1 val loss: 4.79296875  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.34765625  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.875  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 1 train loss: 4.25  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 1 val loss: 4.83984375  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.25390625  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.83203125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 1 train loss: 4.25  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 1 val loss: 4.7890625  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.19140625  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.8359375  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 1 train loss: 4.25  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 1 val loss: 4.7890625  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 2 train loss: 2.69921875  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 2 val loss: 4.66015625  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.19140625  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.8359375  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 1 train loss: 4.25  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 1 val loss: 4.7890625  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.26171875  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.86328125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 1 train loss: 4.296875  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 1 val loss: 4.828125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.26171875  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.828125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 1 train loss: 4.30078125  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 1 val loss: 4.8203125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 2 train loss: 2.763671875  train f1: 0.66667  lr: 0.0003 batch: 16 
epochs: 2 val loss: 4.71484375  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.296875  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.828125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 1 train loss: 4.36328125  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 1 val loss: 4.80859375  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.1796875  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.8359375  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 1 train loss: 4.30078125  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 1 val loss: 4.82421875  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.21875  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.83203125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 1 train loss: 4.36328125  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 1 val loss: 4.80859375  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 2 train loss: 2.66796875  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 2 val loss: 4.73046875  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 3 train loss: 0.888671875  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 3 val loss: 4.55078125  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 4 train loss: 0.371826171875  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 4 val loss: 4.3515625  val f1:0.14286  lr: 0.0003 batch: 16
epochs: 5 train loss: 0.1322021484375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 5 val loss: 4.17578125  val f1:0.14286  lr: 0.0003 batch: 16
epochs: 6 train loss: 0.034149169921875  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 6 val loss: 4.0078125  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 7 train loss: 0.0171051025390625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 7 val loss: 3.8515625  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 8 train loss: 0.01139068603515625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 8 val loss: 3.783203125  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 9 train loss: 0.004077911376953125  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 9 val loss: 3.708984375  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 10 train loss: 0.0035953521728515625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 10 val loss: 3.673828125  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 11 train loss: 0.0024013519287109375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 11 val loss: 3.6640625  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 12 train loss: 0.001827239990234375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 12 val loss: 3.6328125  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 13 train loss: 0.00086212158203125  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 13 val loss: 3.61328125  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 14 train loss: 0.0009508132934570312  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 14 val loss: 3.615234375  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.21875  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.83203125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 1 train loss: 4.36328125  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 1 val loss: 4.80859375  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 0 train loss: 0.6182759349185617  train f1: 0.41643  lr: 0.0003 batch: 16 
epochs: 0 val loss: 0.9806866681134263  val f1:0.57457  lr: 0.0003 batch: 16
epochs: 1 train loss: 0.18437136557334086  train f1: 0.74949  lr: 0.0003 batch: 16 
epochs: 1 val loss: 0.7362223307291667  val f1:0.71075  lr: 0.0003 batch: 16
epochs: 2 train loss: 0.1236606167438917  train f1: 0.83845  lr: 0.0003 batch: 16 
epochs: 2 val loss: 0.8069954766167531  val f1:0.75755  lr: 0.0003 batch: 16
epochs: 3 train loss: 0.08057997589396712  train f1: 0.90459  lr: 0.0003 batch: 16 
epochs: 3 val loss: 0.9337938661928529  val f1:0.73761  lr: 0.0003 batch: 16
epochs: 4 train loss: 0.0658773252197038  train f1: 0.92527  lr: 0.0003 batch: 16 
epochs: 4 val loss: 0.8313436437536172  val f1:0.73844  lr: 0.0003 batch: 16
epochs: 5 train loss: 0.06213724732101702  train f1: 0.92897  lr: 0.0003 batch: 16 
epochs: 5 val loss: 0.8744946515118638  val f1:0.76465  lr: 0.0003 batch: 16
epochs: 6 train loss: 0.04740709809591058  train f1: 0.94099  lr: 0.0003 batch: 16 
epochs: 6 val loss: 0.8998951099537037  val f1:0.74870  lr: 0.0003 batch: 16
epochs: 7 train loss: 0.04287276571230995  train f1: 0.95556  lr: 0.0003 batch: 16 
epochs: 7 val loss: 0.9848613032588253  val f1:0.78445  lr: 0.0003 batch: 16
epochs: 8 train loss: 0.04121258743386019  train f1: 0.95552  lr: 0.0003 batch: 16 
epochs: 8 val loss: 0.9741448296440977  val f1:0.74903  lr: 0.0003 batch: 16
epochs: 9 train loss: 0.03927432368521087  train f1: 0.95993  lr: 0.0003 batch: 16 
epochs: 9 val loss: 0.8965286431489164  val f1:0.77972  lr: 0.0003 batch: 16
epochs: 10 train loss: 0.025115221590175315  train f1: 0.97511  lr: 0.0003 batch: 16 
epochs: 10 val loss: 0.9699497505470557  val f1:0.77955  lr: 0.0003 batch: 16
epochs: 11 train loss: 0.04417648621628109  train f1: 0.96040  lr: 0.0003 batch: 16 
epochs: 11 val loss: 0.8728532861780238  val f1:0.76810  lr: 0.0003 batch: 16
epochs: 12 train loss: 0.02986518291761154  train f1: 0.96626  lr: 0.0003 batch: 16 
epochs: 12 val loss: 0.9958671428539138  val f1:0.78922  lr: 0.0003 batch: 16
epochs: 13 train loss: 0.026208546021929995  train f1: 0.97288  lr: 0.0003 batch: 16 
epochs: 13 val loss: 0.8269449657864041  val f1:0.82123  lr: 0.0003 batch: 16
epochs: 14 train loss: 0.020088393714957114  train f1: 0.97955  lr: 0.0003 batch: 16 
epochs: 14 val loss: 0.9321589434588399  val f1:0.77923  lr: 0.0003 batch: 16
epochs: 15 train loss: 0.031050319534882066  train f1: 0.97050  lr: 0.0003 batch: 16 
epochs: 15 val loss: 0.9411134887624665  val f1:0.79112  lr: 0.0003 batch: 16
epochs: 16 train loss: 0.032217619499363505  train f1: 0.96268  lr: 0.0003 batch: 16 
epochs: 16 val loss: 0.843244093435782  val f1:0.79341  lr: 0.0003 batch: 16
epochs: 17 train loss: 0.03278754997134504  train f1: 0.96726  lr: 0.0003 batch: 16 
epochs: 17 val loss: 0.9610105020028573  val f1:0.77977  lr: 0.0003 batch: 16
epochs: 18 train loss: 0.020468827940876663  train f1: 0.97987  lr: 0.0003 batch: 16 
epochs: 18 val loss: 1.1488327308937356  val f1:0.77815  lr: 0.0003 batch: 16
epochs: 19 train loss: 0.0325631432402461  train f1: 0.96948  lr: 0.0003 batch: 16 
epochs: 19 val loss: 0.8723292050538239  val f1:0.76882  lr: 0.0003 batch: 16
epochs: 20 train loss: 0.019012179309293217  train f1: 0.98340  lr: 0.0003 batch: 16 
epochs: 20 val loss: 1.003086464493363  val f1:0.78488  lr: 0.0003 batch: 16
epochs: 0 train loss: 0.6182759349185617  train f1: 0.41643  lr: 0.0003 batch: 16 
epochs: 0 val loss: 0.9806866681134263  val f1:0.57457  lr: 0.0003 batch: 16
epochs: 1 train loss: 0.18437136557334086  train f1: 0.74949  lr: 0.0003 batch: 16 
epochs: 1 val loss: 0.7362223307291667  val f1:0.71075  lr: 0.0003 batch: 16
epochs: 2 train loss: 0.1236606167438917  train f1: 0.83845  lr: 0.0003 batch: 16 
epochs: 2 val loss: 0.8069954766167531  val f1:0.75755  lr: 0.0003 batch: 16
epochs: 3 train loss: 0.08057997589396712  train f1: 0.90459  lr: 0.0003 batch: 16 
epochs: 3 val loss: 0.9337938661928529  val f1:0.73761  lr: 0.0003 batch: 16
epochs: 4 train loss: 0.0658773252197038  train f1: 0.92527  lr: 0.0003 batch: 16 
epochs: 4 val loss: 0.8313436437536172  val f1:0.73844  lr: 0.0003 batch: 16
epochs: 5 train loss: 0.06213724732101702  train f1: 0.92897  lr: 0.0003 batch: 16 
epochs: 5 val loss: 0.8744946515118638  val f1:0.76465  lr: 0.0003 batch: 16
epochs: 6 train loss: 0.04740709809591058  train f1: 0.94099  lr: 0.0003 batch: 16 
epochs: 6 val loss: 0.8998951099537037  val f1:0.74870  lr: 0.0003 batch: 16
epochs: 7 train loss: 0.04287276571230995  train f1: 0.95556  lr: 0.0003 batch: 16 
epochs: 7 val loss: 0.9848613032588253  val f1:0.78445  lr: 0.0003 batch: 16
epochs: 8 train loss: 0.04121258743386019  train f1: 0.95552  lr: 0.0003 batch: 16 
epochs: 8 val loss: 0.9741448296440977  val f1:0.74903  lr: 0.0003 batch: 16
epochs: 9 train loss: 0.03927432368521087  train f1: 0.95993  lr: 0.0003 batch: 16 
epochs: 9 val loss: 0.8965286431489164  val f1:0.77972  lr: 0.0003 batch: 16
epochs: 10 train loss: 0.025115221590175315  train f1: 0.97511  lr: 0.0003 batch: 16 
epochs: 10 val loss: 0.9699497505470557  val f1:0.77955  lr: 0.0003 batch: 16
epochs: 11 train loss: 0.04417648621628109  train f1: 0.96040  lr: 0.0003 batch: 16 
epochs: 11 val loss: 0.8728532861780238  val f1:0.76810  lr: 0.0003 batch: 16
epochs: 12 train loss: 0.02986518291761154  train f1: 0.96626  lr: 0.0003 batch: 16 
epochs: 12 val loss: 0.9958671428539138  val f1:0.78922  lr: 0.0003 batch: 16
epochs: 13 train loss: 0.026208546021929995  train f1: 0.97288  lr: 0.0003 batch: 16 
epochs: 13 val loss: 0.8269449657864041  val f1:0.82123  lr: 0.0003 batch: 16
epochs: 0 train loss: 0.6086905757685253  train f1: 0.42895  lr: 0.0003 batch: 16 
epochs: 0 val loss: 0.9423032407407405  val f1:0.56418  lr: 0.0003 batch: 16
epochs: 0 train loss: 0.6086905757685253  train f1: 0.42895  lr: 0.0003 batch: 16 
epochs: 0 val loss: 0.9423032407407405  val f1:0.56418  lr: 0.0003 batch: 16
epochs: 1 train loss: 0.20657824994322668  train f1: 0.73546  lr: 0.0003 batch: 16 
epochs: 1 val loss: 0.8643516257957174  val f1:0.67831  lr: 0.0003 batch: 16
epochs: 2 train loss: 0.1232745308531193  train f1: 0.84048  lr: 0.0003 batch: 16 
epochs: 2 val loss: 0.8054167570891202  val f1:0.71093  lr: 0.0003 batch: 16
epochs: 3 train loss: 0.09372267164197064  train f1: 0.89064  lr: 0.0003 batch: 16 
epochs: 3 val loss: 0.6995326289424192  val f1:0.77036  lr: 0.0003 batch: 16
epochs: 4 train loss: 0.0650376453066704  train f1: 0.92153  lr: 0.0003 batch: 16 
epochs: 4 val loss: 0.8334325861047817  val f1:0.76405  lr: 0.0003 batch: 16
epochs: 5 train loss: 0.06124413072914252  train f1: 0.93127  lr: 0.0003 batch: 16 
epochs: 5 val loss: 0.904157398365162  val f1:0.73337  lr: 0.0003 batch: 16
epochs: 6 train loss: 0.05163063989613124  train f1: 0.94369  lr: 0.0003 batch: 16 
epochs: 6 val loss: 1.0364265724464699  val f1:0.74266  lr: 0.0003 batch: 16
epochs: 7 train loss: 0.04861802382956717  train f1: 0.94962  lr: 0.0003 batch: 16 
epochs: 7 val loss: 0.7745009810836226  val f1:0.77904  lr: 0.0003 batch: 16
epochs: 8 train loss: 0.04960482658590756  train f1: 0.95004  lr: 0.0003 batch: 16 
epochs: 8 val loss: 0.852079928362811  val f1:0.78269  lr: 0.0003 batch: 16
epochs: 9 train loss: 0.03242585055548651  train f1: 0.96706  lr: 0.0003 batch: 16 
epochs: 9 val loss: 0.8194676575837311  val f1:0.80626  lr: 0.0003 batch: 16
epochs: 10 train loss: 0.04129629658344683  train f1: 0.96095  lr: 0.0003 batch: 16 
epochs: 10 val loss: 0.9959488939355917  val f1:0.77017  lr: 0.0003 batch: 16
epochs: 11 train loss: 0.03725898615142653  train f1: 0.96505  lr: 0.0003 batch: 16 
epochs: 11 val loss: 0.9860257749204285  val f1:0.78454  lr: 0.0003 batch: 16
epochs: 12 train loss: 0.037947523772270605  train f1: 0.95748  lr: 0.0003 batch: 16 
epochs: 12 val loss: 0.990027251066985  val f1:0.80506  lr: 0.0003 batch: 16
epochs: 13 train loss: 0.04155118068554754  train f1: 0.96095  lr: 0.0003 batch: 16 
epochs: 13 val loss: 1.212867341218171  val f1:0.73852  lr: 0.0003 batch: 16
epochs: 14 train loss: 0.025241727840870705  train f1: 0.98067  lr: 0.0003 batch: 16 
epochs: 14 val loss: 0.844493809452764  val f1:0.82720  lr: 0.0003 batch: 16
epochs: 15 train loss: 0.02785081823270518  train f1: 0.97808  lr: 0.0003 batch: 16 
epochs: 15 val loss: 0.8718018209492716  val f1:0.78516  lr: 0.0003 batch: 16
epochs: 16 train loss: 0.025379911919781648  train f1: 0.97317  lr: 0.0003 batch: 16 
epochs: 16 val loss: 0.8074565710844813  val f1:0.80990  lr: 0.0003 batch: 16
epochs: 17 train loss: 0.032830129463476465  train f1: 0.96701  lr: 0.0003 batch: 16 
epochs: 17 val loss: 0.9964098506503638  val f1:0.79880  lr: 0.0003 batch: 16
epochs: 18 train loss: 0.027393297735889683  train f1: 0.97173  lr: 0.0003 batch: 16 
epochs: 18 val loss: 1.0292289733886717  val f1:0.78303  lr: 0.0003 batch: 16
epochs: 19 train loss: 0.01646114748315027  train f1: 0.97787  lr: 0.0003 batch: 16 
epochs: 19 val loss: 1.157881192807798  val f1:0.75369  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.17578125  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.8359375  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.17578125  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.8359375  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.17578125  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.8359375  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.17578125  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.8359375  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 0 train loss: 0.6142047349354272  train f1: 0.41836  lr: 0.0003 batch: 16 
epochs: 0 val loss: 0.9094514069733797  val f1:0.55735  lr: 0.0003 batch: 16
epochs: 1 train loss: 0.1958640852473918  train f1: 0.74914  lr: 0.0003 batch: 16 
epochs: 1 val loss: 0.7981924551504629  val f1:0.68019  lr: 0.0003 batch: 16
epochs: 2 train loss: 0.10568350746744577  train f1: 0.85827  lr: 0.0003 batch: 16 
epochs: 2 val loss: 0.9128813566984955  val f1:0.69774  lr: 0.0003 batch: 16
epochs: 3 train loss: 0.09465667018272046  train f1: 0.88885  lr: 0.0003 batch: 16 
epochs: 3 val loss: 0.7531645598234956  val f1:0.77451  lr: 0.0003 batch: 16
epochs: 4 train loss: 0.06317815310937211  train f1: 0.92980  lr: 0.0003 batch: 16 
epochs: 4 val loss: 0.7792228981300636  val f1:0.76249  lr: 0.0003 batch: 16
epochs: 5 train loss: 0.05582668299686872  train f1: 0.93456  lr: 0.0003 batch: 16 
epochs: 5 val loss: 0.8963126005949799  val f1:0.76540  lr: 0.0003 batch: 16
epochs: 6 train loss: 0.04853602158755732  train f1: 0.95017  lr: 0.0003 batch: 16 
epochs: 6 val loss: 0.8212576972113717  val f1:0.77280  lr: 0.0003 batch: 16
epochs: 7 train loss: 0.052049795736993074  train f1: 0.94184  lr: 0.0003 batch: 16 
epochs: 7 val loss: 0.7118586222330728  val f1:0.80650  lr: 0.0003 batch: 16
epochs: 8 train loss: 0.029029735603237385  train f1: 0.96834  lr: 0.0003 batch: 16 
epochs: 8 val loss: 0.8434825332076463  val f1:0.78037  lr: 0.0003 batch: 16
epochs: 9 train loss: 0.036491288508560274  train f1: 0.96264  lr: 0.0003 batch: 16 
epochs: 9 val loss: 0.850811089409722  val f1:0.76815  lr: 0.0003 batch: 16
epochs: 10 train loss: 0.0381447079770285  train f1: 0.95836  lr: 0.0003 batch: 16 
epochs: 10 val loss: 0.8111493145978008  val f1:0.79113  lr: 0.0003 batch: 16
epochs: 11 train loss: 0.046058194669999324  train f1: 0.95837  lr: 0.0003 batch: 16 
epochs: 11 val loss: 0.9113114251030816  val f1:0.77925  lr: 0.0003 batch: 16
epochs: 12 train loss: 0.02910746890410519  train f1: 0.97259  lr: 0.0003 batch: 16 
epochs: 12 val loss: 0.96848166430438  val f1:0.78649  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.12109375  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.8671875  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.12109375  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.8671875  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.25  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.8359375  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.12109375  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.8671875  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.12109375  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.8671875  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 1 train loss: 4.25  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 1 val loss: 4.82421875  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 2 train loss: 3.009765625  train f1: 0.50000  lr: 0.0003 batch: 16 
epochs: 2 val loss: 4.77734375  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 3 train loss: 1.0234375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 3 val loss: 4.640625  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 4 train loss: 0.671875  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 4 val loss: 4.44921875  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 5 train loss: 0.1868896484375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 5 val loss: 4.22265625  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 6 train loss: 0.04241943359375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 6 val loss: 4.0546875  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 7 train loss: 0.020965576171875  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 7 val loss: 3.94140625  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.10546875  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.87109375  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 1 train loss: 4.25  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 1 val loss: 4.82421875  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 2 train loss: 3.1484375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 2 val loss: 4.71484375  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 3 train loss: 1.43359375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 3 val loss: 4.55078125  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 4 train loss: 0.6640625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 4 val loss: 4.37890625  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 5 train loss: 0.22607421875  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 5 val loss: 4.1328125  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 6 train loss: 0.0758056640625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 6 val loss: 3.947265625  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 7 train loss: 0.032562255859375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 7 val loss: 3.818359375  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 8 train loss: 0.01406097412109375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 8 val loss: 3.77734375  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 9 train loss: 0.0054931640625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 9 val loss: 3.724609375  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 10 train loss: 0.003711700439453125  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 10 val loss: 3.66015625  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 11 train loss: 0.00484466552734375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 11 val loss: 3.615234375  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 12 train loss: 0.00240325927734375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 12 val loss: 3.580078125  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 13 train loss: 0.001125335693359375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 13 val loss: 3.568359375  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.1640625  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.859375  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 1 train loss: 4.25  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 1 val loss: 4.81640625  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 2 train loss: 2.990234375  train f1: 0.66667  lr: 0.0003 batch: 16 
epochs: 2 val loss: 4.70703125  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 3 train loss: 1.02734375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 3 val loss: 4.53515625  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 4 train loss: 0.401123046875  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 4 val loss: 4.30859375  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 5 train loss: 0.1419677734375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 5 val loss: 4.1015625  val f1:0.14286  lr: 0.0003 batch: 16
epochs: 6 train loss: 0.051483154296875  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 6 val loss: 3.935546875  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 7 train loss: 0.02117919921875  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 7 val loss: 3.81640625  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 8 train loss: 0.01837158203125  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 8 val loss: 3.814453125  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 9 train loss: 0.01296234130859375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 9 val loss: 3.763671875  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 10 train loss: 0.006481170654296875  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 10 val loss: 3.728515625  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.2265625  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.83203125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 1 train loss: 4.296875  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 1 val loss: 4.79296875  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 2 train loss: 2.52734375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 2 val loss: 4.671875  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 3 train loss: 0.916015625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 3 val loss: 4.4453125  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 4 train loss: 0.4521484375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 4 val loss: 4.21484375  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 5 train loss: 0.1083984375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 5 val loss: 4.0234375  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 6 train loss: 0.03839111328125  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 6 val loss: 3.908203125  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 7 train loss: 0.01763916015625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 7 val loss: 3.84765625  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 8 train loss: 0.0099945068359375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 8 val loss: 3.787109375  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 9 train loss: 0.004657745361328125  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 9 val loss: 3.740234375  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 10 train loss: 0.006366729736328125  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 10 val loss: 3.7890625  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 11 train loss: 0.0019159317016601562  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 11 val loss: 3.751953125  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 12 train loss: 0.0019121170043945312  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 12 val loss: 3.771484375  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 13 train loss: 0.002117156982421875  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 13 val loss: 3.810546875  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 14 train loss: 0.0008401870727539062  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 14 val loss: 3.83984375  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 15 train loss: 0.0008301734924316406  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 15 val loss: 3.849609375  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 16 train loss: 0.0007462501525878906  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 16 val loss: 3.8828125  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 17 train loss: 0.0005459785461425781  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 17 val loss: 3.89453125  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 18 train loss: 0.0008697509765625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 18 val loss: 3.892578125  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 19 train loss: 0.0003464221954345703  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 19 val loss: 3.8984375  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.2265625  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.83203125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 1 train loss: 4.296875  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 1 val loss: 4.79296875  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 2 train loss: 2.41796875  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 2 val loss: 4.65234375  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 3 train loss: 1.513671875  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 3 val loss: 4.51953125  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 4 train loss: 0.33642578125  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 4 val loss: 4.32421875  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 5 train loss: 0.12384033203125  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 5 val loss: 4.1171875  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 6 train loss: 0.047607421875  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 6 val loss: 3.943359375  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 7 train loss: 0.03167724609375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 7 val loss: 3.84375  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 8 train loss: 0.01541900634765625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 8 val loss: 3.775390625  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 9 train loss: 0.00934600830078125  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 9 val loss: 3.68359375  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 0 train loss: 0.6138644373030434  train f1: 0.41996  lr: 0.0003 batch: 16 
epochs: 0 val loss: 0.9660590277777776  val f1:0.56326  lr: 0.0003 batch: 16
epochs: 1 train loss: 0.18740519858952479  train f1: 0.74861  lr: 0.0003 batch: 16 
epochs: 1 val loss: 0.7294180410879634  val f1:0.69477  lr: 0.0003 batch: 16
epochs: 2 train loss: 0.12032130412627316  train f1: 0.84220  lr: 0.0003 batch: 16 
epochs: 2 val loss: 0.8129588939525468  val f1:0.73255  lr: 0.0003 batch: 16
epochs: 3 train loss: 0.07791080379723905  train f1: 0.90557  lr: 0.0003 batch: 16 
epochs: 3 val loss: 0.8391934995298032  val f1:0.74634  lr: 0.0003 batch: 16
epochs: 4 train loss: 0.05462736887230239  train f1: 0.94005  lr: 0.0003 batch: 16 
epochs: 4 val loss: 0.9963519626193579  val f1:0.74217  lr: 0.0003 batch: 16
epochs: 5 train loss: 0.055211774577523756  train f1: 0.93887  lr: 0.0003 batch: 16 
epochs: 5 val loss: 0.8845613126401553  val f1:0.76764  lr: 0.0003 batch: 16
epochs: 6 train loss: 0.04409919504512875  train f1: 0.94711  lr: 0.0003 batch: 16 
epochs: 6 val loss: 0.8654644012451167  val f1:0.78120  lr: 0.0003 batch: 16
epochs: 7 train loss: 0.05214655845242549  train f1: 0.94106  lr: 0.0003 batch: 16 
epochs: 7 val loss: 0.8579502812138312  val f1:0.76837  lr: 0.0003 batch: 16
epochs: 8 train loss: 0.046309613676142475  train f1: 0.95257  lr: 0.0003 batch: 16 
epochs: 8 val loss: 1.2264634308991602  val f1:0.76462  lr: 0.0003 batch: 16
epochs: 9 train loss: 0.03811615787242115  train f1: 0.96264  lr: 0.0003 batch: 16 
epochs: 9 val loss: 0.8979156917995879  val f1:0.79264  lr: 0.0003 batch: 16
epochs: 10 train loss: 0.030422819225568123  train f1: 0.97165  lr: 0.0003 batch: 16 
epochs: 10 val loss: 0.828340823562057  val f1:0.77770  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.2265625  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.83203125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.2265625  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.83203125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 1 train loss: 4.3671875  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 1 val loss: 4.81640625  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 2 train loss: 2.6640625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 2 val loss: 4.69140625  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 3 train loss: 1.359375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 3 val loss: 4.5625  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 4 train loss: 0.385009765625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 4 val loss: 4.3046875  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 5 train loss: 0.116943359375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 5 val loss: 4.09375  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 6 train loss: 0.047332763671875  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 6 val loss: 4.0  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 7 train loss: 0.018890380859375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 7 val loss: 3.908203125  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.2265625  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.83203125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.2265625  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.83203125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 1 train loss: 4.3671875  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 1 val loss: 4.81640625  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 2 train loss: 2.6640625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 2 val loss: 4.69140625  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 3 train loss: 1.359375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 3 val loss: 4.5625  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 4 train loss: 0.385009765625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 4 val loss: 4.3046875  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 5 train loss: 0.116943359375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 5 val loss: 4.09375  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 6 train loss: 0.047332763671875  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 6 val loss: 4.0  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.2265625  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.83203125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 1 train loss: 4.3671875  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 1 val loss: 4.81640625  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 2 train loss: 2.6640625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 2 val loss: 4.69140625  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 3 train loss: 1.359375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 3 val loss: 4.5625  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 4 train loss: 0.385009765625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 4 val loss: 4.3046875  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 5 train loss: 0.116943359375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 5 val loss: 4.09375  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 6 train loss: 0.047332763671875  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 6 val loss: 4.0  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 7 train loss: 0.018890380859375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 7 val loss: 3.908203125  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 8 train loss: 0.0084228515625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 8 val loss: 3.833984375  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 9 train loss: 0.00820159912109375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 9 val loss: 3.7578125  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 10 train loss: 0.006679534912109375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 10 val loss: 3.8203125  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 11 train loss: 0.0027866363525390625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 11 val loss: 3.7421875  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 12 train loss: 0.0019893646240234375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 12 val loss: 3.744140625  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 13 train loss: 0.000949859619140625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 13 val loss: 3.73046875  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 14 train loss: 0.0009784698486328125  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 14 val loss: 3.7265625  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 15 train loss: 0.0009250640869140625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 15 val loss: 3.740234375  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 16 train loss: 0.00038743019104003906  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 16 val loss: 3.720703125  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 17 train loss: 0.0002689361572265625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 17 val loss: 3.689453125  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 18 train loss: 0.0002396106719970703  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 18 val loss: 3.69921875  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 19 train loss: 0.00030040740966796875  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 19 val loss: 3.724609375  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.2265625  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.83203125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 1 train loss: 4.3671875  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 1 val loss: 4.81640625  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 2 train loss: 2.6640625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 2 val loss: 4.69140625  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 3 train loss: 1.359375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 3 val loss: 4.5625  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 4 train loss: 0.385009765625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 4 val loss: 4.3046875  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 5 train loss: 0.116943359375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 5 val loss: 4.09375  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 6 train loss: 0.047332763671875  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 6 val loss: 4.0  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 7 train loss: 0.018890380859375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 7 val loss: 3.908203125  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 8 train loss: 0.0084228515625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 8 val loss: 3.833984375  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 9 train loss: 0.00820159912109375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 9 val loss: 3.7578125  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 10 train loss: 0.006679534912109375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 10 val loss: 3.8203125  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 11 train loss: 0.0027866363525390625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 11 val loss: 3.7421875  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 12 train loss: 0.0019893646240234375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 12 val loss: 3.744140625  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 13 train loss: 0.000949859619140625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 13 val loss: 3.73046875  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 14 train loss: 0.0009784698486328125  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 14 val loss: 3.7265625  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.2265625  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.83203125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.2265625  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.83203125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.2265625  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.83203125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.2265625  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.83203125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.2265625  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.83203125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 1 train loss: 4.3671875  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 1 val loss: 4.81640625  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 2 train loss: 2.6640625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 2 val loss: 4.69140625  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 3 train loss: 1.359375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 3 val loss: 4.5625  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 4 train loss: 0.385009765625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 4 val loss: 4.3046875  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 5 train loss: 0.116943359375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 5 val loss: 4.09375  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 6 train loss: 0.047332763671875  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 6 val loss: 4.0  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 7 train loss: 0.018890380859375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 7 val loss: 3.908203125  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 8 train loss: 0.0084228515625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 8 val loss: 3.833984375  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 9 train loss: 0.00820159912109375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 9 val loss: 3.7578125  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 10 train loss: 0.006679534912109375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 10 val loss: 3.8203125  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 11 train loss: 0.0027866363525390625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 11 val loss: 3.7421875  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 12 train loss: 0.0019893646240234375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 12 val loss: 3.744140625  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 13 train loss: 0.000949859619140625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 13 val loss: 3.73046875  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 14 train loss: 0.0009784698486328125  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 14 val loss: 3.7265625  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 15 train loss: 0.0009250640869140625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 15 val loss: 3.740234375  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 16 train loss: 0.00038743019104003906  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 16 val loss: 3.720703125  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 17 train loss: 0.0002689361572265625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 17 val loss: 3.689453125  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 18 train loss: 0.0002396106719970703  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 18 val loss: 3.69921875  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 19 train loss: 0.00030040740966796875  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 19 val loss: 3.724609375  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.2265625  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.83203125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 1 train loss: 4.3671875  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 1 val loss: 4.81640625  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 2 train loss: 2.6640625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 2 val loss: 4.69140625  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 3 train loss: 1.359375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 3 val loss: 4.5625  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 4 train loss: 0.385009765625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 4 val loss: 4.3046875  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.2265625  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.83203125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 1 train loss: 4.3671875  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 1 val loss: 4.81640625  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.2265625  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.83203125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 1 train loss: 4.3671875  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 1 val loss: 4.81640625  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 2 train loss: 2.6640625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 2 val loss: 4.69140625  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 3 train loss: 1.359375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 3 val loss: 4.5625  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 4 train loss: 0.385009765625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 4 val loss: 4.3046875  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 5 train loss: 0.116943359375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 5 val loss: 4.09375  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 6 train loss: 0.047332763671875  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 6 val loss: 4.0  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 7 train loss: 0.018890380859375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 7 val loss: 3.908203125  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 8 train loss: 0.0084228515625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 8 val loss: 3.833984375  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 9 train loss: 0.00820159912109375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 9 val loss: 3.7578125  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 10 train loss: 0.006679534912109375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 10 val loss: 3.8203125  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 11 train loss: 0.0027866363525390625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 11 val loss: 3.7421875  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 12 train loss: 0.0019893646240234375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 12 val loss: 3.744140625  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 13 train loss: 0.000949859619140625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 13 val loss: 3.73046875  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 14 train loss: 0.0009784698486328125  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 14 val loss: 3.7265625  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.2265625  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.83203125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 1 train loss: 4.3671875  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 1 val loss: 4.81640625  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 2 train loss: 2.6640625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 2 val loss: 4.69140625  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 3 train loss: 1.359375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 3 val loss: 4.5625  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 4 train loss: 0.385009765625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 4 val loss: 4.3046875  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 5 train loss: 0.116943359375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 5 val loss: 4.09375  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 6 train loss: 0.047332763671875  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 6 val loss: 4.0  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 7 train loss: 0.018890380859375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 7 val loss: 3.908203125  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 8 train loss: 0.0084228515625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 8 val loss: 3.833984375  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 9 train loss: 0.00820159912109375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 9 val loss: 3.7578125  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 10 train loss: 0.006679534912109375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 10 val loss: 3.8203125  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.2265625  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.83203125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.2265625  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.83203125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.2265625  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.83203125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.2265625  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.83203125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.2265625  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.83203125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 1 train loss: 4.3671875  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 1 val loss: 4.81640625  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 2 train loss: 2.6640625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 2 val loss: 4.69140625  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 3 train loss: 1.359375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 3 val loss: 4.5625  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 4 train loss: 0.385009765625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 4 val loss: 4.3046875  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 5 train loss: 0.116943359375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 5 val loss: 4.09375  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 6 train loss: 0.047332763671875  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 6 val loss: 4.0  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.2265625  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.83203125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 1 train loss: 4.3671875  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 1 val loss: 4.81640625  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 2 train loss: 2.6640625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 2 val loss: 4.69140625  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 3 train loss: 1.359375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 3 val loss: 4.5625  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 4 train loss: 0.385009765625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 4 val loss: 4.3046875  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.2265625  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.83203125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 1 train loss: 4.3671875  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 1 val loss: 4.81640625  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.2265625  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.83203125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 1 train loss: 4.3671875  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 1 val loss: 4.81640625  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.2265625  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.83203125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 1 train loss: 4.3671875  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 1 val loss: 4.81640625  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 2 train loss: 2.6640625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 2 val loss: 4.69140625  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 3 train loss: 1.359375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 3 val loss: 4.5625  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 4 train loss: 0.385009765625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 4 val loss: 4.3046875  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 5 train loss: 0.116943359375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 5 val loss: 4.09375  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 6 train loss: 0.047332763671875  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 6 val loss: 4.0  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 7 train loss: 0.018890380859375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 7 val loss: 3.908203125  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 8 train loss: 0.0084228515625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 8 val loss: 3.833984375  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 9 train loss: 0.00820159912109375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 9 val loss: 3.7578125  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 10 train loss: 0.006679534912109375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 10 val loss: 3.8203125  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 11 train loss: 0.0027866363525390625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 11 val loss: 3.7421875  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 12 train loss: 0.0019893646240234375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 12 val loss: 3.744140625  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 13 train loss: 0.000949859619140625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 13 val loss: 3.73046875  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 14 train loss: 0.0009784698486328125  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 14 val loss: 3.7265625  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 15 train loss: 0.0009250640869140625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 15 val loss: 3.740234375  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 16 train loss: 0.00038743019104003906  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 16 val loss: 3.720703125  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 17 train loss: 0.0002689361572265625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 17 val loss: 3.689453125  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 18 train loss: 0.0002396106719970703  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 18 val loss: 3.69921875  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 19 train loss: 0.00030040740966796875  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 19 val loss: 3.724609375  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.3046875  train f1: 0.00000  lr: 0.0003 batch: 2 
epochs: 0 val loss: 4.890625  val f1:0.00000  lr: 0.0003 batch: 2
epochs: 1 train loss: 3.8330078125  train f1: 0.13333  lr: 0.0003 batch: 2 
epochs: 1 val loss: 4.651041666666667  val f1:0.00000  lr: 0.0003 batch: 2
epochs: 2 train loss: 2.7802734375  train f1: 0.41667  lr: 0.0003 batch: 2 
epochs: 2 val loss: 4.345703125  val f1:0.12500  lr: 0.0003 batch: 2
epochs: 3 train loss: 1.82177734375  train f1: 0.77778  lr: 0.0003 batch: 2 
epochs: 3 val loss: 4.165364583333333  val f1:0.11111  lr: 0.0003 batch: 2
epochs: 4 train loss: 1.02880859375  train f1: 1.00000  lr: 0.0003 batch: 2 
epochs: 4 val loss: 4.052083333333334  val f1:0.11111  lr: 0.0003 batch: 2
epochs: 5 train loss: 0.3541259765625  train f1: 1.00000  lr: 0.0003 batch: 2 
epochs: 5 val loss: 4.04296875  val f1:0.11111  lr: 0.0003 batch: 2
epochs: 6 train loss: 0.10272216796875  train f1: 1.00000  lr: 0.0003 batch: 2 
epochs: 6 val loss: 4.070963541666666  val f1:0.11111  lr: 0.0003 batch: 2
epochs: 7 train loss: 0.0645751953125  train f1: 1.00000  lr: 0.0003 batch: 2 
epochs: 7 val loss: 4.076171875  val f1:0.11111  lr: 0.0003 batch: 2
epochs: 8 train loss: 0.50067138671875  train f1: 1.00000  lr: 0.0003 batch: 2 
epochs: 8 val loss: 4.017578125  val f1:0.11111  lr: 0.0003 batch: 2
epochs: 0 train loss: 4.2265625  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.83203125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 1 train loss: 4.3671875  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 1 val loss: 4.81640625  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 2 train loss: 2.6640625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 2 val loss: 4.69140625  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 3 train loss: 1.359375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 3 val loss: 4.5625  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.2265625  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.83203125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 1 train loss: 4.3671875  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 1 val loss: 4.81640625  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 2 train loss: 2.6640625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 2 val loss: 4.69140625  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.2265625  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.83203125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 1 train loss: 4.3671875  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 1 val loss: 4.81640625  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 2 train loss: 2.6640625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 2 val loss: 4.69140625  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.2265625  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.83203125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 1 train loss: 4.3671875  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 1 val loss: 4.81640625  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 2 train loss: 2.6640625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 2 val loss: 4.69140625  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.2265625  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.83203125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 1 train loss: 4.3671875  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 1 val loss: 4.81640625  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 2 train loss: 2.6640625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 2 val loss: 4.69140625  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 3 train loss: 1.359375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 3 val loss: 4.5625  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 4 train loss: 0.385009765625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 4 val loss: 4.3046875  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 5 train loss: 0.116943359375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 5 val loss: 4.09375  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 6 train loss: 0.047332763671875  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 6 val loss: 4.0  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 7 train loss: 0.018890380859375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 7 val loss: 3.908203125  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 8 train loss: 0.0084228515625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 8 val loss: 3.833984375  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.2265625  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.83203125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 1 train loss: 4.3671875  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 1 val loss: 4.81640625  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.2265625  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.83203125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 1 train loss: 4.3671875  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 1 val loss: 4.81640625  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 2 train loss: 2.6640625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 2 val loss: 4.69140625  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 3 train loss: 1.359375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 3 val loss: 4.5625  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 4 train loss: 0.385009765625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 4 val loss: 4.3046875  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 5 train loss: 0.116943359375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 5 val loss: 4.09375  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 6 train loss: 0.047332763671875  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 6 val loss: 4.0  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 7 train loss: 0.018890380859375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 7 val loss: 3.908203125  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 8 train loss: 0.0084228515625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 8 val loss: 3.833984375  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 9 train loss: 0.00820159912109375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 9 val loss: 3.7578125  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 10 train loss: 0.006679534912109375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 10 val loss: 3.8203125  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 11 train loss: 0.0027866363525390625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 11 val loss: 3.7421875  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 12 train loss: 0.0019893646240234375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 12 val loss: 3.744140625  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 13 train loss: 0.000949859619140625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 13 val loss: 3.73046875  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 14 train loss: 0.0009784698486328125  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 14 val loss: 3.7265625  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 15 train loss: 0.0009250640869140625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 15 val loss: 3.740234375  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 16 train loss: 0.00038743019104003906  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 16 val loss: 3.720703125  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 17 train loss: 0.0002689361572265625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 17 val loss: 3.689453125  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 18 train loss: 0.0002396106719970703  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 18 val loss: 3.69921875  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 19 train loss: 0.00030040740966796875  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 19 val loss: 3.724609375  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.2265625  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.83203125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 1 train loss: 4.3671875  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 1 val loss: 4.81640625  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 2 train loss: 2.6640625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 2 val loss: 4.69140625  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 3 train loss: 1.359375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 3 val loss: 4.5625  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.2265625  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.83203125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 1 train loss: 4.3671875  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 1 val loss: 4.81640625  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 2 train loss: 2.6640625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 2 val loss: 4.69140625  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 3 train loss: 1.359375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 3 val loss: 4.5625  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 4 train loss: 0.385009765625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 4 val loss: 4.3046875  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 5 train loss: 0.116943359375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 5 val loss: 4.09375  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 6 train loss: 0.047332763671875  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 6 val loss: 4.0  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.2265625  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.83203125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 1 train loss: 4.3671875  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 1 val loss: 4.81640625  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.2265625  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.83203125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 1 train loss: 4.3671875  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 1 val loss: 4.81640625  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 2 train loss: 2.6640625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 2 val loss: 4.69140625  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 3 train loss: 1.359375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 3 val loss: 4.5625  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.2265625  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.83203125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 1 train loss: 4.3671875  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 1 val loss: 4.81640625  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 2 train loss: 2.6640625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 2 val loss: 4.69140625  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 3 train loss: 1.359375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 3 val loss: 4.5625  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 4 train loss: 0.385009765625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 4 val loss: 4.3046875  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 5 train loss: 0.116943359375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 5 val loss: 4.09375  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 6 train loss: 0.047332763671875  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 6 val loss: 4.0  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 7 train loss: 0.018890380859375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 7 val loss: 3.908203125  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.2265625  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.83203125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.2265625  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.83203125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.2265625  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.83203125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 1 train loss: 4.3671875  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 1 val loss: 4.81640625  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 2 train loss: 2.6640625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 2 val loss: 4.69140625  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 3 train loss: 1.359375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 3 val loss: 4.5625  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 4 train loss: 0.385009765625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 4 val loss: 4.3046875  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 5 train loss: 0.116943359375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 5 val loss: 4.09375  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 6 train loss: 0.047332763671875  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 6 val loss: 4.0  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 7 train loss: 0.018890380859375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 7 val loss: 3.908203125  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 8 train loss: 0.0084228515625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 8 val loss: 3.833984375  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.2265625  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.83203125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.2265625  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.83203125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 1 train loss: 4.3671875  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 1 val loss: 4.81640625  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 2 train loss: 2.6640625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 2 val loss: 4.69140625  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 3 train loss: 1.359375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 3 val loss: 4.5625  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.2265625  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.83203125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 1 train loss: 4.3671875  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 1 val loss: 4.81640625  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 2 train loss: 2.6640625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 2 val loss: 4.69140625  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 3 train loss: 1.359375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 3 val loss: 4.5625  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 4 train loss: 0.385009765625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 4 val loss: 4.3046875  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 5 train loss: 0.116943359375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 5 val loss: 4.09375  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 6 train loss: 0.047332763671875  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 6 val loss: 4.0  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 7 train loss: 0.018890380859375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 7 val loss: 3.908203125  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 8 train loss: 0.0084228515625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 8 val loss: 3.833984375  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 9 train loss: 0.00820159912109375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 9 val loss: 3.7578125  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 10 train loss: 0.006679534912109375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 10 val loss: 3.8203125  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 11 train loss: 0.0027866363525390625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 11 val loss: 3.7421875  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 12 train loss: 0.0019893646240234375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 12 val loss: 3.744140625  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 13 train loss: 0.000949859619140625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 13 val loss: 3.73046875  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 14 train loss: 0.0009784698486328125  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 14 val loss: 3.7265625  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 15 train loss: 0.0009250640869140625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 15 val loss: 3.740234375  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 16 train loss: 0.00038743019104003906  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 16 val loss: 3.720703125  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 17 train loss: 0.0002689361572265625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 17 val loss: 3.689453125  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 18 train loss: 0.0002396106719970703  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 18 val loss: 3.69921875  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 19 train loss: 0.00030040740966796875  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 19 val loss: 3.724609375  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.2265625  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.83203125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 1 train loss: 4.3671875  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 1 val loss: 4.81640625  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 2 train loss: 2.6640625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 2 val loss: 4.69140625  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 3 train loss: 1.359375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 3 val loss: 4.5625  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 4 train loss: 0.385009765625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 4 val loss: 4.3046875  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 5 train loss: 0.116943359375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 5 val loss: 4.09375  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 6 train loss: 0.047332763671875  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 6 val loss: 4.0  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 7 train loss: 0.018890380859375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 7 val loss: 3.908203125  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 8 train loss: 0.0084228515625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 8 val loss: 3.833984375  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 9 train loss: 0.00820159912109375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 9 val loss: 3.7578125  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 10 train loss: 0.006679534912109375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 10 val loss: 3.8203125  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 11 train loss: 0.0027866363525390625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 11 val loss: 3.7421875  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 12 train loss: 0.0019893646240234375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 12 val loss: 3.744140625  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.2265625  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.83203125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 1 train loss: 4.3671875  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 1 val loss: 4.81640625  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 2 train loss: 2.6640625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 2 val loss: 4.69140625  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.2265625  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.83203125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 1 train loss: 4.3671875  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 1 val loss: 4.81640625  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 2 train loss: 2.6640625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 2 val loss: 4.69140625  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.2265625  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.83203125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 1 train loss: 4.3671875  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 1 val loss: 4.81640625  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 2 train loss: 2.6640625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 2 val loss: 4.69140625  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 3 train loss: 1.359375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 3 val loss: 4.5625  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 4 train loss: 0.385009765625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 4 val loss: 4.3046875  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 5 train loss: 0.116943359375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 5 val loss: 4.09375  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 6 train loss: 0.047332763671875  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 6 val loss: 4.0  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 7 train loss: 0.018890380859375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 7 val loss: 3.908203125  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 8 train loss: 0.0084228515625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 8 val loss: 3.833984375  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 9 train loss: 0.00820159912109375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 9 val loss: 3.7578125  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 10 train loss: 0.006679534912109375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 10 val loss: 3.8203125  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 11 train loss: 0.0027866363525390625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 11 val loss: 3.7421875  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 12 train loss: 0.0019893646240234375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 12 val loss: 3.744140625  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 13 train loss: 0.000949859619140625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 13 val loss: 3.73046875  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 14 train loss: 0.0009784698486328125  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 14 val loss: 3.7265625  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.2265625  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.83203125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 1 train loss: 4.3671875  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 1 val loss: 4.81640625  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 2 train loss: 2.6640625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 2 val loss: 4.69140625  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 3 train loss: 1.359375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 3 val loss: 4.5625  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 4 train loss: 0.385009765625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 4 val loss: 4.3046875  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 5 train loss: 0.116943359375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 5 val loss: 4.09375  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 6 train loss: 0.047332763671875  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 6 val loss: 4.0  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 7 train loss: 0.018890380859375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 7 val loss: 3.908203125  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 8 train loss: 0.0084228515625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 8 val loss: 3.833984375  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 9 train loss: 0.00820159912109375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 9 val loss: 3.7578125  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 10 train loss: 0.006679534912109375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 10 val loss: 3.8203125  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 11 train loss: 0.0027866363525390625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 11 val loss: 3.7421875  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 12 train loss: 0.0019893646240234375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 12 val loss: 3.744140625  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 13 train loss: 0.000949859619140625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 13 val loss: 3.73046875  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 14 train loss: 0.0009784698486328125  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 14 val loss: 3.7265625  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 15 train loss: 0.0009250640869140625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 15 val loss: 3.740234375  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 16 train loss: 0.00038743019104003906  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 16 val loss: 3.720703125  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 17 train loss: 0.0002689361572265625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 17 val loss: 3.689453125  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.2265625  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.83203125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 1 train loss: 4.3671875  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 1 val loss: 4.81640625  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 2 train loss: 2.6640625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 2 val loss: 4.69140625  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 3 train loss: 1.359375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 3 val loss: 4.5625  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 4 train loss: 0.385009765625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 4 val loss: 4.3046875  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 5 train loss: 0.116943359375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 5 val loss: 4.09375  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.2265625  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.83203125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 1 train loss: 4.3671875  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 1 val loss: 4.81640625  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 2 train loss: 2.6640625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 2 val loss: 4.69140625  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 3 train loss: 1.359375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 3 val loss: 4.5625  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 4 train loss: 0.385009765625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 4 val loss: 4.3046875  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 5 train loss: 0.116943359375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 5 val loss: 4.09375  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 6 train loss: 0.047332763671875  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 6 val loss: 4.0  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 7 train loss: 0.018890380859375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 7 val loss: 3.908203125  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 8 train loss: 0.0084228515625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 8 val loss: 3.833984375  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 9 train loss: 0.00820159912109375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 9 val loss: 3.7578125  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 10 train loss: 0.006679534912109375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 10 val loss: 3.8203125  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 11 train loss: 0.0027866363525390625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 11 val loss: 3.7421875  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 12 train loss: 0.0019893646240234375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 12 val loss: 3.744140625  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 13 train loss: 0.000949859619140625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 13 val loss: 3.73046875  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 14 train loss: 0.0009784698486328125  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 14 val loss: 3.7265625  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 15 train loss: 0.0009250640869140625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 15 val loss: 3.740234375  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 16 train loss: 0.00038743019104003906  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 16 val loss: 3.720703125  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 17 train loss: 0.0002689361572265625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 17 val loss: 3.689453125  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 18 train loss: 0.0002396106719970703  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 18 val loss: 3.69921875  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 19 train loss: 0.00030040740966796875  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 19 val loss: 3.724609375  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.2265625  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.83203125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 1 train loss: 4.3671875  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 1 val loss: 4.81640625  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 2 train loss: 2.6640625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 2 val loss: 4.69140625  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 3 train loss: 1.359375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 3 val loss: 4.5625  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 4 train loss: 0.385009765625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 4 val loss: 4.3046875  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 5 train loss: 0.116943359375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 5 val loss: 4.09375  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 6 train loss: 0.047332763671875  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 6 val loss: 4.0  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 7 train loss: 0.018890380859375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 7 val loss: 3.908203125  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 8 train loss: 0.0084228515625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 8 val loss: 3.833984375  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 9 train loss: 0.00820159912109375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 9 val loss: 3.7578125  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.2265625  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.83203125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 1 train loss: 4.3671875  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 1 val loss: 4.81640625  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 2 train loss: 2.6640625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 2 val loss: 4.69140625  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 3 train loss: 1.359375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 3 val loss: 4.5625  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 4 train loss: 0.385009765625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 4 val loss: 4.3046875  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 5 train loss: 0.116943359375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 5 val loss: 4.09375  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 6 train loss: 0.047332763671875  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 6 val loss: 4.0  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 7 train loss: 0.018890380859375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 7 val loss: 3.908203125  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 8 train loss: 0.0084228515625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 8 val loss: 3.833984375  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 9 train loss: 0.00820159912109375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 9 val loss: 3.7578125  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 10 train loss: 0.006679534912109375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 10 val loss: 3.8203125  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 11 train loss: 0.0027866363525390625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 11 val loss: 3.7421875  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 12 train loss: 0.0019893646240234375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 12 val loss: 3.744140625  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 13 train loss: 0.000949859619140625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 13 val loss: 3.73046875  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 14 train loss: 0.0009784698486328125  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 14 val loss: 3.7265625  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 15 train loss: 0.0009250640869140625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 15 val loss: 3.740234375  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 16 train loss: 0.00038743019104003906  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 16 val loss: 3.720703125  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 17 train loss: 0.0002689361572265625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 17 val loss: 3.689453125  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 18 train loss: 0.0002396106719970703  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 18 val loss: 3.69921875  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 19 train loss: 0.00030040740966796875  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 19 val loss: 3.724609375  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.2265625  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.83203125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 1 train loss: 4.3671875  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 1 val loss: 4.81640625  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 2 train loss: 2.6640625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 2 val loss: 4.69140625  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 3 train loss: 1.359375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 3 val loss: 4.5625  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 4 train loss: 0.385009765625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 4 val loss: 4.3046875  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 5 train loss: 0.116943359375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 5 val loss: 4.09375  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 6 train loss: 0.047332763671875  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 6 val loss: 4.0  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 7 train loss: 0.018890380859375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 7 val loss: 3.908203125  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 8 train loss: 0.0084228515625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 8 val loss: 3.833984375  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 9 train loss: 0.00820159912109375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 9 val loss: 3.7578125  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 10 train loss: 0.006679534912109375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 10 val loss: 3.8203125  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 11 train loss: 0.0027866363525390625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 11 val loss: 3.7421875  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 12 train loss: 0.0019893646240234375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 12 val loss: 3.744140625  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 13 train loss: 0.000949859619140625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 13 val loss: 3.73046875  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 14 train loss: 0.0009784698486328125  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 14 val loss: 3.7265625  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 15 train loss: 0.0009250640869140625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 15 val loss: 3.740234375  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 16 train loss: 0.00038743019104003906  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 16 val loss: 3.720703125  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 17 train loss: 0.0002689361572265625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 17 val loss: 3.689453125  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 18 train loss: 0.0002396106719970703  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 18 val loss: 3.69921875  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 19 train loss: 0.00030040740966796875  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 19 val loss: 3.724609375  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.2265625  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.83203125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 1 train loss: 4.3671875  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 1 val loss: 4.81640625  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 2 train loss: 2.6640625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 2 val loss: 4.69140625  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.2265625  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.83203125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 1 train loss: 4.3671875  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 1 val loss: 4.81640625  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 2 train loss: 2.6640625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 2 val loss: 4.69140625  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 3 train loss: 1.359375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 3 val loss: 4.5625  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.2265625  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.83203125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 1 train loss: 4.3671875  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 1 val loss: 4.81640625  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 2 train loss: 2.6640625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 2 val loss: 4.69140625  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 3 train loss: 1.359375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 3 val loss: 4.5625  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 4 train loss: 0.385009765625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 4 val loss: 4.3046875  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 5 train loss: 0.116943359375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 5 val loss: 4.09375  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 6 train loss: 0.047332763671875  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 6 val loss: 4.0  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 7 train loss: 0.018890380859375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 7 val loss: 3.908203125  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 8 train loss: 0.0084228515625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 8 val loss: 3.833984375  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 9 train loss: 0.00820159912109375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 9 val loss: 3.7578125  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 10 train loss: 0.006679534912109375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 10 val loss: 3.8203125  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 11 train loss: 0.0027866363525390625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 11 val loss: 3.7421875  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.2265625  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.83203125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 1 train loss: 4.3671875  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 1 val loss: 4.81640625  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 2 train loss: 2.6640625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 2 val loss: 4.69140625  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 3 train loss: 1.359375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 3 val loss: 4.5625  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 4 train loss: 0.385009765625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 4 val loss: 4.3046875  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 5 train loss: 0.116943359375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 5 val loss: 4.09375  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 6 train loss: 0.047332763671875  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 6 val loss: 4.0  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 7 train loss: 0.018890380859375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 7 val loss: 3.908203125  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 8 train loss: 0.0084228515625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 8 val loss: 3.833984375  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 9 train loss: 0.00820159912109375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 9 val loss: 3.7578125  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 10 train loss: 0.006679534912109375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 10 val loss: 3.8203125  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.2265625  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.83203125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 1 train loss: 4.3671875  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 1 val loss: 4.81640625  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 2 train loss: 2.6640625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 2 val loss: 4.69140625  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 3 train loss: 1.359375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 3 val loss: 4.5625  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 4 train loss: 0.385009765625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 4 val loss: 4.3046875  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 5 train loss: 0.116943359375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 5 val loss: 4.09375  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 6 train loss: 0.047332763671875  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 6 val loss: 4.0  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.2265625  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.83203125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 1 train loss: 4.3671875  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 1 val loss: 4.81640625  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.2265625  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.83203125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 1 train loss: 4.3671875  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 1 val loss: 4.81640625  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 2 train loss: 2.6640625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 2 val loss: 4.69140625  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 3 train loss: 1.359375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 3 val loss: 4.5625  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 4 train loss: 0.385009765625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 4 val loss: 4.3046875  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 5 train loss: 0.116943359375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 5 val loss: 4.09375  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 6 train loss: 0.047332763671875  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 6 val loss: 4.0  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 7 train loss: 0.018890380859375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 7 val loss: 3.908203125  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 8 train loss: 0.0084228515625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 8 val loss: 3.833984375  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 9 train loss: 0.00820159912109375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 9 val loss: 3.7578125  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 10 train loss: 0.006679534912109375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 10 val loss: 3.8203125  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 11 train loss: 0.0027866363525390625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 11 val loss: 3.7421875  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 12 train loss: 0.0019893646240234375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 12 val loss: 3.744140625  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 13 train loss: 0.000949859619140625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 13 val loss: 3.73046875  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 14 train loss: 0.0009784698486328125  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 14 val loss: 3.7265625  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 15 train loss: 0.0009250640869140625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 15 val loss: 3.740234375  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 16 train loss: 0.00038743019104003906  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 16 val loss: 3.720703125  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 17 train loss: 0.0002689361572265625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 17 val loss: 3.689453125  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 18 train loss: 0.0002396106719970703  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 18 val loss: 3.69921875  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 19 train loss: 0.00030040740966796875  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 19 val loss: 3.724609375  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.2265625  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.83203125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 1 train loss: 4.3671875  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 1 val loss: 4.81640625  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 2 train loss: 2.6640625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 2 val loss: 4.69140625  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 3 train loss: 1.359375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 3 val loss: 4.5625  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 4 train loss: 0.385009765625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 4 val loss: 4.3046875  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.2265625  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.83203125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 1 train loss: 4.3671875  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 1 val loss: 4.81640625  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 2 train loss: 2.6640625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 2 val loss: 4.69140625  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.2265625  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.83203125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.2265625  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.83203125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.2265625  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.83203125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 1 train loss: 4.3671875  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 1 val loss: 4.81640625  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.2265625  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.83203125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.2265625  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.83203125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.2265625  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.83203125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 1 train loss: 4.3671875  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 1 val loss: 4.81640625  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 2 train loss: 2.6640625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 2 val loss: 4.69140625  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 3 train loss: 1.359375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 3 val loss: 4.5625  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 4 train loss: 0.385009765625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 4 val loss: 4.3046875  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 5 train loss: 0.116943359375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 5 val loss: 4.09375  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 6 train loss: 0.047332763671875  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 6 val loss: 4.0  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 7 train loss: 0.018890380859375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 7 val loss: 3.908203125  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 8 train loss: 0.0084228515625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 8 val loss: 3.833984375  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 9 train loss: 0.00820159912109375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 9 val loss: 3.7578125  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 10 train loss: 0.006679534912109375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 10 val loss: 3.8203125  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 11 train loss: 0.0027866363525390625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 11 val loss: 3.7421875  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 12 train loss: 0.0019893646240234375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 12 val loss: 3.744140625  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 13 train loss: 0.000949859619140625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 13 val loss: 3.73046875  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 14 train loss: 0.0009784698486328125  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 14 val loss: 3.7265625  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 15 train loss: 0.0009250640869140625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 15 val loss: 3.740234375  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 16 train loss: 0.00038743019104003906  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 16 val loss: 3.720703125  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 17 train loss: 0.0002689361572265625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 17 val loss: 3.689453125  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 18 train loss: 0.0002396106719970703  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 18 val loss: 3.69921875  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 19 train loss: 0.00030040740966796875  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 19 val loss: 3.724609375  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.2265625  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.83203125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 1 train loss: 4.3671875  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 1 val loss: 4.81640625  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 2 train loss: 2.6640625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 2 val loss: 4.69140625  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 3 train loss: 1.359375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 3 val loss: 4.5625  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 4 train loss: 0.385009765625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 4 val loss: 4.3046875  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 5 train loss: 0.116943359375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 5 val loss: 4.09375  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 6 train loss: 0.047332763671875  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 6 val loss: 4.0  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 7 train loss: 0.018890380859375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 7 val loss: 3.908203125  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 8 train loss: 0.0084228515625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 8 val loss: 3.833984375  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 9 train loss: 0.00820159912109375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 9 val loss: 3.7578125  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.2265625  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.83203125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 1 train loss: 4.3671875  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 1 val loss: 4.81640625  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 2 train loss: 2.6640625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 2 val loss: 4.69140625  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.2265625  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.83203125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 1 train loss: 4.3671875  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 1 val loss: 4.81640625  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 2 train loss: 2.6640625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 2 val loss: 4.69140625  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.2265625  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.83203125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 1 train loss: 4.3671875  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 1 val loss: 4.81640625  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 2 train loss: 2.6640625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 2 val loss: 4.69140625  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 3 train loss: 1.359375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 3 val loss: 4.5625  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 4 train loss: 0.385009765625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 4 val loss: 4.3046875  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 5 train loss: 0.116943359375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 5 val loss: 4.09375  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 6 train loss: 0.047332763671875  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 6 val loss: 4.0  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 7 train loss: 0.018890380859375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 7 val loss: 3.908203125  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 8 train loss: 0.0084228515625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 8 val loss: 3.833984375  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 9 train loss: 0.00820159912109375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 9 val loss: 3.7578125  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 10 train loss: 0.006679534912109375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 10 val loss: 3.8203125  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.2265625  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.83203125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.2265625  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.83203125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 1 train loss: 4.3671875  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 1 val loss: 4.81640625  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 2 train loss: 2.6640625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 2 val loss: 4.69140625  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.2265625  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.83203125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 1 train loss: 4.3671875  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 1 val loss: 4.81640625  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 2 train loss: 2.6640625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 2 val loss: 4.69140625  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 3 train loss: 1.359375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 3 val loss: 4.5625  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 4 train loss: 0.385009765625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 4 val loss: 4.3046875  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 5 train loss: 0.116943359375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 5 val loss: 4.09375  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 6 train loss: 0.047332763671875  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 6 val loss: 4.0  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 7 train loss: 0.018890380859375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 7 val loss: 3.908203125  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 8 train loss: 0.0084228515625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 8 val loss: 3.833984375  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 9 train loss: 0.00820159912109375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 9 val loss: 3.7578125  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.2265625  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.83203125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 1 train loss: 4.3671875  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 1 val loss: 4.81640625  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 2 train loss: 2.6640625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 2 val loss: 4.69140625  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.2265625  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.83203125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 1 train loss: 4.3671875  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 1 val loss: 4.81640625  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 2 train loss: 2.6640625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 2 val loss: 4.69140625  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 3 train loss: 1.359375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 3 val loss: 4.5625  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 4 train loss: 0.385009765625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 4 val loss: 4.3046875  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 5 train loss: 0.116943359375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 5 val loss: 4.09375  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 6 train loss: 0.047332763671875  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 6 val loss: 4.0  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.2265625  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.83203125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.2265625  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.83203125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.2265625  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.83203125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 1 train loss: 4.3671875  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 1 val loss: 4.81640625  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.2265625  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.83203125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.2265625  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.83203125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.2265625  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.83203125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 1 train loss: 4.3671875  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 1 val loss: 4.81640625  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 2 train loss: 2.6640625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 2 val loss: 4.69140625  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 3 train loss: 1.359375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 3 val loss: 4.5625  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 4 train loss: 0.385009765625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 4 val loss: 4.3046875  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 5 train loss: 0.116943359375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 5 val loss: 4.09375  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 6 train loss: 0.047332763671875  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 6 val loss: 4.0  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 7 train loss: 0.018890380859375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 7 val loss: 3.908203125  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 8 train loss: 0.0084228515625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 8 val loss: 3.833984375  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.2265625  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.83203125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.2265625  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.83203125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.2265625  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.83203125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 1 train loss: 4.3671875  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 1 val loss: 4.81640625  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 2 train loss: 2.6640625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 2 val loss: 4.69140625  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 3 train loss: 1.359375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 3 val loss: 4.5625  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 4 train loss: 0.385009765625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 4 val loss: 4.3046875  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 5 train loss: 0.116943359375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 5 val loss: 4.09375  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 6 train loss: 0.047332763671875  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 6 val loss: 4.0  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 7 train loss: 0.018890380859375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 7 val loss: 3.908203125  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 8 train loss: 0.0084228515625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 8 val loss: 3.833984375  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 9 train loss: 0.00820159912109375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 9 val loss: 3.7578125  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 10 train loss: 0.006679534912109375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 10 val loss: 3.8203125  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 11 train loss: 0.0027866363525390625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 11 val loss: 3.7421875  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 12 train loss: 0.0019893646240234375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 12 val loss: 3.744140625  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 13 train loss: 0.000949859619140625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 13 val loss: 3.73046875  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 14 train loss: 0.0009784698486328125  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 14 val loss: 3.7265625  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 15 train loss: 0.0009250640869140625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 15 val loss: 3.740234375  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 16 train loss: 0.00038743019104003906  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 16 val loss: 3.720703125  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 17 train loss: 0.0002689361572265625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 17 val loss: 3.689453125  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 18 train loss: 0.0002396106719970703  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 18 val loss: 3.69921875  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 19 train loss: 0.00030040740966796875  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 19 val loss: 3.724609375  val f1:0.11111  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.2265625  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.83203125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 1 train loss: 4.3671875  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 1 val loss: 4.81640625  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 2 train loss: 2.6640625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 2 val loss: 4.69140625  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 3 train loss: 1.359375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 3 val loss: 4.5625  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.2265625  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.83203125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 1 train loss: 4.3671875  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 1 val loss: 4.81640625  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 2 train loss: 2.6640625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 2 val loss: 4.69140625  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.2265625  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.83203125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 1 train loss: 4.3671875  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 1 val loss: 4.81640625  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 2 train loss: 2.6640625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 2 val loss: 4.69140625  val f1:0.12500  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.2265625  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.83203125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 0 train loss: 0.6182759349185617  train f1: 0.41643  lr: 0.0003 batch: 16 
epochs: 0 val loss: 0.9806866681134263  val f1:0.57457  lr: 0.0003 batch: 16
epochs: 1 train loss: 0.19499481586446787  train f1: 0.74043  lr: 0.0003 batch: 16 
epochs: 1 val loss: 0.6918303313078707  val f1:0.72345  lr: 0.0003 batch: 16
epochs: 2 train loss: 0.11705371388176151  train f1: 0.84674  lr: 0.0003 batch: 16 
epochs: 2 val loss: 0.7371333369502312  val f1:0.74116  lr: 0.0003 batch: 16
epochs: 3 train loss: 0.08605314192926507  train f1: 0.88751  lr: 0.0003 batch: 16 
epochs: 3 val loss: 0.9308325167055483  val f1:0.72972  lr: 0.0003 batch: 16
epochs: 4 train loss: 0.06414947872447255  train f1: 0.92337  lr: 0.0003 batch: 16 
epochs: 4 val loss: 0.807210710313585  val f1:0.78644  lr: 0.0003 batch: 16
epochs: 5 train loss: 0.0514236102229045  train f1: 0.94333  lr: 0.0003 batch: 16 
epochs: 5 val loss: 0.9471306694878469  val f1:0.75595  lr: 0.0003 batch: 16
epochs: 6 train loss: 0.048475552378152736  train f1: 0.94821  lr: 0.0003 batch: 16 
epochs: 6 val loss: 0.9443966335720486  val f1:0.76569  lr: 0.0003 batch: 16
epochs: 7 train loss: 0.04937441806840785  train f1: 0.94698  lr: 0.0003 batch: 16 
epochs: 7 val loss: 0.81913491708261  val f1:0.77834  lr: 0.0003 batch: 16
epochs: 8 train loss: 0.034365307140231394  train f1: 0.95921  lr: 0.0003 batch: 16 
epochs: 8 val loss: 0.7489488919576004  val f1:0.81027  lr: 0.0003 batch: 16
epochs: 9 train loss: 0.03157425907781888  train f1: 0.96227  lr: 0.0003 batch: 16 
epochs: 9 val loss: 0.8862346861097544  val f1:0.76780  lr: 0.0003 batch: 16
epochs: 10 train loss: 0.032457665686595485  train f1: 0.96812  lr: 0.0003 batch: 16 
epochs: 10 val loss: 1.270830472310384  val f1:0.73968  lr: 0.0003 batch: 16
epochs: 11 train loss: 0.04432054462278273  train f1: 0.95190  lr: 0.0003 batch: 16 
epochs: 11 val loss: 1.0681382497151692  val f1:0.75348  lr: 0.0003 batch: 16
epochs: 12 train loss: 0.037528138802830416  train f1: 0.96584  lr: 0.0003 batch: 16 
epochs: 12 val loss: 0.8463199650799795  val f1:0.81614  lr: 0.0003 batch: 16
epochs: 13 train loss: 0.02626649310761262  train f1: 0.97512  lr: 0.0003 batch: 16 
epochs: 13 val loss: 1.1337641115541808  val f1:0.77858  lr: 0.0003 batch: 16
epochs: 0 train loss: 0.6171068110668158  train f1: 0.41985  lr: 0.0003 batch: 16 
epochs: 0 val loss: 1.1027117693865742  val f1:0.55512  lr: 0.0003 batch: 16
epochs: 1 train loss: 0.20030105619359184  train f1: 0.73196  lr: 0.0003 batch: 16 
epochs: 1 val loss: 0.7963130244502314  val f1:0.66386  lr: 0.0003 batch: 16
epochs: 2 train loss: 0.10630266446425136  train f1: 0.85821  lr: 0.0003 batch: 16 
epochs: 2 val loss: 0.8716464572482636  val f1:0.71050  lr: 0.0003 batch: 16
epochs: 3 train loss: 0.0953962160762112  train f1: 0.88552  lr: 0.0003 batch: 16 
epochs: 3 val loss: 0.9570615415219909  val f1:0.74816  lr: 0.0003 batch: 16
epochs: 4 train loss: 0.05651880588912018  train f1: 0.92791  lr: 0.0003 batch: 16 
epochs: 4 val loss: 0.8240616268581813  val f1:0.77171  lr: 0.0003 batch: 16
epochs: 5 train loss: 0.055828098941622294  train f1: 0.94818  lr: 0.0003 batch: 16 
epochs: 5 val loss: 1.0061530925609452  val f1:0.73172  lr: 0.0003 batch: 16
epochs: 6 train loss: 0.05390110946355621  train f1: 0.94131  lr: 0.0003 batch: 16 
epochs: 6 val loss: 0.9487760543823239  val f1:0.76059  lr: 0.0003 batch: 16
epochs: 7 train loss: 0.04241820300309138  train f1: 0.95525  lr: 0.0003 batch: 16 
epochs: 7 val loss: 0.9200299510249381  val f1:0.76236  lr: 0.0003 batch: 16
epochs: 8 train loss: 0.04930793666780145  train f1: 0.95316  lr: 0.0003 batch: 16 
epochs: 8 val loss: 0.9563728615089698  val f1:0.76704  lr: 0.0003 batch: 16
epochs: 9 train loss: 0.03771726508390283  train f1: 0.96372  lr: 0.0003 batch: 16 
epochs: 9 val loss: 1.018552405745895  val f1:0.76267  lr: 0.0003 batch: 16
epochs: 10 train loss: 0.03213155774998844  train f1: 0.96318  lr: 0.0003 batch: 16 
epochs: 10 val loss: 0.8990374105947982  val f1:0.76310  lr: 0.0003 batch: 16
epochs: 11 train loss: 0.050717278980554795  train f1: 0.94630  lr: 0.0003 batch: 16 
epochs: 11 val loss: 0.7662780196578414  val f1:0.78529  lr: 0.0003 batch: 16
epochs: 12 train loss: 0.024006605817195468  train f1: 0.97822  lr: 0.0003 batch: 16 
epochs: 12 val loss: 0.873808119032118  val f1:0.78742  lr: 0.0003 batch: 16
epochs: 13 train loss: 0.02817103987620062  train f1: 0.97489  lr: 0.0003 batch: 16 
epochs: 13 val loss: 1.0436854962949396  val f1:0.78843  lr: 0.0003 batch: 16
epochs: 14 train loss: 0.03842993745780054  train f1: 0.96328  lr: 0.0003 batch: 16 
epochs: 14 val loss: 1.0831374556929978  val f1:0.75504  lr: 0.0003 batch: 16
epochs: 0 train loss: 0.6090206612375313  train f1: 0.40814  lr: 0.0003 batch: 32 
epochs: 0 val loss: 1.0505012063419117  val f1:0.54206  lr: 0.0003 batch: 32
epochs: 1 train loss: 0.17699986027363218  train f1: 0.76117  lr: 0.0003 batch: 32 
epochs: 1 val loss: 0.7107876048368567  val f1:0.71178  lr: 0.0003 batch: 32
epochs: 2 train loss: 0.0858088467186526  train f1: 0.89317  lr: 0.0003 batch: 32 
epochs: 2 val loss: 0.7038035673253676  val f1:0.75119  lr: 0.0003 batch: 32
epochs: 3 train loss: 0.06236409189695132  train f1: 0.92501  lr: 0.0003 batch: 32 
epochs: 3 val loss: 0.5995555204503676  val f1:0.78114  lr: 0.0003 batch: 32
epochs: 4 train loss: 0.044444264913734946  train f1: 0.95295  lr: 0.0003 batch: 32 
epochs: 4 val loss: 0.8006807215073529  val f1:0.75926  lr: 0.0003 batch: 32
epochs: 5 train loss: 0.0410457159812908  train f1: 0.95104  lr: 0.0003 batch: 32 
epochs: 5 val loss: 0.830644495346967  val f1:0.75608  lr: 0.0003 batch: 32
epochs: 6 train loss: 0.04198378874476712  train f1: 0.95824  lr: 0.0003 batch: 32 
epochs: 6 val loss: 0.7214876062729781  val f1:0.78475  lr: 0.0003 batch: 32
epochs: 7 train loss: 0.029936284198428035  train f1: 0.96874  lr: 0.0003 batch: 32 
epochs: 7 val loss: 0.7465892118566176  val f1:0.76587  lr: 0.0003 batch: 32
epochs: 8 train loss: 0.036584449825144146  train f1: 0.96891  lr: 0.0003 batch: 32 
epochs: 8 val loss: 0.9048093907973347  val f1:0.76803  lr: 0.0003 batch: 32
epochs: 9 train loss: 0.032062491276615  train f1: 0.96040  lr: 0.0003 batch: 32 
epochs: 9 val loss: 0.9559308220358456  val f1:0.76549  lr: 0.0003 batch: 32
epochs: 10 train loss: 0.023679763301649594  train f1: 0.97914  lr: 0.0003 batch: 32 
epochs: 10 val loss: 0.8529606987448299  val f1:0.76234  lr: 0.0003 batch: 32
epochs: 11 train loss: 0.031139555863311467  train f1: 0.97206  lr: 0.0003 batch: 32 
epochs: 11 val loss: 0.8050366569967831  val f1:0.77457  lr: 0.0003 batch: 32
epochs: 12 train loss: 0.02157931642936649  train f1: 0.97522  lr: 0.0003 batch: 32 
epochs: 12 val loss: 0.8102596507352942  val f1:0.79277  lr: 0.0003 batch: 32
epochs: 0 train loss: 0.6042257484949738  train f1: 0.41192  lr: 0.0003 batch: 32 
epochs: 0 val loss: 0.999856387867647  val f1:0.56005  lr: 0.0003 batch: 32
epochs: 1 train loss: 0.17227673708946603  train f1: 0.76876  lr: 0.0003 batch: 32 
epochs: 1 val loss: 0.7830379710477942  val f1:0.69918  lr: 0.0003 batch: 32
epochs: 2 train loss: 0.08109732934661637  train f1: 0.88601  lr: 0.0003 batch: 32 
epochs: 2 val loss: 0.6326563218060661  val f1:0.75811  lr: 0.0003 batch: 32
epochs: 3 train loss: 0.06637495235909252  train f1: 0.92007  lr: 0.0003 batch: 32 
epochs: 3 val loss: 0.8510006175321692  val f1:0.74285  lr: 0.0003 batch: 32
epochs: 4 train loss: 0.038794370066198035  train f1: 0.95602  lr: 0.0003 batch: 32 
epochs: 4 val loss: 0.7327189725988049  val f1:0.78186  lr: 0.0003 batch: 32
epochs: 5 train loss: 0.04554240305227531  train f1: 0.94799  lr: 0.0003 batch: 32 
epochs: 5 val loss: 0.7917794620289523  val f1:0.76684  lr: 0.0003 batch: 32
epochs: 6 train loss: 0.02851048372035608  train f1: 0.96619  lr: 0.0003 batch: 32 
epochs: 6 val loss: 0.7177025290096507  val f1:0.79233  lr: 0.0003 batch: 32
epochs: 7 train loss: 0.039108469599203007  train f1: 0.95830  lr: 0.0003 batch: 32 
epochs: 7 val loss: 0.8275308048023897  val f1:0.79492  lr: 0.0003 batch: 32
epochs: 8 train loss: 0.03010156237870977  train f1: 0.97369  lr: 0.0003 batch: 32 
epochs: 8 val loss: 0.7892904842601102  val f1:0.81064  lr: 0.0003 batch: 32
epochs: 9 train loss: 0.021244670982075435  train f1: 0.97769  lr: 0.0003 batch: 32 
epochs: 9 val loss: 0.9002003389246325  val f1:0.75666  lr: 0.0003 batch: 32
epochs: 10 train loss: 0.03379261047762826  train f1: 0.96837  lr: 0.0003 batch: 32 
epochs: 10 val loss: 0.7404803107766542  val f1:0.77706  lr: 0.0003 batch: 32
epochs: 11 train loss: 0.030788055530510037  train f1: 0.97107  lr: 0.0003 batch: 32 
epochs: 11 val loss: 0.758959601907169  val f1:0.76970  lr: 0.0003 batch: 32
epochs: 12 train loss: 0.02252430853403715  train f1: 0.96994  lr: 0.0003 batch: 32 
epochs: 12 val loss: 0.8316973517922793  val f1:0.77611  lr: 0.0003 batch: 32
epochs: 13 train loss: 0.020262509064186845  train f1: 0.97607  lr: 0.0003 batch: 32 
epochs: 13 val loss: 0.8586030847886029  val f1:0.77445  lr: 0.0003 batch: 32
epochs: 0 train loss: 4.12890625  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 train loss: 4.125  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.8515625  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 0 train loss: 0.6260465588652877  train f1: 0.39884  lr: 0.0003 batch: 16 
epochs: 0 val loss: 0.9496202256944447  val f1:0.55674  lr: 0.0003 batch: 16
epochs: 1 train loss: 0.2266583430796784  train f1: 0.70535  lr: 0.0003 batch: 16 
epochs: 1 val loss: 0.7242088035300928  val f1:0.69909  lr: 0.0003 batch: 16
epochs: 2 train loss: 0.13312848369379576  train f1: 0.83772  lr: 0.0003 batch: 16 
epochs: 2 val loss: 0.6320712619357637  val f1:0.75589  lr: 0.0003 batch: 16
epochs: 3 train loss: 0.09175600732056566  train f1: 0.89144  lr: 0.0003 batch: 16 
epochs: 3 val loss: 0.7450953730830436  val f1:0.71982  lr: 0.0003 batch: 16
epochs: 4 train loss: 0.08091873094030741  train f1: 0.90423  lr: 0.0003 batch: 16 
epochs: 4 val loss: 0.9858127170138895  val f1:0.71338  lr: 0.0003 batch: 16
epochs: 5 train loss: 0.06701054269833459  train f1: 0.92721  lr: 0.0003 batch: 16 
epochs: 5 val loss: 0.8500148066767947  val f1:0.78295  lr: 0.0003 batch: 16
epochs: 6 train loss: 0.0471584053705458  train f1: 0.94820  lr: 0.0003 batch: 16 
epochs: 6 val loss: 0.8418162027994791  val f1:0.77663  lr: 0.0003 batch: 16
epochs: 7 train loss: 0.0662050737704422  train f1: 0.93520  lr: 0.0003 batch: 16 
epochs: 7 val loss: 0.8088290179217302  val f1:0.77875  lr: 0.0003 batch: 16
epochs: 8 train loss: 0.04994983216770863  train f1: 0.95512  lr: 0.0003 batch: 16 
epochs: 8 val loss: 0.9213384628295898  val f1:0.75980  lr: 0.0003 batch: 16
epochs: 9 train loss: 0.045029311406047545  train f1: 0.95473  lr: 0.0003 batch: 16 
epochs: 9 val loss: 0.8560893729881003  val f1:0.79215  lr: 0.0003 batch: 16
epochs: 10 train loss: 0.04720770123593522  train f1: 0.94647  lr: 0.0003 batch: 16 
epochs: 10 val loss: 0.7770801685474538  val f1:0.79170  lr: 0.0003 batch: 16
epochs: 11 train loss: 0.044666541336183245  train f1: 0.95841  lr: 0.0003 batch: 16 
epochs: 11 val loss: 0.9717792369701247  val f1:0.75456  lr: 0.0003 batch: 16
epochs: 12 train loss: 0.040934777765202685  train f1: 0.95816  lr: 0.0003 batch: 16 
epochs: 12 val loss: 1.0542018042670358  val f1:0.77901  lr: 0.0003 batch: 16
epochs: 13 train loss: 0.0417379157204283  train f1: 0.95969  lr: 0.0003 batch: 16 
epochs: 13 val loss: 0.877323517975984  val f1:0.78989  lr: 0.0003 batch: 16
epochs: 14 train loss: 0.03573901650316993  train f1: 0.96416  lr: 0.0003 batch: 16 
epochs: 14 val loss: 0.7756833888866284  val f1:0.81194  lr: 0.0003 batch: 16
epochs: 15 train loss: 0.03890690096001373  train f1: 0.96585  lr: 0.0003 batch: 16 
epochs: 15 val loss: 0.88095028841937  val f1:0.79270  lr: 0.0003 batch: 16
epochs: 16 train loss: 0.02742202033723086  train f1: 0.97854  lr: 0.0003 batch: 16 
epochs: 16 val loss: 0.8848547829522025  val f1:0.79726  lr: 0.0003 batch: 16
epochs: 17 train loss: 0.027338291715802685  train f1: 0.97362  lr: 0.0003 batch: 16 
epochs: 17 val loss: 0.8824003590477841  val f1:0.79794  lr: 0.0003 batch: 16
epochs: 18 train loss: 0.03869094582567193  train f1: 0.96641  lr: 0.0003 batch: 16 
epochs: 18 val loss: 0.8009570369014035  val f1:0.80477  lr: 0.0003 batch: 16
epochs: 19 train loss: 0.03453905951055207  train f1: 0.97138  lr: 0.0003 batch: 16 
epochs: 19 val loss: 0.8968246601246018  val f1:0.79294  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.25  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.8359375  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.16015625  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.515625  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 1 train loss: 4.26171875  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 1 val loss: 4.62109375  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 2 train loss: 3.0  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 2 val loss: 4.53125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 3 train loss: 1.4853515625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 3 val loss: 4.4609375  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 4 train loss: 0.7080078125  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 4 val loss: 4.421875  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 5 train loss: 0.1728515625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 5 val loss: 4.4765625  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 6 train loss: 0.0650634765625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 6 val loss: 4.52734375  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 7 train loss: 0.0241241455078125  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 7 val loss: 4.62890625  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 8 train loss: 0.01273345947265625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 8 val loss: 4.65625  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 9 train loss: 0.004688262939453125  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 9 val loss: 4.62890625  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 10 train loss: 0.003635406494140625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 10 val loss: 4.66015625  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 11 train loss: 0.00453948974609375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 11 val loss: 4.72265625  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 12 train loss: 0.002044677734375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 12 val loss: 4.83203125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 13 train loss: 0.0006990432739257812  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 13 val loss: 4.85546875  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.16015625  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.515625  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 1 train loss: 4.26171875  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 1 val loss: 4.62109375  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 2 train loss: 3.0  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 2 val loss: 4.53125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 3 train loss: 1.4853515625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 3 val loss: 4.4609375  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 4 train loss: 0.7080078125  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 4 val loss: 4.421875  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 5 train loss: 0.1728515625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 5 val loss: 4.4765625  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 6 train loss: 0.0650634765625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 6 val loss: 4.52734375  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 7 train loss: 0.0241241455078125  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 7 val loss: 4.62890625  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 8 train loss: 0.01273345947265625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 8 val loss: 4.65625  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 9 train loss: 0.004688262939453125  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 9 val loss: 4.62890625  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 10 train loss: 0.003635406494140625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 10 val loss: 4.66015625  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 11 train loss: 0.00453948974609375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 11 val loss: 4.72265625  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 12 train loss: 0.002044677734375  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 12 val loss: 4.83203125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 13 train loss: 0.0006990432739257812  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 13 val loss: 4.85546875  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 14 train loss: 0.0010023117065429688  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 14 val loss: 4.85546875  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 15 train loss: 0.001834869384765625  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 15 val loss: 4.83984375  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 16 train loss: 0.00045299530029296875  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 16 val loss: 4.8671875  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 17 train loss: 0.00036644935607910156  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 17 val loss: 4.87890625  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 18 train loss: 0.00098419189453125  train f1: 1.00000  lr: 0.0003 batch: 16 
epochs: 18 val loss: 4.83984375  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.1640625  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.453125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 1 train loss: 4.26171875  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 1 val loss: 4.5546875  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 2 train loss: 3.2890625  train f1: 0.66667  lr: 0.0003 batch: 16 
epochs: 2 val loss: 4.54296875  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.203125  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 0 val loss: 4.8125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 1 train loss: 4.25  train f1: 0.00000  lr: 0.0003 batch: 16 
epochs: 1 val loss: 4.7578125  val f1:0.00000  lr: 0.0003 batch: 16
epochs: 0 train loss: 0.6402335702035186  train f1: 0.39426  lr: 0.0003 batch: 16 
epochs: 0 val loss: 1.0129774305555552  val f1:0.54486  lr: 0.0003 batch: 16
epochs: 1 train loss: 0.23595311933027552  train f1: 0.70387  lr: 0.0003 batch: 16 
epochs: 1 val loss: 0.8638057002314816  val f1:0.66659  lr: 0.0003 batch: 16
epochs: 2 train loss: 0.1457082719874204  train f1: 0.81540  lr: 0.0003 batch: 16 
epochs: 2 val loss: 0.7465530960648147  val f1:0.70270  lr: 0.0003 batch: 16
epochs: 3 train loss: 0.11460017385030927  train f1: 0.86468  lr: 0.0003 batch: 16 
epochs: 3 val loss: 0.7899420844184029  val f1:0.74031  lr: 0.0003 batch: 16
epochs: 4 train loss: 0.08761924044449719  train f1: 0.89741  lr: 0.0003 batch: 16 
epochs: 4 val loss: 0.8002264517324942  val f1:0.72524  lr: 0.0003 batch: 16
epochs: 5 train loss: 0.0766598117024524  train f1: 0.91124  lr: 0.0003 batch: 16 
epochs: 5 val loss: 0.8692496970847806  val f1:0.75178  lr: 0.0003 batch: 16
epochs: 6 train loss: 0.0736288346554572  train f1: 0.91923  lr: 0.0003 batch: 16 
epochs: 6 val loss: 0.7480523533291287  val f1:0.77351  lr: 0.0003 batch: 16
epochs: 7 train loss: 0.06430009832405989  train f1: 0.92725  lr: 0.0003 batch: 16 
epochs: 7 val loss: 0.8977732905635122  val f1:0.74627  lr: 0.0003 batch: 16
epochs: 8 train loss: 0.051376068755575606  train f1: 0.95059  lr: 0.0003 batch: 16 
epochs: 8 val loss: 1.0503250969780815  val f1:0.75568  lr: 0.0003 batch: 16
epochs: 9 train loss: 0.053814203513233426  train f1: 0.94002  lr: 0.0003 batch: 16 
epochs: 9 val loss: 0.8183792114257814  val f1:0.78403  lr: 0.0003 batch: 16
epochs: 10 train loss: 0.04834758016533985  train f1: 0.95309  lr: 0.0003 batch: 16 
epochs: 10 val loss: 0.8744154612223304  val f1:0.77277  lr: 0.0003 batch: 16
epochs: 11 train loss: 0.056511979447933125  train f1: 0.94347  lr: 0.0003 batch: 16 
epochs: 11 val loss: 0.8431184698034214  val f1:0.78260  lr: 0.0003 batch: 16
epochs: 12 train loss: 0.04501825303508151  train f1: 0.95547  lr: 0.0003 batch: 16 
epochs: 12 val loss: 1.0986626801667387  val f1:0.76452  lr: 0.0003 batch: 16
epochs: 0 train loss: 4.3671875  train f1: 0.00000  lr: 0.0003 batch: 32 
epochs: 0 val loss: 4.83203125  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 0 train loss: 4.29296875  train f1: 0.00000  lr: 0.0003 batch: 32 
epochs: 0 val loss: 4.82421875  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 0 train loss: 0.6046977435561484  train f1: 0.40227  lr: 0.0003 batch: 32 
epochs: 0 val loss: 0.9912145278033088  val f1:0.53551  lr: 0.0003 batch: 32
epochs: 1 train loss: 0.175573353755504  train f1: 0.76233  lr: 0.0003 batch: 32 
epochs: 1 val loss: 0.7151884191176473  val f1:0.70211  lr: 0.0003 batch: 32
epochs: 2 train loss: 0.09013783842548166  train f1: 0.87284  lr: 0.0003 batch: 32 
epochs: 2 val loss: 0.7318007525275733  val f1:0.71909  lr: 0.0003 batch: 32
epochs: 3 train loss: 0.06030184015668837  train f1: 0.92608  lr: 0.0003 batch: 32 
epochs: 3 val loss: 0.7660019818474264  val f1:0.75145  lr: 0.0003 batch: 32
epochs: 4 train loss: 0.04439693555570299  train f1: 0.94755  lr: 0.0003 batch: 32 
epochs: 4 val loss: 0.7956174962660847  val f1:0.79132  lr: 0.0003 batch: 32
epochs: 5 train loss: 0.043115374453347205  train f1: 0.95799  lr: 0.0003 batch: 32 
epochs: 5 val loss: 0.7780878403607535  val f1:0.79259  lr: 0.0003 batch: 32
epochs: 6 train loss: 0.04031662572351776  train f1: 0.95129  lr: 0.0003 batch: 32 
epochs: 6 val loss: 0.7037066291360293  val f1:0.79975  lr: 0.0003 batch: 32
epochs: 7 train loss: 0.0373230468007989  train f1: 0.96238  lr: 0.0003 batch: 32 
epochs: 7 val loss: 0.759587287902832  val f1:0.79405  lr: 0.0003 batch: 32
epochs: 8 train loss: 0.025760470185791168  train f1: 0.97003  lr: 0.0003 batch: 32 
epochs: 8 val loss: 0.8038092220530793  val f1:0.79644  lr: 0.0003 batch: 32
epochs: 9 train loss: 0.0278143410076227  train f1: 0.97060  lr: 0.0003 batch: 32 
epochs: 9 val loss: 0.77698875876034  val f1:0.78839  lr: 0.0003 batch: 32
epochs: 10 train loss: 0.02883056200056006  train f1: 0.96549  lr: 0.0003 batch: 32 
epochs: 10 val loss: 0.8981412999770222  val f1:0.76417  lr: 0.0003 batch: 32
epochs: 0 train loss: 0.6050109102244392  train f1: 0.41299  lr: 0.0003 batch: 32 
epochs: 0 val loss: 1.0416439280790437  val f1:0.54316  lr: 0.0003 batch: 32
epochs: 1 train loss: 0.17366948686633024  train f1: 0.75887  lr: 0.0003 batch: 32 
epochs: 1 val loss: 0.6679974724264703  val f1:0.71408  lr: 0.0003 batch: 32
epochs: 2 train loss: 0.09680156517504458  train f1: 0.87102  lr: 0.0003 batch: 32 
epochs: 2 val loss: 0.6378425149356618  val f1:0.77326  lr: 0.0003 batch: 32
epochs: 3 train loss: 0.058489816147193036  train f1: 0.92323  lr: 0.0003 batch: 32 
epochs: 3 val loss: 0.8382590798770683  val f1:0.74350  lr: 0.0003 batch: 32
epochs: 4 train loss: 0.047177267193496956  train f1: 0.94178  lr: 0.0003 batch: 32 
epochs: 4 val loss: 0.7348780912511489  val f1:0.76634  lr: 0.0003 batch: 32
epochs: 5 train loss: 0.0504685019019833  train f1: 0.94352  lr: 0.0003 batch: 32 
epochs: 5 val loss: 0.801276711856618  val f1:0.75819  lr: 0.0003 batch: 32
epochs: 6 train loss: 0.03734217023017102  train f1: 0.96870  lr: 0.0003 batch: 32 
epochs: 6 val loss: 0.9060327866498162  val f1:0.74288  lr: 0.0003 batch: 32
epochs: 7 train loss: 0.03230300181524412  train f1: 0.96064  lr: 0.0003 batch: 32 
epochs: 7 val loss: 0.6574738446403952  val f1:0.80417  lr: 0.0003 batch: 32
epochs: 8 train loss: 0.02038014886385189  train f1: 0.98181  lr: 0.0003 batch: 32 
epochs: 8 val loss: 0.7958409365485695  val f1:0.78357  lr: 0.0003 batch: 32
epochs: 9 train loss: 0.04629261089382028  train f1: 0.95031  lr: 0.0003 batch: 32 
epochs: 9 val loss: 0.7370569565716913  val f1:0.78049  lr: 0.0003 batch: 32
epochs: 10 train loss: 0.034731927952564266  train f1: 0.95710  lr: 0.0003 batch: 32 
epochs: 10 val loss: 0.8480628518497243  val f1:0.76813  lr: 0.0003 batch: 32
epochs: 11 train loss: 0.019280538891913612  train f1: 0.97912  lr: 0.0003 batch: 32 
epochs: 11 val loss: 0.8243141174316405  val f1:0.79330  lr: 0.0003 batch: 32
epochs: 12 train loss: 0.02983197532687104  train f1: 0.97305  lr: 0.0003 batch: 32 
epochs: 12 val loss: 0.8736159380744485  val f1:0.78040  lr: 0.0003 batch: 32
epochs: 13 train loss: 0.026745400821181587  train f1: 0.97700  lr: 0.0003 batch: 32 
epochs: 13 val loss: 0.794314188115737  val f1:0.81104  lr: 0.0003 batch: 32
epochs: 14 train loss: 0.025376946105624076  train f1: 0.97860  lr: 0.0003 batch: 32 
epochs: 14 val loss: 0.7855583639705879  val f1:0.78075  lr: 0.0003 batch: 32
epochs: 15 train loss: 0.021574473291858468  train f1: 0.97879  lr: 0.0003 batch: 32 
epochs: 15 val loss: 0.7204993752872241  val f1:0.80482  lr: 0.0003 batch: 32
epochs: 16 train loss: 0.0206958583168258  train f1: 0.98126  lr: 0.0003 batch: 32 
epochs: 16 val loss: 0.9848345588235294  val f1:0.75568  lr: 0.0003 batch: 32
epochs: 17 train loss: 0.023169405145240855  train f1: 0.97592  lr: 0.0003 batch: 32 
epochs: 17 val loss: 0.7685834099264706  val f1:0.81031  lr: 0.0003 batch: 32
epochs: 18 train loss: 0.017497294412884046  train f1: 0.98754  lr: 0.0003 batch: 32 
epochs: 18 val loss: 0.770832286161535  val f1:0.80755  lr: 0.0003 batch: 32
epochs: 19 train loss: 0.015138226702920827  train f1: 0.98654  lr: 0.0003 batch: 32 
epochs: 19 val loss: 0.8272471708409932  val f1:0.80292  lr: 0.0003 batch: 32
epochs: 0 train loss: 4.671875  train f1: 0.00000  lr: 0.0003 batch: 32 
epochs: 0 val loss: 4.68359375  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 0 train loss: 4.671875  train f1: 0.00000  lr: 0.0003 batch: 32 
epochs: 0 val loss: 4.68359375  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 1 train loss: 4.5625  train f1: 0.00000  lr: 0.0003 batch: 32 
epochs: 1 val loss: 4.71875  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 2 train loss: 2.8515625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 2 val loss: 4.71484375  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 3 train loss: 2.7734375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 3 val loss: 4.703125  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 4 train loss: 1.6376953125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 4 val loss: 4.65625  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 5 train loss: 0.896484375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 5 val loss: 4.55859375  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 6 train loss: 0.5390625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 6 val loss: 4.54296875  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 7 train loss: 0.1986083984375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 7 val loss: 4.62890625  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 8 train loss: 0.133056640625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 8 val loss: 4.671875  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 9 train loss: 0.04754638671875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 9 val loss: 4.68359375  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 10 train loss: 0.031524658203125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 10 val loss: 4.6875  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 11 train loss: 0.021759033203125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 11 val loss: 4.61328125  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 12 train loss: 0.0135498046875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 12 val loss: 4.60546875  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 13 train loss: 0.009918212890625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 13 val loss: 4.5390625  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 14 train loss: 0.008544921875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 14 val loss: 4.4296875  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 15 train loss: 0.00684356689453125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 15 val loss: 4.296875  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 16 train loss: 0.005584716796875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 16 val loss: 4.21484375  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 17 train loss: 0.00531005859375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 17 val loss: 4.23828125  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 18 train loss: 0.0034160614013671875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 18 val loss: 4.28125  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 19 train loss: 0.002819061279296875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 19 val loss: 4.28125  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 0 train loss: 4.46484375  train f1: 0.00000  lr: 0.0003 batch: 32 
epochs: 0 val loss: 4.47265625  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 1 train loss: 4.40625  train f1: 0.00000  lr: 0.0003 batch: 32 
epochs: 1 val loss: 4.5078125  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 2 train loss: 3.232421875  train f1: 0.33333  lr: 0.0003 batch: 32 
epochs: 2 val loss: 4.5859375  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 3 train loss: 3.16796875  train f1: 0.55556  lr: 0.0003 batch: 32 
epochs: 3 val loss: 4.62890625  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 4 train loss: 2.962890625  train f1: 0.33333  lr: 0.0003 batch: 32 
epochs: 4 val loss: 4.67578125  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 5 train loss: 2.28515625  train f1: 0.55556  lr: 0.0003 batch: 32 
epochs: 5 val loss: 4.84375  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 6 train loss: 3.650390625  train f1: 0.37500  lr: 0.0003 batch: 32 
epochs: 6 val loss: 4.91796875  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 7 train loss: 1.53125  train f1: 0.55556  lr: 0.0003 batch: 32 
epochs: 7 val loss: 5.19140625  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 8 train loss: 2.095703125  train f1: 0.60000  lr: 0.0003 batch: 32 
epochs: 8 val loss: 5.65625  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 9 train loss: 2.447265625  train f1: 0.26667  lr: 0.0003 batch: 32 
epochs: 9 val loss: 6.26953125  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 10 train loss: 0.521484375  train f1: 0.55556  lr: 0.0003 batch: 32 
epochs: 10 val loss: 6.9296875  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 11 train loss: 0.79296875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 11 val loss: 7.76171875  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 12 train loss: 0.65380859375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 12 val loss: 8.71875  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 13 train loss: 0.056121826171875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 13 val loss: 9.546875  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 14 train loss: 0.0350341796875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 14 val loss: 10.3359375  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 15 train loss: 0.017486572265625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 15 val loss: 11.171875  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 16 train loss: 0.0106964111328125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 16 val loss: 11.984375  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 17 train loss: 0.009185791015625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 17 val loss: 12.796875  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 18 train loss: 0.1370849609375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 18 val loss: 14.359375  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 19 train loss: 0.003475189208984375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 19 val loss: 15.546875  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 0 train loss: 4.5078125  train f1: 0.00000  lr: 0.0003 batch: 32 
epochs: 0 val loss: 5.015625  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 1 train loss: 4.3671875  train f1: 0.00000  lr: 0.0003 batch: 32 
epochs: 1 val loss: 4.9375  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 2 train loss: 2.66796875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 2 val loss: 4.859375  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 3 train loss: 1.7060546875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 3 val loss: 4.7734375  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 4 train loss: 0.5712890625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 4 val loss: 4.625  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 5 train loss: 0.181640625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 5 val loss: 4.4453125  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 6 train loss: 0.053802490234375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 6 val loss: 4.3046875  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 7 train loss: 0.027984619140625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 7 val loss: 4.171875  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 8 train loss: 0.01160430908203125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 8 val loss: 4.1015625  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 9 train loss: 0.0084381103515625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 9 val loss: 4.046875  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 10 train loss: 0.00556182861328125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 10 val loss: 4.0078125  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 11 train loss: 0.0025577545166015625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 11 val loss: 3.98046875  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 12 train loss: 0.0048828125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 12 val loss: 3.9453125  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 13 train loss: 0.00243377685546875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 13 val loss: 3.91796875  val f1:0.14286  lr: 0.0003 batch: 32
epochs: 14 train loss: 0.00125885009765625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 14 val loss: 3.896484375  val f1:0.14286  lr: 0.0003 batch: 32
epochs: 15 train loss: 0.001796722412109375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 15 val loss: 3.884765625  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 16 train loss: 0.0009021759033203125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 16 val loss: 3.876953125  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 17 train loss: 0.0006151199340820312  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 17 val loss: 3.880859375  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 18 train loss: 0.0007066726684570312  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 18 val loss: 3.873046875  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 19 train loss: 0.0007157325744628906  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 19 val loss: 3.853515625  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 0 train loss: 4.48828125  train f1: 0.00000  lr: 0.0003 batch: 32 
epochs: 0 val loss: 4.4453125  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 1 train loss: 4.515625  train f1: 0.00000  lr: 0.0003 batch: 32 
epochs: 1 val loss: 4.3125  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 2 train loss: 3.080078125  train f1: 0.66667  lr: 0.0003 batch: 32 
epochs: 2 val loss: 4.1328125  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 3 train loss: 2.26953125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 3 val loss: 4.0078125  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 4 train loss: 1.1708984375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 4 val loss: 3.912109375  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 5 train loss: 0.375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 5 val loss: 3.8046875  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 6 train loss: 0.24267578125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 6 val loss: 3.7421875  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 7 train loss: 0.057647705078125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 7 val loss: 3.703125  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 8 train loss: 0.039764404296875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 8 val loss: 3.6484375  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 9 train loss: 0.01209259033203125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 9 val loss: 3.642578125  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 10 train loss: 0.00591278076171875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 10 val loss: 3.66796875  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 11 train loss: 0.0067596435546875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 11 val loss: 3.66015625  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 12 train loss: 0.0037403106689453125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 12 val loss: 3.671875  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 13 train loss: 0.00336456298828125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 13 val loss: 3.728515625  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 14 train loss: 0.0031185150146484375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 14 val loss: 3.73828125  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 15 train loss: 0.0018901824951171875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 15 val loss: 3.794921875  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 16 train loss: 0.00177764892578125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 16 val loss: 3.81640625  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 17 train loss: 0.0016078948974609375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 17 val loss: 3.828125  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 18 train loss: 0.0013141632080078125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 18 val loss: 3.853515625  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 19 train loss: 0.0011930465698242188  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 19 val loss: 3.837890625  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 0 train loss: 4.62109375  train f1: 0.00000  lr: 0.0003 batch: 32 
epochs: 0 val loss: 4.88671875  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 1 train loss: 4.5859375  train f1: 0.00000  lr: 0.0003 batch: 32 
epochs: 1 val loss: 4.796875  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 2 train loss: 3.16796875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 2 val loss: 4.7421875  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 3 train loss: 1.794921875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 3 val loss: 4.66015625  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 4 train loss: 0.8994140625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 4 val loss: 4.51171875  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 5 train loss: 0.35009765625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 5 val loss: 4.29296875  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 6 train loss: 0.119384765625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 6 val loss: 4.125  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 7 train loss: 0.044219970703125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 7 val loss: 4.00390625  val f1:0.08333  lr: 0.0003 batch: 32
epochs: 8 train loss: 0.0243682861328125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 8 val loss: 3.88671875  val f1:0.09524  lr: 0.0003 batch: 32
epochs: 9 train loss: 0.0100555419921875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 9 val loss: 3.8125  val f1:0.09524  lr: 0.0003 batch: 32
epochs: 10 train loss: 0.00742340087890625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 10 val loss: 3.7578125  val f1:0.08333  lr: 0.0003 batch: 32
epochs: 11 train loss: 0.005664825439453125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 11 val loss: 3.724609375  val f1:0.09524  lr: 0.0003 batch: 32
epochs: 12 train loss: 0.003765106201171875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 12 val loss: 3.701171875  val f1:0.27778  lr: 0.0003 batch: 32
epochs: 13 train loss: 0.00394439697265625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 13 val loss: 3.669921875  val f1:0.27778  lr: 0.0003 batch: 32
epochs: 14 train loss: 0.002410888671875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 14 val loss: 3.64453125  val f1:0.27778  lr: 0.0003 batch: 32
epochs: 15 train loss: 0.0018558502197265625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 15 val loss: 3.62890625  val f1:0.27778  lr: 0.0003 batch: 32
epochs: 16 train loss: 0.0013875961303710938  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 16 val loss: 3.623046875  val f1:0.27778  lr: 0.0003 batch: 32
epochs: 17 train loss: 0.0010900497436523438  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 17 val loss: 3.630859375  val f1:0.27778  lr: 0.0003 batch: 32
epochs: 18 train loss: 0.0025348663330078125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 18 val loss: 3.6328125  val f1:0.27778  lr: 0.0003 batch: 32
epochs: 19 train loss: 0.0008945465087890625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 19 val loss: 3.6328125  val f1:0.27778  lr: 0.0003 batch: 32
epochs: 0 train loss: 4.5078125  train f1: 0.00000  lr: 0.0003 batch: 32 
epochs: 0 val loss: 4.82421875  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 1 train loss: 4.45703125  train f1: 0.00000  lr: 0.0003 batch: 32 
epochs: 1 val loss: 4.640625  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 2 train loss: 2.810546875  train f1: 0.66667  lr: 0.0003 batch: 32 
epochs: 2 val loss: 4.4921875  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 3 train loss: 1.7509765625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 3 val loss: 4.34375  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 4 train loss: 1.416015625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 4 val loss: 4.25390625  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 5 train loss: 0.458740234375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 5 val loss: 4.21875  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 6 train loss: 0.154541015625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 6 val loss: 4.34375  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 7 train loss: 0.068115234375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 7 val loss: 4.40234375  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 8 train loss: 0.04730224609375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 8 val loss: 4.46484375  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 9 train loss: 0.0286712646484375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 9 val loss: 4.45703125  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 10 train loss: 0.01214599609375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 10 val loss: 4.44921875  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 11 train loss: 0.005634307861328125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 11 val loss: 4.44140625  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 12 train loss: 0.006076812744140625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 12 val loss: 4.43359375  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 13 train loss: 0.01180267333984375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 13 val loss: 4.40625  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 14 train loss: 0.0028820037841796875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 14 val loss: 4.3828125  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 15 train loss: 0.004047393798828125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 15 val loss: 4.359375  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 16 train loss: 0.0026149749755859375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 16 val loss: 4.296875  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 17 train loss: 0.00131988525390625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 17 val loss: 4.33984375  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 18 train loss: 0.0013093948364257812  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 18 val loss: 4.375  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 19 train loss: 0.0021533966064453125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 19 val loss: 4.3671875  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 0 train loss: 4.62109375  train f1: 0.00000  lr: 0.0003 batch: 32 
epochs: 0 val loss: 4.88671875  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 1 train loss: 4.5859375  train f1: 0.00000  lr: 0.0003 batch: 32 
epochs: 1 val loss: 4.796875  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 2 train loss: 3.16796875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 2 val loss: 4.7421875  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 3 train loss: 1.794921875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 3 val loss: 4.66015625  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 4 train loss: 0.8994140625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 4 val loss: 4.51171875  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 5 train loss: 0.35009765625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 5 val loss: 4.29296875  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 6 train loss: 0.119384765625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 6 val loss: 4.125  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 7 train loss: 0.044219970703125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 7 val loss: 4.00390625  val f1:0.08333  lr: 0.0003 batch: 32
epochs: 8 train loss: 0.0243682861328125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 8 val loss: 3.88671875  val f1:0.09524  lr: 0.0003 batch: 32
epochs: 9 train loss: 0.0100555419921875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 9 val loss: 3.8125  val f1:0.09524  lr: 0.0003 batch: 32
epochs: 10 train loss: 0.00742340087890625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 10 val loss: 3.7578125  val f1:0.08333  lr: 0.0003 batch: 32
epochs: 11 train loss: 0.005664825439453125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 11 val loss: 3.724609375  val f1:0.09524  lr: 0.0003 batch: 32
epochs: 12 train loss: 0.003765106201171875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 12 val loss: 3.701171875  val f1:0.27778  lr: 0.0003 batch: 32
epochs: 13 train loss: 0.00394439697265625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 13 val loss: 3.669921875  val f1:0.27778  lr: 0.0003 batch: 32
epochs: 14 train loss: 0.002410888671875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 14 val loss: 3.64453125  val f1:0.27778  lr: 0.0003 batch: 32
epochs: 15 train loss: 0.0018558502197265625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 15 val loss: 3.62890625  val f1:0.27778  lr: 0.0003 batch: 32
epochs: 16 train loss: 0.0013875961303710938  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 16 val loss: 3.623046875  val f1:0.27778  lr: 0.0003 batch: 32
epochs: 17 train loss: 0.0010900497436523438  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 17 val loss: 3.630859375  val f1:0.27778  lr: 0.0003 batch: 32
epochs: 18 train loss: 0.0025348663330078125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 18 val loss: 3.6328125  val f1:0.27778  lr: 0.0003 batch: 32
epochs: 19 train loss: 0.0008945465087890625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 19 val loss: 3.6328125  val f1:0.27778  lr: 0.0003 batch: 32
epochs: 0 train loss: 4.65625  train f1: 0.00000  lr: 0.0003 batch: 32 
epochs: 0 val loss: 4.7578125  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 1 train loss: 4.625  train f1: 0.00000  lr: 0.0003 batch: 32 
epochs: 1 val loss: 4.59375  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 2 train loss: 3.25390625  train f1: 0.40000  lr: 0.0003 batch: 32 
epochs: 2 val loss: 4.41796875  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 3 train loss: 1.693359375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 3 val loss: 4.2578125  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 4 train loss: 0.72265625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 4 val loss: 4.12109375  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 5 train loss: 0.30224609375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 5 val loss: 4.046875  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 6 train loss: 0.18408203125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 6 val loss: 4.0390625  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 7 train loss: 0.0364990234375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 7 val loss: 4.02734375  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 8 train loss: 0.034210205078125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 8 val loss: 4.0625  val f1:0.14286  lr: 0.0003 batch: 32
epochs: 9 train loss: 0.021392822265625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 9 val loss: 4.0703125  val f1:0.14286  lr: 0.0003 batch: 32
epochs: 10 train loss: 0.018096923828125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 10 val loss: 4.09375  val f1:0.14286  lr: 0.0003 batch: 32
epochs: 11 train loss: 0.00678253173828125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 11 val loss: 4.12109375  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 12 train loss: 0.00528717041015625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 12 val loss: 4.1328125  val f1:0.14286  lr: 0.0003 batch: 32
epochs: 13 train loss: 0.0069732666015625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 13 val loss: 4.140625  val f1:0.14286  lr: 0.0003 batch: 32
epochs: 14 train loss: 0.003627777099609375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 14 val loss: 4.18359375  val f1:0.14286  lr: 0.0003 batch: 32
epochs: 15 train loss: 0.003047943115234375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 15 val loss: 4.2734375  val f1:0.14286  lr: 0.0003 batch: 32
epochs: 16 train loss: 0.0020084381103515625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 16 val loss: 4.25  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 17 train loss: 0.001529693603515625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 17 val loss: 4.3359375  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 18 train loss: 0.0015239715576171875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 18 val loss: 4.3828125  val f1:0.14286  lr: 0.0003 batch: 32
epochs: 19 train loss: 0.0019426345825195312  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 19 val loss: 4.34765625  val f1:0.14286  lr: 0.0003 batch: 32
epochs: 0 train loss: 4.33984375  train f1: 0.00000  lr: 0.0003 batch: 32 
epochs: 0 val loss: 4.5  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 1 train loss: 4.3828125  train f1: 0.00000  lr: 0.0003 batch: 32 
epochs: 1 val loss: 4.3828125  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 2 train loss: 3.009765625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 2 val loss: 4.23828125  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 3 train loss: 1.7216796875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 3 val loss: 4.10546875  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 4 train loss: 1.10546875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 4 val loss: 3.9609375  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 5 train loss: 0.278564453125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 5 val loss: 3.935546875  val f1:0.14286  lr: 0.0003 batch: 32
epochs: 6 train loss: 0.177001953125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 6 val loss: 3.951171875  val f1:0.14286  lr: 0.0003 batch: 32
epochs: 7 train loss: 0.10723876953125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 7 val loss: 3.935546875  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 8 train loss: 0.044036865234375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 8 val loss: 3.931640625  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 9 train loss: 0.0152130126953125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 9 val loss: 3.92578125  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 10 train loss: 0.0152130126953125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 10 val loss: 3.939453125  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 11 train loss: 0.00795745849609375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 11 val loss: 3.9609375  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 12 train loss: 0.007518768310546875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 12 val loss: 3.890625  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 13 train loss: 0.004913330078125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 13 val loss: 3.87109375  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 14 train loss: 0.0025272369384765625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 14 val loss: 3.90625  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 15 train loss: 0.0017004013061523438  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 15 val loss: 3.916015625  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 16 train loss: 0.002216339111328125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 16 val loss: 3.87890625  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 17 train loss: 0.0014162063598632812  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 17 val loss: 3.892578125  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 18 train loss: 0.003414154052734375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 18 val loss: 3.98046875  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 19 train loss: 0.00146484375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 19 val loss: 3.978515625  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 0 train loss: 4.2890625  train f1: 0.00000  lr: 0.0003 batch: 32 
epochs: 0 val loss: 4.8671875  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 1 train loss: 4.21875  train f1: 0.00000  lr: 0.0003 batch: 32 
epochs: 1 val loss: 4.640625  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 2 train loss: 3.107421875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 2 val loss: 4.46875  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 3 train loss: 1.705078125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 3 val loss: 4.296875  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 4 train loss: 0.6484375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 4 val loss: 4.1796875  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 5 train loss: 0.3251953125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 5 val loss: 4.140625  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 6 train loss: 0.1343994140625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 6 val loss: 4.125  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 7 train loss: 0.05145263671875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 7 val loss: 4.12890625  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 8 train loss: 0.02813720703125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 8 val loss: 4.09765625  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 9 train loss: 0.0271453857421875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 9 val loss: 4.04296875  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 10 train loss: 0.01593017578125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 10 val loss: 3.98046875  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 11 train loss: 0.0110626220703125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 11 val loss: 3.935546875  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 12 train loss: 0.006961822509765625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 12 val loss: 3.919921875  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 13 train loss: 0.00882720947265625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 13 val loss: 3.951171875  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 14 train loss: 0.00760650634765625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 14 val loss: 3.9140625  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 15 train loss: 0.00408935546875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 15 val loss: 3.904296875  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 16 train loss: 0.0021038055419921875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 16 val loss: 3.896484375  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 17 train loss: 0.00186920166015625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 17 val loss: 3.939453125  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 18 train loss: 0.0014371871948242188  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 18 val loss: 4.015625  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 19 train loss: 0.001552581787109375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 19 val loss: 4.0078125  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 0 train loss: 4.62109375  train f1: 0.00000  lr: 0.0003 batch: 32 
epochs: 0 val loss: 4.703125  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 1 train loss: 4.59375  train f1: 0.00000  lr: 0.0003 batch: 32 
epochs: 1 val loss: 4.47265625  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 2 train loss: 3.291015625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 2 val loss: 4.28515625  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 3 train loss: 2.119140625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 3 val loss: 4.1328125  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 4 train loss: 0.78271484375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 4 val loss: 4.015625  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 5 train loss: 0.218994140625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 5 val loss: 3.865234375  val f1:0.14286  lr: 0.0003 batch: 32
epochs: 6 train loss: 0.28173828125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 6 val loss: 3.763671875  val f1:0.14286  lr: 0.0003 batch: 32
epochs: 7 train loss: 0.055145263671875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 7 val loss: 3.6328125  val f1:0.14286  lr: 0.0003 batch: 32
epochs: 8 train loss: 0.0265045166015625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 8 val loss: 3.55078125  val f1:0.14286  lr: 0.0003 batch: 32
epochs: 9 train loss: 0.0186920166015625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 9 val loss: 3.494140625  val f1:0.14286  lr: 0.0003 batch: 32
epochs: 10 train loss: 0.01126861572265625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 10 val loss: 3.451171875  val f1:0.14286  lr: 0.0003 batch: 32
epochs: 11 train loss: 0.006992340087890625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 11 val loss: 3.478515625  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 12 train loss: 0.00405120849609375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 12 val loss: 3.484375  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 13 train loss: 0.00255584716796875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 13 val loss: 3.525390625  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 14 train loss: 0.0025043487548828125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 14 val loss: 3.55078125  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 15 train loss: 0.0017709732055664062  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 15 val loss: 3.55078125  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 16 train loss: 0.0017690658569335938  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 16 val loss: 3.548828125  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 17 train loss: 0.0013599395751953125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 17 val loss: 3.537109375  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 18 train loss: 0.0011434555053710938  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 18 val loss: 3.517578125  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 19 train loss: 0.0009412765502929688  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 19 val loss: 3.5234375  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 0 train loss: 4.86328125  train f1: 0.00000  lr: 0.0003 batch: 32 
epochs: 0 val loss: 4.5234375  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 1 train loss: 4.92578125  train f1: 0.00000  lr: 0.0003 batch: 32 
epochs: 1 val loss: 4.44921875  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 2 train loss: 3.2890625  train f1: 0.66667  lr: 0.0003 batch: 32 
epochs: 2 val loss: 4.359375  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 3 train loss: 1.9130859375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 3 val loss: 4.25  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 4 train loss: 0.755859375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 4 val loss: 4.16796875  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 5 train loss: 0.54296875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 5 val loss: 4.01953125  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 6 train loss: 0.08099365234375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 6 val loss: 3.88671875  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 7 train loss: 0.09332275390625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 7 val loss: 3.80078125  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 8 train loss: 0.04803466796875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 8 val loss: 3.76171875  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 9 train loss: 0.01393890380859375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 9 val loss: 3.76953125  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 10 train loss: 0.0227813720703125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 10 val loss: 3.75  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 11 train loss: 0.015716552734375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 11 val loss: 3.73828125  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 12 train loss: 0.004039764404296875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 12 val loss: 3.771484375  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 13 train loss: 0.0076446533203125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 13 val loss: 3.73828125  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 14 train loss: 0.003253936767578125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 14 val loss: 3.779296875  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 15 train loss: 0.004817962646484375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 15 val loss: 3.75  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 16 train loss: 0.0013551712036132812  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 16 val loss: 3.755859375  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 17 train loss: 0.0008935928344726562  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 17 val loss: 3.8046875  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 18 train loss: 0.0009360313415527344  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 18 val loss: 3.853515625  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 19 train loss: 0.0006246566772460938  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 19 val loss: 3.810546875  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 0 train loss: 4.78125  train f1: 0.00000  lr: 0.0003 batch: 32 
epochs: 0 val loss: 5.01171875  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 1 train loss: 4.8671875  train f1: 0.00000  lr: 0.0003 batch: 32 
epochs: 1 val loss: 4.8046875  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 2 train loss: 3.6015625  train f1: 0.40000  lr: 0.0003 batch: 32 
epochs: 2 val loss: 4.62109375  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 3 train loss: 1.66015625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 3 val loss: 4.42578125  val f1:0.14286  lr: 0.0003 batch: 32
epochs: 4 train loss: 1.322265625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 4 val loss: 4.28515625  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 5 train loss: 0.41943359375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 5 val loss: 4.16796875  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 6 train loss: 0.11572265625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 6 val loss: 4.13671875  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 7 train loss: 0.06854248046875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 7 val loss: 4.19921875  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 8 train loss: 0.048126220703125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 8 val loss: 4.1796875  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 9 train loss: 0.0201263427734375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 9 val loss: 4.09375  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 10 train loss: 0.027008056640625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 10 val loss: 4.03515625  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 11 train loss: 0.012664794921875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 11 val loss: 4.0  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 12 train loss: 0.005565643310546875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 12 val loss: 3.990234375  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 13 train loss: 0.00394439697265625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 13 val loss: 3.98046875  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 14 train loss: 0.00264739990234375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 14 val loss: 3.986328125  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 15 train loss: 0.001468658447265625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 15 val loss: 3.982421875  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 16 train loss: 0.001522064208984375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 16 val loss: 3.939453125  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 17 train loss: 0.0010356903076171875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 17 val loss: 3.908203125  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 18 train loss: 0.0009608268737792969  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 18 val loss: 3.88671875  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 19 train loss: 0.0011196136474609375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 19 val loss: 3.86328125  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 0 train loss: 4.265625  train f1: 0.00000  lr: 0.0003 batch: 32 
epochs: 0 val loss: 5.015625  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 1 train loss: 4.2109375  train f1: 0.00000  lr: 0.0003 batch: 32 
epochs: 1 val loss: 4.85546875  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 2 train loss: 2.6328125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 2 val loss: 4.68359375  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 3 train loss: 1.4921875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 3 val loss: 4.5703125  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 4 train loss: 0.5068359375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 4 val loss: 4.44140625  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 5 train loss: 0.202392578125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 5 val loss: 4.29296875  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 6 train loss: 0.08197021484375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 6 val loss: 4.15625  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 7 train loss: 0.026580810546875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 7 val loss: 3.990234375  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 8 train loss: 0.033477783203125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 8 val loss: 3.89453125  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 9 train loss: 0.0136871337890625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 9 val loss: 3.81640625  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 10 train loss: 0.00716400146484375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 10 val loss: 3.736328125  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 11 train loss: 0.00583648681640625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 11 val loss: 3.70703125  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 12 train loss: 0.004550933837890625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 12 val loss: 3.669921875  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 13 train loss: 0.0015840530395507812  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 13 val loss: 3.609375  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 14 train loss: 0.0014371871948242188  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 14 val loss: 3.576171875  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 15 train loss: 0.0015010833740234375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 15 val loss: 3.587890625  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 16 train loss: 0.00193023681640625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 16 val loss: 3.51953125  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 17 train loss: 0.0007328987121582031  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 17 val loss: 3.45703125  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 18 train loss: 0.0009975433349609375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 18 val loss: 3.435546875  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 19 train loss: 0.0005984306335449219  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 19 val loss: 3.390625  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 0 train loss: 4.6640625  train f1: 0.00000  lr: 0.0003 batch: 32 
epochs: 0 val loss: 5.12890625  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 1 train loss: 4.75  train f1: 0.00000  lr: 0.0003 batch: 32 
epochs: 1 val loss: 5.0  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 2 train loss: 2.85546875  train f1: 0.66667  lr: 0.0003 batch: 32 
epochs: 2 val loss: 4.85546875  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 3 train loss: 1.7265625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 3 val loss: 4.68359375  val f1:0.14286  lr: 0.0003 batch: 32
epochs: 4 train loss: 0.82421875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 4 val loss: 4.45703125  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 5 train loss: 0.30517578125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 5 val loss: 4.27734375  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 6 train loss: 0.11480712890625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 6 val loss: 4.17578125  val f1:0.09524  lr: 0.0003 batch: 32
epochs: 7 train loss: 0.083740234375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 7 val loss: 4.1484375  val f1:0.09524  lr: 0.0003 batch: 32
epochs: 8 train loss: 0.02423095703125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 8 val loss: 4.1328125  val f1:0.09524  lr: 0.0003 batch: 32
epochs: 9 train loss: 0.01641845703125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 9 val loss: 4.12109375  val f1:0.08333  lr: 0.0003 batch: 32
epochs: 10 train loss: 0.03094482421875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 10 val loss: 4.10546875  val f1:0.08333  lr: 0.0003 batch: 32
epochs: 11 train loss: 0.01024627685546875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 11 val loss: 4.171875  val f1:0.08333  lr: 0.0003 batch: 32
epochs: 12 train loss: 0.008026123046875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 12 val loss: 4.16015625  val f1:0.08333  lr: 0.0003 batch: 32
epochs: 13 train loss: 0.00550079345703125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 13 val loss: 4.15234375  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 14 train loss: 0.006183624267578125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 14 val loss: 4.14453125  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 15 train loss: 0.005397796630859375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 15 val loss: 4.140625  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 16 train loss: 0.00229644775390625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 16 val loss: 4.12890625  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 17 train loss: 0.0026226043701171875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 17 val loss: 4.1171875  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 18 train loss: 0.0014219284057617188  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 18 val loss: 4.08203125  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 19 train loss: 0.0014429092407226562  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 19 val loss: 4.04296875  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 0 train loss: 4.48828125  train f1: 0.00000  lr: 0.0003 batch: 32 
epochs: 0 val loss: 4.4453125  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 1 train loss: 4.515625  train f1: 0.00000  lr: 0.0003 batch: 32 
epochs: 1 val loss: 4.3125  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 2 train loss: 3.080078125  train f1: 0.66667  lr: 0.0003 batch: 32 
epochs: 2 val loss: 4.1328125  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 3 train loss: 2.26953125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 3 val loss: 4.0078125  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 4 train loss: 1.1708984375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 4 val loss: 3.912109375  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 5 train loss: 0.375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 5 val loss: 3.8046875  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 6 train loss: 0.24267578125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 6 val loss: 3.7421875  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 7 train loss: 0.057647705078125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 7 val loss: 3.703125  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 8 train loss: 0.039764404296875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 8 val loss: 3.6484375  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 9 train loss: 0.01209259033203125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 9 val loss: 3.642578125  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 10 train loss: 0.00591278076171875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 10 val loss: 3.66796875  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 11 train loss: 0.0067596435546875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 11 val loss: 3.66015625  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 12 train loss: 0.0037403106689453125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 12 val loss: 3.671875  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 13 train loss: 0.00336456298828125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 13 val loss: 3.728515625  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 14 train loss: 0.0031185150146484375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 14 val loss: 3.73828125  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 15 train loss: 0.0018901824951171875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 15 val loss: 3.794921875  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 16 train loss: 0.00177764892578125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 16 val loss: 3.81640625  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 17 train loss: 0.0016078948974609375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 17 val loss: 3.828125  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 18 train loss: 0.0013141632080078125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 18 val loss: 3.853515625  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 19 train loss: 0.0011930465698242188  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 19 val loss: 3.837890625  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 0 train loss: 4.29296875  train f1: 0.00000  lr: 0.0003 batch: 32 
epochs: 0 val loss: 4.8984375  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 1 train loss: 4.44140625  train f1: 0.00000  lr: 0.0003 batch: 32 
epochs: 1 val loss: 4.81640625  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 2 train loss: 2.830078125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 2 val loss: 4.69921875  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 3 train loss: 2.078125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 3 val loss: 4.6171875  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 4 train loss: 0.943359375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 4 val loss: 4.5390625  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 5 train loss: 0.328857421875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 5 val loss: 4.46875  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 6 train loss: 0.18017578125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 6 val loss: 4.34375  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 7 train loss: 0.068603515625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 7 val loss: 4.26171875  val f1:0.14286  lr: 0.0003 batch: 32
epochs: 8 train loss: 0.035736083984375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 8 val loss: 4.26171875  val f1:0.14286  lr: 0.0003 batch: 32
epochs: 9 train loss: 0.0218353271484375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 9 val loss: 4.26953125  val f1:0.14286  lr: 0.0003 batch: 32
epochs: 10 train loss: 0.01416778564453125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 10 val loss: 4.26953125  val f1:0.14286  lr: 0.0003 batch: 32
epochs: 11 train loss: 0.01053619384765625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 11 val loss: 4.296875  val f1:0.14286  lr: 0.0003 batch: 32
epochs: 12 train loss: 0.00978851318359375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 12 val loss: 4.27734375  val f1:0.14286  lr: 0.0003 batch: 32
epochs: 13 train loss: 0.0053558349609375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 13 val loss: 4.265625  val f1:0.14286  lr: 0.0003 batch: 32
epochs: 14 train loss: 0.003559112548828125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 14 val loss: 4.234375  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 15 train loss: 0.002513885498046875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 15 val loss: 4.203125  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 16 train loss: 0.0037975311279296875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 16 val loss: 4.19921875  val f1:0.14286  lr: 0.0003 batch: 32
epochs: 17 train loss: 0.0020351409912109375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 17 val loss: 4.24609375  val f1:0.14286  lr: 0.0003 batch: 32
epochs: 18 train loss: 0.0017786026000976562  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 18 val loss: 4.24609375  val f1:0.14286  lr: 0.0003 batch: 32
epochs: 19 train loss: 0.001094818115234375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 19 val loss: 4.31640625  val f1:0.14286  lr: 0.0003 batch: 32
epochs: 0 train loss: 5.0078125  train f1: 0.00000  lr: 0.0003 batch: 32 
epochs: 0 val loss: 5.3046875  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 1 train loss: 5.015625  train f1: 0.00000  lr: 0.0003 batch: 32 
epochs: 1 val loss: 5.05078125  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 2 train loss: 3.3671875  train f1: 0.66667  lr: 0.0003 batch: 32 
epochs: 2 val loss: 4.8515625  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 3 train loss: 2.140625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 3 val loss: 4.6875  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 4 train loss: 1.4453125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 4 val loss: 4.54296875  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 5 train loss: 0.62451171875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 5 val loss: 4.3984375  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 6 train loss: 0.177978515625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 6 val loss: 4.19140625  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 7 train loss: 0.1290283203125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 7 val loss: 4.01953125  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 8 train loss: 0.02874755859375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 8 val loss: 3.859375  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 9 train loss: 0.042022705078125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 9 val loss: 3.75  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 10 train loss: 0.016937255859375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 10 val loss: 3.685546875  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 11 train loss: 0.0186920166015625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 11 val loss: 3.591796875  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 12 train loss: 0.0041046142578125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 12 val loss: 3.55078125  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 13 train loss: 0.002300262451171875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 13 val loss: 3.556640625  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 14 train loss: 0.00199127197265625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 14 val loss: 3.5625  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 15 train loss: 0.0018701553344726562  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 15 val loss: 3.556640625  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 16 train loss: 0.0018167495727539062  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 16 val loss: 3.556640625  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 17 train loss: 0.00186920166015625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 17 val loss: 3.5625  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 18 train loss: 0.0018587112426757812  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 18 val loss: 3.611328125  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 19 train loss: 0.0013866424560546875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 19 val loss: 3.697265625  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 0 train loss: 4.828125  train f1: 0.00000  lr: 0.0003 batch: 32 
epochs: 0 val loss: 4.62890625  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 1 train loss: 4.8828125  train f1: 0.00000  lr: 0.0003 batch: 32 
epochs: 1 val loss: 4.515625  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 2 train loss: 3.365234375  train f1: 0.20000  lr: 0.0003 batch: 32 
epochs: 2 val loss: 4.44140625  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 3 train loss: 2.3828125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 3 val loss: 4.37109375  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 4 train loss: 0.88427734375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 4 val loss: 4.2734375  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 5 train loss: 0.35791015625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 5 val loss: 4.1953125  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 6 train loss: 0.176025390625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 6 val loss: 4.1328125  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 7 train loss: 0.0992431640625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 7 val loss: 4.0546875  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 8 train loss: 0.04058837890625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 8 val loss: 3.97265625  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 9 train loss: 0.0242156982421875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 9 val loss: 3.89453125  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 10 train loss: 0.011138916015625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 10 val loss: 3.822265625  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 11 train loss: 0.0073089599609375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 11 val loss: 3.74609375  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 12 train loss: 0.004756927490234375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 12 val loss: 3.728515625  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 13 train loss: 0.00749969482421875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 13 val loss: 3.69921875  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 14 train loss: 0.002933502197265625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 14 val loss: 3.68359375  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 15 train loss: 0.0024318695068359375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 15 val loss: 3.6875  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 16 train loss: 0.0009965896606445312  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 16 val loss: 3.6953125  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 17 train loss: 0.0011034011840820312  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 17 val loss: 3.71875  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 18 train loss: 0.0012111663818359375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 18 val loss: 3.728515625  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 19 train loss: 0.00069427490234375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 19 val loss: 3.73828125  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 0 train loss: 4.3984375  train f1: 0.00000  lr: 0.0003 batch: 32 
epochs: 0 val loss: 4.6015625  val f1:0.08333  lr: 0.0003 batch: 32
epochs: 1 train loss: 4.43359375  train f1: 0.00000  lr: 0.0003 batch: 32 
epochs: 1 val loss: 4.484375  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 2 train loss: 2.861328125  train f1: 0.50000  lr: 0.0003 batch: 32 
epochs: 2 val loss: 4.31640625  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 3 train loss: 2.41796875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 3 val loss: 4.2109375  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 4 train loss: 1.0  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 4 val loss: 3.986328125  val f1:0.14286  lr: 0.0003 batch: 32
epochs: 5 train loss: 0.456298828125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 5 val loss: 3.8515625  val f1:0.14286  lr: 0.0003 batch: 32
epochs: 6 train loss: 0.12335205078125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 6 val loss: 3.826171875  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 7 train loss: 0.1434326171875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 7 val loss: 3.875  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 8 train loss: 0.025848388671875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 8 val loss: 4.01953125  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 9 train loss: 0.02630615234375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 9 val loss: 4.0234375  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 10 train loss: 0.01557159423828125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 10 val loss: 4.01171875  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 11 train loss: 0.00991058349609375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 11 val loss: 4.03125  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 12 train loss: 0.00640869140625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 12 val loss: 4.01953125  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 13 train loss: 0.005970001220703125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 13 val loss: 4.03515625  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 14 train loss: 0.00771331787109375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 14 val loss: 4.04296875  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 15 train loss: 0.00417327880859375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 15 val loss: 3.994140625  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 16 train loss: 0.004791259765625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 16 val loss: 3.943359375  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 17 train loss: 0.00252532958984375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 17 val loss: 3.9765625  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 18 train loss: 0.002044677734375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 18 val loss: 3.92578125  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 19 train loss: 0.0022716522216796875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 19 val loss: 3.87890625  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 0 train loss: 4.5625  train f1: 0.00000  lr: 0.0003 batch: 32 
epochs: 0 val loss: 5.1015625  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 1 train loss: 4.4921875  train f1: 0.00000  lr: 0.0003 batch: 32 
epochs: 1 val loss: 4.90234375  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 2 train loss: 2.94921875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 2 val loss: 4.71484375  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 3 train loss: 1.4814453125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 3 val loss: 4.5  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 4 train loss: 0.7412109375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 4 val loss: 4.24609375  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 5 train loss: 0.337158203125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 5 val loss: 3.955078125  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 6 train loss: 0.19580078125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 6 val loss: 3.775390625  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 7 train loss: 0.06561279296875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 7 val loss: 3.6640625  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 8 train loss: 0.03704833984375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 8 val loss: 3.57421875  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 9 train loss: 0.0152130126953125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 9 val loss: 3.5234375  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 10 train loss: 0.00946807861328125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 10 val loss: 3.5390625  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 11 train loss: 0.0038585662841796875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 11 val loss: 3.544921875  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 12 train loss: 0.001956939697265625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 12 val loss: 3.5546875  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 13 train loss: 0.00142669677734375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 13 val loss: 3.58203125  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 14 train loss: 0.001575469970703125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 14 val loss: 3.599609375  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 15 train loss: 0.0009331703186035156  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 15 val loss: 3.587890625  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 16 train loss: 0.0017461776733398438  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 16 val loss: 3.615234375  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 17 train loss: 0.0011014938354492188  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 17 val loss: 3.623046875  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 18 train loss: 0.0004496574401855469  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 18 val loss: 3.6328125  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 19 train loss: 0.0007719993591308594  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 19 val loss: 3.6171875  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 0 train loss: 4.390625  train f1: 0.00000  lr: 0.0003 batch: 32 
epochs: 0 val loss: 3.970703125  val f1:0.08333  lr: 0.0003 batch: 32
epochs: 1 train loss: 4.375  train f1: 0.00000  lr: 0.0003 batch: 32 
epochs: 1 val loss: 3.849609375  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 2 train loss: 3.044921875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 2 val loss: 3.80078125  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 3 train loss: 1.4677734375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 3 val loss: 3.763671875  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 4 train loss: 0.6748046875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 4 val loss: 3.767578125  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 5 train loss: 0.192626953125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 5 val loss: 3.744140625  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 6 train loss: 0.07940673828125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 6 val loss: 3.73046875  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 7 train loss: 0.09564208984375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 7 val loss: 3.693359375  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 8 train loss: 0.0240631103515625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 8 val loss: 3.630859375  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 9 train loss: 0.030059814453125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 9 val loss: 3.5703125  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 10 train loss: 0.0101165771484375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 10 val loss: 3.55078125  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 11 train loss: 0.0034923553466796875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 11 val loss: 3.517578125  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 12 train loss: 0.003047943115234375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 12 val loss: 3.505859375  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 13 train loss: 0.029937744140625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 13 val loss: 3.490234375  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 14 train loss: 0.0105438232421875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 14 val loss: 3.478515625  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 15 train loss: 0.004840850830078125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 15 val loss: 3.462890625  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 16 train loss: 0.004150390625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 16 val loss: 3.45703125  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 17 train loss: 0.00223541259765625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 17 val loss: 3.451171875  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 18 train loss: 0.001995086669921875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 18 val loss: 3.451171875  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 19 train loss: 0.001430511474609375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 19 val loss: 3.46484375  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 0 train loss: 4.609375  train f1: 0.00000  lr: 0.0003 batch: 32 
epochs: 0 val loss: 4.54296875  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 1 train loss: 4.66796875  train f1: 0.00000  lr: 0.0003 batch: 32 
epochs: 1 val loss: 4.45703125  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 2 train loss: 2.419921875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 2 val loss: 4.35546875  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 3 train loss: 2.01171875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 3 val loss: 4.29296875  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 4 train loss: 0.78857421875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 4 val loss: 4.18359375  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 5 train loss: 0.1585693359375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 5 val loss: 4.0390625  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 6 train loss: 0.271728515625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 6 val loss: 3.98828125  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 7 train loss: 0.0888671875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 7 val loss: 3.927734375  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 8 train loss: 0.040313720703125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 8 val loss: 3.90625  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 9 train loss: 0.018218994140625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 9 val loss: 3.8828125  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 10 train loss: 0.008209228515625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 10 val loss: 3.86328125  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 11 train loss: 0.006137847900390625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 11 val loss: 3.83203125  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 12 train loss: 0.007724761962890625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 12 val loss: 3.81640625  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 13 train loss: 0.003818511962890625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 13 val loss: 3.796875  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 14 train loss: 0.002361297607421875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 14 val loss: 3.80078125  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 15 train loss: 0.0030670166015625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 15 val loss: 3.779296875  val f1:0.14286  lr: 0.0003 batch: 32
epochs: 16 train loss: 0.002460479736328125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 16 val loss: 3.775390625  val f1:0.14286  lr: 0.0003 batch: 32
epochs: 17 train loss: 0.002368927001953125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 17 val loss: 3.7421875  val f1:0.14286  lr: 0.0003 batch: 32
epochs: 18 train loss: 0.0012350082397460938  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 18 val loss: 3.72265625  val f1:0.14286  lr: 0.0003 batch: 32
epochs: 19 train loss: 0.0010175704956054688  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 19 val loss: 3.72265625  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 0 train loss: 4.8046875  train f1: 0.00000  lr: 0.0003 batch: 32 
epochs: 0 val loss: 4.74609375  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 1 train loss: 4.9296875  train f1: 0.00000  lr: 0.0003 batch: 32 
epochs: 1 val loss: 4.68359375  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 2 train loss: 3.529296875  train f1: 0.16667  lr: 0.0003 batch: 32 
epochs: 2 val loss: 4.62890625  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 3 train loss: 2.109375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 3 val loss: 4.5859375  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 4 train loss: 0.85546875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 4 val loss: 4.51953125  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 5 train loss: 0.23388671875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 5 val loss: 4.49609375  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 6 train loss: 0.2017822265625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 6 val loss: 4.51171875  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 7 train loss: 0.08404541015625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 7 val loss: 4.53515625  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 8 train loss: 0.049591064453125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 8 val loss: 4.49609375  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 9 train loss: 0.01322174072265625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 9 val loss: 4.47265625  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 10 train loss: 0.0229949951171875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 10 val loss: 4.37890625  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 11 train loss: 0.0053863525390625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 11 val loss: 4.30078125  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 12 train loss: 0.006664276123046875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 12 val loss: 4.2421875  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 13 train loss: 0.00405120849609375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 13 val loss: 4.171875  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 14 train loss: 0.0034122467041015625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 14 val loss: 4.109375  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 15 train loss: 0.0013561248779296875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 15 val loss: 4.07421875  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 16 train loss: 0.001171112060546875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 16 val loss: 4.04296875  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 17 train loss: 0.0010814666748046875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 17 val loss: 4.02734375  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 18 train loss: 0.0014209747314453125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 18 val loss: 4.02734375  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 19 train loss: 0.000949859619140625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 19 val loss: 4.0546875  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 0 train loss: 4.5703125  train f1: 0.00000  lr: 0.0003 batch: 32 
epochs: 0 val loss: 5.15625  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 1 train loss: 4.4609375  train f1: 0.00000  lr: 0.0003 batch: 32 
epochs: 1 val loss: 4.9921875  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 2 train loss: 3.05859375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 2 val loss: 4.85546875  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 3 train loss: 1.6396484375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 3 val loss: 4.671875  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 4 train loss: 0.79736328125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 4 val loss: 4.36328125  val f1:0.23810  lr: 0.0003 batch: 32
epochs: 5 train loss: 0.27294921875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 5 val loss: 4.11328125  val f1:0.23810  lr: 0.0003 batch: 32
epochs: 6 train loss: 0.12060546875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 6 val loss: 4.03125  val f1:0.23810  lr: 0.0003 batch: 32
epochs: 7 train loss: 0.0428466796875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 7 val loss: 3.970703125  val f1:0.23810  lr: 0.0003 batch: 32
epochs: 8 train loss: 0.0230865478515625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 8 val loss: 3.98828125  val f1:0.23810  lr: 0.0003 batch: 32
epochs: 9 train loss: 0.027740478515625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 9 val loss: 3.96875  val f1:0.14286  lr: 0.0003 batch: 32
epochs: 10 train loss: 0.0084686279296875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 10 val loss: 3.95703125  val f1:0.14286  lr: 0.0003 batch: 32
epochs: 11 train loss: 0.00612640380859375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 11 val loss: 3.97265625  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 12 train loss: 0.005462646484375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 12 val loss: 4.00390625  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 13 train loss: 0.0037899017333984375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 13 val loss: 4.015625  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 14 train loss: 0.002288818359375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 14 val loss: 4.01171875  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 15 train loss: 0.002536773681640625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 15 val loss: 4.015625  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 16 train loss: 0.0021610260009765625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 16 val loss: 4.0234375  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 17 train loss: 0.0015239715576171875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 17 val loss: 4.05859375  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 18 train loss: 0.001293182373046875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 18 val loss: 4.01953125  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 19 train loss: 0.001644134521484375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 19 val loss: 3.998046875  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 0 train loss: 4.0546875  train f1: 0.00000  lr: 0.0003 batch: 32 
epochs: 0 val loss: 4.37890625  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 1 train loss: 4.140625  train f1: 0.00000  lr: 0.0003 batch: 32 
epochs: 1 val loss: 4.2265625  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 2 train loss: 2.41796875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 2 val loss: 4.04296875  val f1:0.14286  lr: 0.0003 batch: 32
epochs: 3 train loss: 1.09375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 3 val loss: 3.869140625  val f1:0.14286  lr: 0.0003 batch: 32
epochs: 4 train loss: 0.40771484375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 4 val loss: 3.71875  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 5 train loss: 0.318115234375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 5 val loss: 3.7265625  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 6 train loss: 0.08026123046875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 6 val loss: 3.763671875  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 7 train loss: 0.03912353515625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 7 val loss: 3.791015625  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 8 train loss: 0.0270843505859375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 8 val loss: 3.74609375  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 9 train loss: 0.0144500732421875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 9 val loss: 3.732421875  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 10 train loss: 0.047882080078125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 10 val loss: 3.693359375  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 11 train loss: 0.020599365234375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 11 val loss: 3.705078125  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 12 train loss: 0.0029773712158203125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 12 val loss: 3.767578125  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 13 train loss: 0.007965087890625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 13 val loss: 3.77734375  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 14 train loss: 0.001667022705078125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 14 val loss: 3.8203125  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 15 train loss: 0.0006375312805175781  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 15 val loss: 3.908203125  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 16 train loss: 0.0008778572082519531  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 16 val loss: 3.919921875  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 17 train loss: 0.0017833709716796875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 17 val loss: 3.92578125  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 18 train loss: 0.00102996826171875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 18 val loss: 3.904296875  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 19 train loss: 0.0005049705505371094  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 19 val loss: 3.921875  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 0 train loss: 4.60546875  train f1: 0.00000  lr: 0.0003 batch: 32 
epochs: 0 val loss: 5.23828125  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 1 train loss: 4.6328125  train f1: 0.00000  lr: 0.0003 batch: 32 
epochs: 1 val loss: 4.9765625  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 2 train loss: 2.787109375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 2 val loss: 4.80859375  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 3 train loss: 1.005859375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 3 val loss: 4.59765625  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 4 train loss: 0.83544921875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 4 val loss: 4.31640625  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 5 train loss: 0.439453125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 5 val loss: 4.171875  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 6 train loss: 0.11102294921875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 6 val loss: 4.10546875  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 7 train loss: 0.0989990234375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 7 val loss: 4.0625  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 8 train loss: 0.026885986328125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 8 val loss: 4.02734375  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 9 train loss: 0.0205535888671875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 9 val loss: 3.9765625  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 10 train loss: 0.0152130126953125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 10 val loss: 3.931640625  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 11 train loss: 0.00943756103515625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 11 val loss: 3.923828125  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 12 train loss: 0.0086212158203125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 12 val loss: 3.94140625  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 13 train loss: 0.0037136077880859375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 13 val loss: 3.986328125  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 14 train loss: 0.004001617431640625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 14 val loss: 3.98828125  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 15 train loss: 0.002422332763671875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 15 val loss: 3.990234375  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 16 train loss: 0.00136566162109375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 16 val loss: 3.982421875  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 17 train loss: 0.001110076904296875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 17 val loss: 3.96875  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 18 train loss: 0.0008707046508789062  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 18 val loss: 4.015625  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 19 train loss: 0.0010957717895507812  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 19 val loss: 4.03515625  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 0 train loss: 4.203125  train f1: 0.00000  lr: 0.0003 batch: 32 
epochs: 0 val loss: 4.73046875  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 1 train loss: 4.25  train f1: 0.00000  lr: 0.0003 batch: 32 
epochs: 1 val loss: 4.55078125  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 2 train loss: 2.66015625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 2 val loss: 4.3828125  val f1:0.14286  lr: 0.0003 batch: 32
epochs: 3 train loss: 1.55078125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 3 val loss: 4.2578125  val f1:0.14286  lr: 0.0003 batch: 32
epochs: 4 train loss: 0.6015625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 4 val loss: 4.15625  val f1:0.14286  lr: 0.0003 batch: 32
epochs: 5 train loss: 0.5185546875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 5 val loss: 4.09375  val f1:0.14286  lr: 0.0003 batch: 32
epochs: 6 train loss: 0.1827392578125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 6 val loss: 4.05078125  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 7 train loss: 0.06787109375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 7 val loss: 4.02734375  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 8 train loss: 0.03985595703125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 8 val loss: 3.982421875  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 9 train loss: 0.03887939453125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 9 val loss: 3.93359375  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 10 train loss: 0.0135955810546875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 10 val loss: 3.904296875  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 11 train loss: 0.00649261474609375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 11 val loss: 3.86328125  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 12 train loss: 0.00689697265625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 12 val loss: 3.8671875  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 13 train loss: 0.0038928985595703125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 13 val loss: 3.87109375  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 14 train loss: 0.0031528472900390625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 14 val loss: 3.87890625  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 15 train loss: 0.00147247314453125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 15 val loss: 3.87890625  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 16 train loss: 0.0022792816162109375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 16 val loss: 3.890625  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 17 train loss: 0.0008983612060546875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 17 val loss: 3.8828125  val f1:0.14286  lr: 0.0003 batch: 32
epochs: 18 train loss: 0.0013036727905273438  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 18 val loss: 3.908203125  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 19 train loss: 0.0008106231689453125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 19 val loss: 3.927734375  val f1:0.14286  lr: 0.0003 batch: 32
epochs: 0 train loss: 3.859375  train f1: 0.11111  lr: 0.0003 batch: 32 
epochs: 0 val loss: 4.5703125  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 1 train loss: 3.875  train f1: 0.11111  lr: 0.0003 batch: 32 
epochs: 1 val loss: 4.46484375  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 2 train loss: 2.6640625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 2 val loss: 4.3515625  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 3 train loss: 1.287109375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 3 val loss: 4.234375  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 4 train loss: 0.79296875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 4 val loss: 4.15234375  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 5 train loss: 0.44189453125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 5 val loss: 4.078125  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 6 train loss: 0.1640625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 6 val loss: 4.09375  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 7 train loss: 0.047607421875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 7 val loss: 4.12109375  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 8 train loss: 0.03302001953125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 8 val loss: 4.06640625  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 9 train loss: 0.03240966796875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 9 val loss: 3.984375  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 10 train loss: 0.0275115966796875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 10 val loss: 3.931640625  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 11 train loss: 0.01336669921875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 11 val loss: 3.927734375  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 12 train loss: 0.012664794921875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 12 val loss: 3.916015625  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 13 train loss: 0.00817108154296875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 13 val loss: 3.955078125  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 14 train loss: 0.0037555694580078125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 14 val loss: 3.97265625  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 15 train loss: 0.0022029876708984375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 15 val loss: 4.08984375  val f1:0.14286  lr: 0.0003 batch: 32
epochs: 16 train loss: 0.0027484893798828125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 16 val loss: 4.1171875  val f1:0.14286  lr: 0.0003 batch: 32
epochs: 17 train loss: 0.0028705596923828125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 17 val loss: 4.265625  val f1:0.14286  lr: 0.0003 batch: 32
epochs: 18 train loss: 0.00174713134765625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 18 val loss: 4.40234375  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 19 train loss: 0.0009775161743164062  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 19 val loss: 4.53515625  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 0 train loss: 4.62109375  train f1: 0.00000  lr: 0.0003 batch: 32 
epochs: 0 val loss: 4.3046875  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 1 train loss: 4.5234375  train f1: 0.00000  lr: 0.0003 batch: 32 
epochs: 1 val loss: 4.2421875  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 2 train loss: 2.103515625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 2 val loss: 4.203125  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 3 train loss: 1.1962890625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 3 val loss: 4.0625  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 4 train loss: 0.437744140625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 4 val loss: 3.912109375  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 5 train loss: 0.19091796875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 5 val loss: 3.78515625  val f1:0.14286  lr: 0.0003 batch: 32
epochs: 6 train loss: 0.06732177734375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 6 val loss: 3.70703125  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 7 train loss: 0.036102294921875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 7 val loss: 3.693359375  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 8 train loss: 0.01629638671875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 8 val loss: 3.68359375  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 9 train loss: 0.00917816162109375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 9 val loss: 3.658203125  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 10 train loss: 0.00609588623046875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 10 val loss: 3.64453125  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 11 train loss: 0.0045013427734375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 11 val loss: 3.619140625  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 12 train loss: 0.005115509033203125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 12 val loss: 3.6171875  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 13 train loss: 0.004131317138671875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 13 val loss: 3.60546875  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 14 train loss: 0.0023632049560546875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 14 val loss: 3.578125  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 15 train loss: 0.0016260147094726562  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 15 val loss: 3.560546875  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 16 train loss: 0.0016260147094726562  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 16 val loss: 3.55078125  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 17 train loss: 0.0008292198181152344  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 17 val loss: 3.58203125  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 18 train loss: 0.0012941360473632812  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 18 val loss: 3.564453125  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 19 train loss: 0.0007185935974121094  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 19 val loss: 3.583984375  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 0 train loss: 4.421875  train f1: 0.00000  lr: 0.0003 batch: 32 
epochs: 0 val loss: 4.71484375  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 1 train loss: 4.55859375  train f1: 0.00000  lr: 0.0003 batch: 32 
epochs: 1 val loss: 4.54296875  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 2 train loss: 2.802734375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 2 val loss: 4.375  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 3 train loss: 1.7265625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 3 val loss: 4.25  val f1:0.25000  lr: 0.0003 batch: 32
epochs: 4 train loss: 0.7802734375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 4 val loss: 4.16796875  val f1:0.23810  lr: 0.0003 batch: 32
epochs: 5 train loss: 0.2529296875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 5 val loss: 4.09375  val f1:0.23810  lr: 0.0003 batch: 32
epochs: 6 train loss: 0.099609375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 6 val loss: 4.0234375  val f1:0.23810  lr: 0.0003 batch: 32
epochs: 7 train loss: 0.04583740234375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 7 val loss: 4.03125  val f1:0.23810  lr: 0.0003 batch: 32
epochs: 8 train loss: 0.0302581787109375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 8 val loss: 3.986328125  val f1:0.25000  lr: 0.0003 batch: 32
epochs: 9 train loss: 0.017791748046875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 9 val loss: 3.97265625  val f1:0.25000  lr: 0.0003 batch: 32
epochs: 10 train loss: 0.0092315673828125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 10 val loss: 3.939453125  val f1:0.25000  lr: 0.0003 batch: 32
epochs: 11 train loss: 0.0046539306640625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 11 val loss: 3.923828125  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 12 train loss: 0.00505828857421875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 12 val loss: 3.935546875  val f1:0.14286  lr: 0.0003 batch: 32
epochs: 13 train loss: 0.0056304931640625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 13 val loss: 3.9375  val f1:0.14286  lr: 0.0003 batch: 32
epochs: 14 train loss: 0.004802703857421875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 14 val loss: 3.923828125  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 15 train loss: 0.00319671630859375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 15 val loss: 3.94140625  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 16 train loss: 0.0021457672119140625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 16 val loss: 3.9921875  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 17 train loss: 0.0011653900146484375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 17 val loss: 4.015625  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 18 train loss: 0.0014123916625976562  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 18 val loss: 4.03125  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 19 train loss: 0.0011949539184570312  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 19 val loss: 4.0859375  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 0 train loss: 0.5920842472751845  train f1: 0.44341  lr: 0.0003 batch: 8 
epochs: 0 val loss: 1.0730538544831447  val f1:0.54995  lr: 0.0003 batch: 8
epochs: 1 train loss: 0.2184416055679318  train f1: 0.73064  lr: 0.0003 batch: 8 
epochs: 1 val loss: 0.8110257961131908  val f1:0.68297  lr: 0.0003 batch: 8
epochs: 2 train loss: 0.1416850953625325  train f1: 0.81801  lr: 0.0003 batch: 8 
epochs: 2 val loss: 0.7878433810340036  val f1:0.72854  lr: 0.0003 batch: 8
epochs: 3 train loss: 0.10536915784762094  train f1: 0.86528  lr: 0.0003 batch: 8 
epochs: 3 val loss: 0.8052694002787268  val f1:0.71468  lr: 0.0003 batch: 8
epochs: 4 train loss: 0.08249606730932  train f1: 0.90319  lr: 0.0003 batch: 8 
epochs: 4 val loss: 0.8886121003716081  val f1:0.74275  lr: 0.0003 batch: 8
epochs: 5 train loss: 0.07326794740862376  train f1: 0.91275  lr: 0.0003 batch: 8 
epochs: 5 val loss: 0.9879308082439278  val f1:0.71916  lr: 0.0003 batch: 8
epochs: 6 train loss: 0.061928020897054306  train f1: 0.93522  lr: 0.0003 batch: 8 
epochs: 6 val loss: 0.9703274885813398  val f1:0.76620  lr: 0.0003 batch: 8
epochs: 7 train loss: 0.059433901659271114  train f1: 0.93440  lr: 0.0003 batch: 8 
epochs: 7 val loss: 0.9612365236988775  val f1:0.73685  lr: 0.0003 batch: 8
epochs: 8 train loss: 0.039656026627952  train f1: 0.96175  lr: 0.0003 batch: 8 
epochs: 8 val loss: 0.9615356930979975  val f1:0.79475  lr: 0.0003 batch: 8
epochs: 9 train loss: 0.046818649233725244  train f1: 0.96018  lr: 0.0003 batch: 8 
epochs: 9 val loss: 1.0418511924920257  val f1:0.79208  lr: 0.0003 batch: 8
epochs: 10 train loss: 0.03807651881416541  train f1: 0.95419  lr: 0.0003 batch: 8 
epochs: 10 val loss: 1.142962004741032  val f1:0.75456  lr: 0.0003 batch: 8
epochs: 11 train loss: 0.030165154403284586  train f1: 0.96927  lr: 0.0003 batch: 8 
epochs: 11 val loss: 1.3189186365516101  val f1:0.76090  lr: 0.0003 batch: 8
epochs: 12 train loss: 0.045266132105021145  train f1: 0.95653  lr: 0.0003 batch: 8 
epochs: 12 val loss: 1.3344476827868699  val f1:0.74877  lr: 0.0003 batch: 8
epochs: 13 train loss: 0.039144953252965985  train f1: 0.95851  lr: 0.0003 batch: 8 
epochs: 13 val loss: 1.0653310303334835  val f1:0.76845  lr: 0.0003 batch: 8
epochs: 14 train loss: 0.036816055526459904  train f1: 0.96106  lr: 0.0003 batch: 8 
epochs: 14 val loss: 1.0571158060321104  val f1:0.76221  lr: 0.0003 batch: 8
epochs: 15 train loss: 0.023161774450109498  train f1: 0.97634  lr: 0.0003 batch: 8 
epochs: 15 val loss: 1.2765011573279352  val f1:0.76173  lr: 0.0003 batch: 8
epochs: 16 train loss: 0.035159325510486436  train f1: 0.95971  lr: 0.0003 batch: 8 
epochs: 16 val loss: 0.9169708859037475  val f1:0.80637  lr: 0.0003 batch: 8
epochs: 17 train loss: 0.027882456296399943  train f1: 0.97378  lr: 0.0003 batch: 8 
epochs: 17 val loss: 1.1550539509013853  val f1:0.77809  lr: 0.0003 batch: 8
epochs: 18 train loss: 0.02734810779070912  train f1: 0.97813  lr: 0.0003 batch: 8 
epochs: 18 val loss: 1.464297082026799  val f1:0.75558  lr: 0.0003 batch: 8
epochs: 19 train loss: 0.028539043077804218  train f1: 0.97426  lr: 0.0003 batch: 8 
epochs: 19 val loss: 1.1313402001504549  val f1:0.77891  lr: 0.0003 batch: 8
epochs: 0 train loss: 4.55859375  train f1: 0.00000  lr: 0.0003 batch: 8 
epochs: 0 val loss: 4.8828125  val f1:0.00000  lr: 0.0003 batch: 8
epochs: 0 train loss: 4.55859375  train f1: 0.00000  lr: 0.0003 batch: 8 
epochs: 0 val loss: 4.8828125  val f1:0.00000  lr: 0.0003 batch: 8
epochs: 1 train loss: 4.55859375  train f1: 0.00000  lr: 0.0003 batch: 8 
epochs: 1 val loss: 4.80859375  val f1:0.00000  lr: 0.0003 batch: 8
epochs: 0 train loss: 4.390625  train f1: 0.00000  lr: 0.0003 batch: 8 
epochs: 0 val loss: 4.796875  val f1:0.00000  lr: 0.0003 batch: 8
epochs: 1 train loss: 4.30078125  train f1: 0.00000  lr: 0.0003 batch: 8 
epochs: 1 val loss: 4.7890625  val f1:0.00000  lr: 0.0003 batch: 8
epochs: 2 train loss: 3.515625  train f1: 0.33333  lr: 0.0003 batch: 8 
epochs: 2 val loss: 4.78125  val f1:0.00000  lr: 0.0003 batch: 8
epochs: 0 train loss: 4.37109375  train f1: 0.00000  lr: 0.0003 batch: 8 
epochs: 0 val loss: 4.8046875  val f1:0.00000  lr: 0.0003 batch: 8
epochs: 1 train loss: 4.25  train f1: 0.00000  lr: 0.0003 batch: 8 
epochs: 1 val loss: 4.76171875  val f1:0.00000  lr: 0.0003 batch: 8
epochs: 2 train loss: 2.619140625  train f1: 1.00000  lr: 0.0003 batch: 8 
epochs: 2 val loss: 4.609375  val f1:0.12500  lr: 0.0003 batch: 8
epochs: 3 train loss: 0.9208984375  train f1: 1.00000  lr: 0.0003 batch: 8 
epochs: 3 val loss: 4.41015625  val f1:0.12500  lr: 0.0003 batch: 8
epochs: 4 train loss: 0.397705078125  train f1: 1.00000  lr: 0.0003 batch: 8 
epochs: 4 val loss: 4.15625  val f1:0.12500  lr: 0.0003 batch: 8
epochs: 5 train loss: 0.09356689453125  train f1: 1.00000  lr: 0.0003 batch: 8 
epochs: 5 val loss: 3.94921875  val f1:0.12500  lr: 0.0003 batch: 8
epochs: 6 train loss: 0.034393310546875  train f1: 1.00000  lr: 0.0003 batch: 8 
epochs: 6 val loss: 3.833984375  val f1:0.11111  lr: 0.0003 batch: 8
epochs: 7 train loss: 0.017730712890625  train f1: 1.00000  lr: 0.0003 batch: 8 
epochs: 7 val loss: 3.73828125  val f1:0.12500  lr: 0.0003 batch: 8
epochs: 8 train loss: 0.016571044921875  train f1: 1.00000  lr: 0.0003 batch: 8 
epochs: 8 val loss: 3.677734375  val f1:0.12500  lr: 0.0003 batch: 8
epochs: 9 train loss: 0.01079559326171875  train f1: 1.00000  lr: 0.0003 batch: 8 
epochs: 9 val loss: 3.671875  val f1:0.12500  lr: 0.0003 batch: 8
epochs: 10 train loss: 0.0042266845703125  train f1: 1.00000  lr: 0.0003 batch: 8 
epochs: 10 val loss: 3.666015625  val f1:0.11111  lr: 0.0003 batch: 8
epochs: 11 train loss: 0.001979827880859375  train f1: 1.00000  lr: 0.0003 batch: 8 
epochs: 11 val loss: 3.646484375  val f1:0.11111  lr: 0.0003 batch: 8
epochs: 12 train loss: 0.002506256103515625  train f1: 1.00000  lr: 0.0003 batch: 8 
epochs: 12 val loss: 3.62890625  val f1:0.11111  lr: 0.0003 batch: 8
epochs: 0 train loss: 4.3984375  train f1: 0.00000  lr: 0.0003 batch: 8 
epochs: 0 val loss: 4.81640625  val f1:0.00000  lr: 0.0003 batch: 8
epochs: 1 train loss: 4.25  train f1: 0.00000  lr: 0.0003 batch: 8 
epochs: 1 val loss: 4.7734375  val f1:0.00000  lr: 0.0003 batch: 8
epochs: 2 train loss: 2.705078125  train f1: 1.00000  lr: 0.0003 batch: 8 
epochs: 2 val loss: 4.64453125  val f1:0.12500  lr: 0.0003 batch: 8
epochs: 3 train loss: 1.05859375  train f1: 1.00000  lr: 0.0003 batch: 8 
epochs: 3 val loss: 4.453125  val f1:0.12500  lr: 0.0003 batch: 8
epochs: 4 train loss: 0.31494140625  train f1: 1.00000  lr: 0.0003 batch: 8 
epochs: 4 val loss: 4.19921875  val f1:0.12500  lr: 0.0003 batch: 8
epochs: 5 train loss: 0.12225341796875  train f1: 1.00000  lr: 0.0003 batch: 8 
epochs: 5 val loss: 4.02734375  val f1:0.12500  lr: 0.0003 batch: 8
epochs: 6 train loss: 0.042877197265625  train f1: 1.00000  lr: 0.0003 batch: 8 
epochs: 6 val loss: 3.92578125  val f1:0.11111  lr: 0.0003 batch: 8
epochs: 7 train loss: 0.018463134765625  train f1: 1.00000  lr: 0.0003 batch: 8 
epochs: 7 val loss: 3.8671875  val f1:0.11111  lr: 0.0003 batch: 8
epochs: 8 train loss: 0.0125885009765625  train f1: 1.00000  lr: 0.0003 batch: 8 
epochs: 8 val loss: 3.826171875  val f1:0.11111  lr: 0.0003 batch: 8
epochs: 9 train loss: 0.00479888916015625  train f1: 1.00000  lr: 0.0003 batch: 8 
epochs: 9 val loss: 3.818359375  val f1:0.11111  lr: 0.0003 batch: 8
epochs: 10 train loss: 0.0031585693359375  train f1: 1.00000  lr: 0.0003 batch: 8 
epochs: 10 val loss: 3.81640625  val f1:0.11111  lr: 0.0003 batch: 8
epochs: 11 train loss: 0.00560760498046875  train f1: 1.00000  lr: 0.0003 batch: 8 
epochs: 11 val loss: 3.798828125  val f1:0.11111  lr: 0.0003 batch: 8
epochs: 12 train loss: 0.0018148422241210938  train f1: 1.00000  lr: 0.0003 batch: 8 
epochs: 12 val loss: 3.771484375  val f1:0.11111  lr: 0.0003 batch: 8
epochs: 13 train loss: 0.0009350776672363281  train f1: 1.00000  lr: 0.0003 batch: 8 
epochs: 13 val loss: 3.76171875  val f1:0.11111  lr: 0.0003 batch: 8
epochs: 14 train loss: 0.0012750625610351562  train f1: 1.00000  lr: 0.0003 batch: 8 
epochs: 14 val loss: 3.783203125  val f1:0.12500  lr: 0.0003 batch: 8
epochs: 15 train loss: 0.0005784034729003906  train f1: 1.00000  lr: 0.0003 batch: 8 
epochs: 15 val loss: 3.822265625  val f1:0.12500  lr: 0.0003 batch: 8
epochs: 16 train loss: 0.0004248619079589844  train f1: 1.00000  lr: 0.0003 batch: 8 
epochs: 16 val loss: 3.84375  val f1:0.11111  lr: 0.0003 batch: 8
epochs: 17 train loss: 0.0005850791931152344  train f1: 1.00000  lr: 0.0003 batch: 8 
epochs: 17 val loss: 3.869140625  val f1:0.11111  lr: 0.0003 batch: 8
epochs: 18 train loss: 0.0003561973571777344  train f1: 1.00000  lr: 0.0003 batch: 8 
epochs: 18 val loss: 3.9140625  val f1:0.11111  lr: 0.0003 batch: 8
epochs: 19 train loss: 0.0002472400665283203  train f1: 1.00000  lr: 0.0003 batch: 8 
epochs: 19 val loss: 3.95703125  val f1:0.11111  lr: 0.0003 batch: 8
epochs: 0 train loss: 4.359375  train f1: 0.00000  lr: 0.0003 batch: 8 
epochs: 0 val loss: 4.80859375  val f1:0.00000  lr: 0.0003 batch: 8
epochs: 1 train loss: 4.30078125  train f1: 0.00000  lr: 0.0003 batch: 8 
epochs: 1 val loss: 4.796875  val f1:0.00000  lr: 0.0003 batch: 8
epochs: 2 train loss: 2.5390625  train f1: 1.00000  lr: 0.0003 batch: 8 
epochs: 2 val loss: 4.7109375  val f1:0.12500  lr: 0.0003 batch: 8
epochs: 3 train loss: 0.822265625  train f1: 1.00000  lr: 0.0003 batch: 8 
epochs: 3 val loss: 4.4765625  val f1:0.14286  lr: 0.0003 batch: 8
epochs: 4 train loss: 0.379150390625  train f1: 1.00000  lr: 0.0003 batch: 8 
epochs: 4 val loss: 4.22265625  val f1:0.14286  lr: 0.0003 batch: 8
epochs: 5 train loss: 0.09344482421875  train f1: 1.00000  lr: 0.0003 batch: 8 
epochs: 5 val loss: 4.03515625  val f1:0.12500  lr: 0.0003 batch: 8
epochs: 6 train loss: 0.0255126953125  train f1: 1.00000  lr: 0.0003 batch: 8 
epochs: 6 val loss: 3.90625  val f1:0.11111  lr: 0.0003 batch: 8
epochs: 7 train loss: 0.015655517578125  train f1: 1.00000  lr: 0.0003 batch: 8 
epochs: 7 val loss: 3.8203125  val f1:0.12500  lr: 0.0003 batch: 8
epochs: 0 train loss: 4.3671875  train f1: 0.00000  lr: 0.0003 batch: 8 
epochs: 0 val loss: 4.8203125  val f1:0.00000  lr: 0.0003 batch: 8
epochs: 1 train loss: 4.25  train f1: 0.00000  lr: 0.0003 batch: 8 
epochs: 1 val loss: 4.77734375  val f1:0.00000  lr: 0.0003 batch: 8
epochs: 2 train loss: 2.630859375  train f1: 1.00000  lr: 0.0003 batch: 8 
epochs: 2 val loss: 4.640625  val f1:0.12500  lr: 0.0003 batch: 8
epochs: 3 train loss: 0.99658203125  train f1: 1.00000  lr: 0.0003 batch: 8 
epochs: 3 val loss: 4.46484375  val f1:0.12500  lr: 0.0003 batch: 8
epochs: 4 train loss: 0.321044921875  train f1: 1.00000  lr: 0.0003 batch: 8 
epochs: 4 val loss: 4.2421875  val f1:0.12500  lr: 0.0003 batch: 8
epochs: 5 train loss: 0.11395263671875  train f1: 1.00000  lr: 0.0003 batch: 8 
epochs: 5 val loss: 4.08203125  val f1:0.14286  lr: 0.0003 batch: 8
epochs: 6 train loss: 0.035614013671875  train f1: 1.00000  lr: 0.0003 batch: 8 
epochs: 6 val loss: 3.998046875  val f1:0.12500  lr: 0.0003 batch: 8
epochs: 7 train loss: 0.0159454345703125  train f1: 1.00000  lr: 0.0003 batch: 8 
epochs: 7 val loss: 3.931640625  val f1:0.11111  lr: 0.0003 batch: 8
epochs: 8 train loss: 0.00936126708984375  train f1: 1.00000  lr: 0.0003 batch: 8 
epochs: 8 val loss: 3.8828125  val f1:0.11111  lr: 0.0003 batch: 8
epochs: 9 train loss: 0.004688262939453125  train f1: 1.00000  lr: 0.0003 batch: 8 
epochs: 9 val loss: 3.86328125  val f1:0.11111  lr: 0.0003 batch: 8
epochs: 10 train loss: 0.007389068603515625  train f1: 1.00000  lr: 0.0003 batch: 8 
epochs: 10 val loss: 3.86328125  val f1:0.11111  lr: 0.0003 batch: 8
epochs: 11 train loss: 0.0025768280029296875  train f1: 1.00000  lr: 0.0003 batch: 8 
epochs: 11 val loss: 3.83203125  val f1:0.11111  lr: 0.0003 batch: 8
epochs: 12 train loss: 0.0021572113037109375  train f1: 1.00000  lr: 0.0003 batch: 8 
epochs: 12 val loss: 3.783203125  val f1:0.11111  lr: 0.0003 batch: 8
epochs: 13 train loss: 0.00217437744140625  train f1: 1.00000  lr: 0.0003 batch: 8 
epochs: 13 val loss: 3.755859375  val f1:0.11111  lr: 0.0003 batch: 8
epochs: 14 train loss: 0.00257110595703125  train f1: 1.00000  lr: 0.0003 batch: 8 
epochs: 14 val loss: 3.759765625  val f1:0.11111  lr: 0.0003 batch: 8
epochs: 15 train loss: 0.00154876708984375  train f1: 1.00000  lr: 0.0003 batch: 8 
epochs: 15 val loss: 3.796875  val f1:0.11111  lr: 0.0003 batch: 8
epochs: 16 train loss: 0.0005078315734863281  train f1: 1.00000  lr: 0.0003 batch: 8 
epochs: 16 val loss: 3.80078125  val f1:0.11111  lr: 0.0003 batch: 8
epochs: 17 train loss: 0.0004591941833496094  train f1: 1.00000  lr: 0.0003 batch: 8 
epochs: 17 val loss: 3.818359375  val f1:0.12500  lr: 0.0003 batch: 8
epochs: 18 train loss: 0.0007581710815429688  train f1: 1.00000  lr: 0.0003 batch: 8 
epochs: 18 val loss: 3.837890625  val f1:0.12500  lr: 0.0003 batch: 8
epochs: 19 train loss: 0.0003120899200439453  train f1: 1.00000  lr: 0.0003 batch: 8 
epochs: 19 val loss: 3.853515625  val f1:0.12500  lr: 0.0003 batch: 8
epochs: 0 train loss: 4.3203125  train f1: 0.00000  lr: 0.0003 batch: 8 
epochs: 0 val loss: 4.8125  val f1:0.00000  lr: 0.0003 batch: 8
epochs: 1 train loss: 4.296875  train f1: 0.00000  lr: 0.0003 batch: 8 
epochs: 1 val loss: 4.796875  val f1:0.00000  lr: 0.0003 batch: 8
epochs: 2 train loss: 2.51953125  train f1: 0.66667  lr: 0.0003 batch: 8 
epochs: 2 val loss: 4.75  val f1:0.12500  lr: 0.0003 batch: 8
epochs: 3 train loss: 0.92138671875  train f1: 1.00000  lr: 0.0003 batch: 8 
epochs: 3 val loss: 4.55078125  val f1:0.14286  lr: 0.0003 batch: 8
epochs: 4 train loss: 0.544921875  train f1: 1.00000  lr: 0.0003 batch: 8 
epochs: 4 val loss: 4.359375  val f1:0.14286  lr: 0.0003 batch: 8
epochs: 5 train loss: 0.0970458984375  train f1: 1.00000  lr: 0.0003 batch: 8 
epochs: 5 val loss: 4.15234375  val f1:0.14286  lr: 0.0003 batch: 8
epochs: 6 train loss: 0.024810791015625  train f1: 1.00000  lr: 0.0003 batch: 8 
epochs: 6 val loss: 3.998046875  val f1:0.14286  lr: 0.0003 batch: 8
epochs: 7 train loss: 0.01873779296875  train f1: 1.00000  lr: 0.0003 batch: 8 
epochs: 7 val loss: 3.875  val f1:0.11111  lr: 0.0003 batch: 8
epochs: 8 train loss: 0.0157012939453125  train f1: 1.00000  lr: 0.0003 batch: 8 
epochs: 8 val loss: 3.80078125  val f1:0.11111  lr: 0.0003 batch: 8
epochs: 9 train loss: 0.006587982177734375  train f1: 1.00000  lr: 0.0003 batch: 8 
epochs: 9 val loss: 3.763671875  val f1:0.11111  lr: 0.0003 batch: 8
epochs: 10 train loss: 0.0043182373046875  train f1: 1.00000  lr: 0.0003 batch: 8 
epochs: 10 val loss: 3.767578125  val f1:0.11111  lr: 0.0003 batch: 8
epochs: 11 train loss: 0.00296783447265625  train f1: 1.00000  lr: 0.0003 batch: 8 
epochs: 11 val loss: 3.755859375  val f1:0.12500  lr: 0.0003 batch: 8
epochs: 12 train loss: 0.002475738525390625  train f1: 1.00000  lr: 0.0003 batch: 8 
epochs: 12 val loss: 3.783203125  val f1:0.11111  lr: 0.0003 batch: 8
epochs: 13 train loss: 0.0014028549194335938  train f1: 1.00000  lr: 0.0003 batch: 8 
epochs: 13 val loss: 3.7890625  val f1:0.11111  lr: 0.0003 batch: 8
epochs: 14 train loss: 0.0011796951293945312  train f1: 1.00000  lr: 0.0003 batch: 8 
epochs: 14 val loss: 3.802734375  val f1:0.11111  lr: 0.0003 batch: 8
epochs: 15 train loss: 0.0007977485656738281  train f1: 1.00000  lr: 0.0003 batch: 8 
epochs: 15 val loss: 3.8125  val f1:0.11111  lr: 0.0003 batch: 8
epochs: 16 train loss: 0.0005750656127929688  train f1: 1.00000  lr: 0.0003 batch: 8 
epochs: 16 val loss: 3.822265625  val f1:0.11111  lr: 0.0003 batch: 8
epochs: 17 train loss: 0.0004801750183105469  train f1: 1.00000  lr: 0.0003 batch: 8 
epochs: 17 val loss: 3.78125  val f1:0.11111  lr: 0.0003 batch: 8
epochs: 18 train loss: 0.0004911422729492188  train f1: 1.00000  lr: 0.0003 batch: 8 
epochs: 18 val loss: 3.787109375  val f1:0.11111  lr: 0.0003 batch: 8
epochs: 19 train loss: 0.00025844573974609375  train f1: 1.00000  lr: 0.0003 batch: 8 
epochs: 19 val loss: 3.755859375  val f1:0.11111  lr: 0.0003 batch: 8
epochs: 0 train loss: 4.2734375  train f1: 0.00000  lr: 0.0003 batch: 8 
epochs: 0 val loss: 4.8203125  val f1:0.00000  lr: 0.0003 batch: 8
epochs: 1 train loss: 4.296875  train f1: 0.00000  lr: 0.0003 batch: 8 
epochs: 1 val loss: 4.78125  val f1:0.00000  lr: 0.0003 batch: 8
epochs: 0 train loss: 4.34375  train f1: 0.00000  lr: 0.0003 batch: 8 
epochs: 0 val loss: 4.796875  val f1:0.00000  lr: 0.0003 batch: 8
epochs: 1 train loss: 4.296875  train f1: 0.00000  lr: 0.0003 batch: 8 
epochs: 1 val loss: 4.75390625  val f1:0.00000  lr: 0.0003 batch: 8
epochs: 2 train loss: 2.548828125  train f1: 1.00000  lr: 0.0003 batch: 8 
epochs: 2 val loss: 4.66015625  val f1:0.12500  lr: 0.0003 batch: 8
epochs: 3 train loss: 0.87255859375  train f1: 1.00000  lr: 0.0003 batch: 8 
epochs: 3 val loss: 4.453125  val f1:0.14286  lr: 0.0003 batch: 8
epochs: 4 train loss: 0.396240234375  train f1: 1.00000  lr: 0.0003 batch: 8 
epochs: 4 val loss: 4.2265625  val f1:0.14286  lr: 0.0003 batch: 8
epochs: 5 train loss: 0.0960693359375  train f1: 1.00000  lr: 0.0003 batch: 8 
epochs: 5 val loss: 4.0078125  val f1:0.14286  lr: 0.0003 batch: 8
epochs: 6 train loss: 0.033966064453125  train f1: 1.00000  lr: 0.0003 batch: 8 
epochs: 6 val loss: 3.830078125  val f1:0.12500  lr: 0.0003 batch: 8
epochs: 7 train loss: 0.0136566162109375  train f1: 1.00000  lr: 0.0003 batch: 8 
epochs: 7 val loss: 3.7265625  val f1:0.11111  lr: 0.0003 batch: 8
epochs: 8 train loss: 0.00677490234375  train f1: 1.00000  lr: 0.0003 batch: 8 
epochs: 8 val loss: 3.66796875  val f1:0.12500  lr: 0.0003 batch: 8
epochs: 9 train loss: 0.003452301025390625  train f1: 1.00000  lr: 0.0003 batch: 8 
epochs: 9 val loss: 3.62890625  val f1:0.12500  lr: 0.0003 batch: 8
epochs: 10 train loss: 0.0025768280029296875  train f1: 1.00000  lr: 0.0003 batch: 8 
epochs: 10 val loss: 3.623046875  val f1:0.12500  lr: 0.0003 batch: 8
epochs: 11 train loss: 0.002773284912109375  train f1: 1.00000  lr: 0.0003 batch: 8 
epochs: 11 val loss: 3.625  val f1:0.12500  lr: 0.0003 batch: 8
epochs: 12 train loss: 0.0015630722045898438  train f1: 1.00000  lr: 0.0003 batch: 8 
epochs: 12 val loss: 3.640625  val f1:0.12500  lr: 0.0003 batch: 8
epochs: 13 train loss: 0.0006480216979980469  train f1: 1.00000  lr: 0.0003 batch: 8 
epochs: 13 val loss: 3.658203125  val f1:0.12500  lr: 0.0003 batch: 8
epochs: 14 train loss: 0.0009508132934570312  train f1: 1.00000  lr: 0.0003 batch: 8 
epochs: 14 val loss: 3.6875  val f1:0.12500  lr: 0.0003 batch: 8
epochs: 15 train loss: 0.0006375312805175781  train f1: 1.00000  lr: 0.0003 batch: 8 
epochs: 15 val loss: 3.740234375  val f1:0.12500  lr: 0.0003 batch: 8
epochs: 16 train loss: 0.0005788803100585938  train f1: 1.00000  lr: 0.0003 batch: 8 
epochs: 16 val loss: 3.79296875  val f1:0.12500  lr: 0.0003 batch: 8
epochs: 17 train loss: 0.00025916099548339844  train f1: 1.00000  lr: 0.0003 batch: 8 
epochs: 17 val loss: 3.79296875  val f1:0.14286  lr: 0.0003 batch: 8
epochs: 18 train loss: 0.0005283355712890625  train f1: 1.00000  lr: 0.0003 batch: 8 
epochs: 18 val loss: 3.826171875  val f1:0.14286  lr: 0.0003 batch: 8
epochs: 19 train loss: 0.00022470951080322266  train f1: 1.00000  lr: 0.0003 batch: 8 
epochs: 19 val loss: 3.814453125  val f1:0.14286  lr: 0.0003 batch: 8
epochs: 0 train loss: 4.21875  train f1: 0.00000  lr: 0.0003 batch: 8 
epochs: 0 val loss: 4.14453125  val f1:0.00000  lr: 0.0003 batch: 8
epochs: 1 train loss: 4.26171875  train f1: 0.00000  lr: 0.0003 batch: 8 
epochs: 1 val loss: 4.0859375  val f1:0.12500  lr: 0.0003 batch: 8
epochs: 0 train loss: 4.4375  train f1: 0.00000  lr: 0.0003 batch: 8 
epochs: 0 val loss: 4.27734375  val f1:0.00000  lr: 0.0003 batch: 8
epochs: 1 train loss: 4.4140625  train f1: 0.00000  lr: 0.0003 batch: 8 
epochs: 1 val loss: 4.17578125  val f1:0.14286  lr: 0.0003 batch: 8
epochs: 2 train loss: 2.728515625  train f1: 1.00000  lr: 0.0003 batch: 8 
epochs: 2 val loss: 4.0859375  val f1:0.14286  lr: 0.0003 batch: 8
epochs: 0 train loss: 0.6428531637215549  train f1: 0.38117  lr: 0.0003 batch: 32 
epochs: 0 val loss: 1.2435877182904411  val f1:0.49808  lr: 0.0003 batch: 32
epochs: 1 train loss: 0.21258543019282855  train f1: 0.72917  lr: 0.0003 batch: 32 
epochs: 1 val loss: 1.0797334558823528  val f1:0.64567  lr: 0.0003 batch: 32
epochs: 2 train loss: 0.10558480812129822  train f1: 0.86899  lr: 0.0003 batch: 32 
epochs: 2 val loss: 0.8736464556525735  val f1:0.70113  lr: 0.0003 batch: 32
epochs: 3 train loss: 0.07541153258516306  train f1: 0.91672  lr: 0.0003 batch: 32 
epochs: 3 val loss: 0.7341515036190258  val f1:0.77681  lr: 0.0003 batch: 32
epochs: 4 train loss: 0.05453536873149152  train f1: 0.93847  lr: 0.0003 batch: 32 
epochs: 4 val loss: 0.8500886804917278  val f1:0.72606  lr: 0.0003 batch: 32
epochs: 5 train loss: 0.03834504795787935  train f1: 0.96146  lr: 0.0003 batch: 32 
epochs: 5 val loss: 0.9591010598575367  val f1:0.73155  lr: 0.0003 batch: 32
epochs: 6 train loss: 0.04071352666155658  train f1: 0.95881  lr: 0.0003 batch: 32 
epochs: 6 val loss: 0.951132381663603  val f1:0.75568  lr: 0.0003 batch: 32
epochs: 7 train loss: 0.03417167996527849  train f1: 0.96750  lr: 0.0003 batch: 32 
epochs: 7 val loss: 0.888759837431066  val f1:0.76008  lr: 0.0003 batch: 32
epochs: 8 train loss: 0.04100387441249859  train f1: 0.95498  lr: 0.0003 batch: 32 
epochs: 8 val loss: 0.8293743133544923  val f1:0.76977  lr: 0.0003 batch: 32
epochs: 9 train loss: 0.032581972659674614  train f1: 0.96580  lr: 0.0003 batch: 32 
epochs: 9 val loss: 1.0757778392118567  val f1:0.73278  lr: 0.0003 batch: 32
epochs: 10 train loss: 0.026638118406186385  train f1: 0.97514  lr: 0.0003 batch: 32 
epochs: 10 val loss: 1.183075848747702  val f1:0.74315  lr: 0.0003 batch: 32
epochs: 11 train loss: 0.037618420665103625  train f1: 0.95383  lr: 0.0003 batch: 32 
epochs: 11 val loss: 1.1264504825367647  val f1:0.74958  lr: 0.0003 batch: 32
epochs: 12 train loss: 0.02840205380446892  train f1: 0.97494  lr: 0.0003 batch: 32 
epochs: 12 val loss: 0.9331772748161767  val f1:0.76747  lr: 0.0003 batch: 32
epochs: 13 train loss: 0.021524137689585693  train f1: 0.98072  lr: 0.0003 batch: 32 
epochs: 13 val loss: 1.182977115406709  val f1:0.75489  lr: 0.0003 batch: 32
epochs: 14 train loss: 0.028487790552457957  train f1: 0.96937  lr: 0.0003 batch: 32 
epochs: 14 val loss: 0.9722370820886949  val f1:0.76235  lr: 0.0003 batch: 32
epochs: 15 train loss: 0.0248073684307108  train f1: 0.98149  lr: 0.0003 batch: 32 
epochs: 15 val loss: 1.0352316463694855  val f1:0.75536  lr: 0.0003 batch: 32
epochs: 16 train loss: 0.022988600623875186  train f1: 0.97836  lr: 0.0003 batch: 32 
epochs: 16 val loss: 0.9422670252182904  val f1:0.76633  lr: 0.0003 batch: 32
epochs: 17 train loss: 0.022990167735520765  train f1: 0.97404  lr: 0.0003 batch: 32 
epochs: 17 val loss: 1.251319436465992  val f1:0.73403  lr: 0.0003 batch: 32
epochs: 18 train loss: 0.023764278526020786  train f1: 0.97765  lr: 0.0003 batch: 32 
epochs: 18 val loss: 1.0752222397748168  val f1:0.76200  lr: 0.0003 batch: 32
epochs: 19 train loss: 0.018568042359149967  train f1: 0.98636  lr: 0.0003 batch: 32 
epochs: 19 val loss: 1.0454783720128678  val f1:0.76731  lr: 0.0003 batch: 32
epochs: 0 train loss: 4.34375  train f1: 0.00000  lr: 0.0003 batch: 32 
epochs: 0 val loss: 4.796875  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 0 train loss: 4.34375  train f1: 0.00000  lr: 0.0003 batch: 32 
epochs: 0 train loss: 4.34375  train f1: 0.00000  lr: 0.0003 batch: 32 
epochs: 0 val loss: 4.796875  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 1 train loss: 4.296875  train f1: 0.00000  lr: 0.0003 batch: 32 
epochs: 1 val loss: 4.75390625  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 2 train loss: 2.548828125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 2 val loss: 4.66015625  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 3 train loss: 0.87255859375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 3 val loss: 4.453125  val f1:0.14286  lr: 0.0003 batch: 32
epochs: 4 train loss: 0.396240234375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 4 val loss: 4.2265625  val f1:0.14286  lr: 0.0003 batch: 32
epochs: 5 train loss: 0.0960693359375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 5 val loss: 4.0078125  val f1:0.14286  lr: 0.0003 batch: 32
epochs: 6 train loss: 0.033966064453125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 6 val loss: 3.830078125  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 7 train loss: 0.0136566162109375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 7 val loss: 3.7265625  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 8 train loss: 0.00677490234375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 8 val loss: 3.66796875  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 9 train loss: 0.003452301025390625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 9 val loss: 3.62890625  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 10 train loss: 0.0025768280029296875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 10 val loss: 3.623046875  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 11 train loss: 0.002773284912109375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 11 val loss: 3.625  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 12 train loss: 0.0015630722045898438  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 12 val loss: 3.640625  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 0 train loss: 4.34375  train f1: 0.00000  lr: 0.0003 batch: 32 
epochs: 0 train loss: 4.34375  train f1: 0.00000  lr: 0.0003 batch: 32 
epochs: 0 val loss: 4.796875  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 1 train loss: 4.296875  train f1: 0.00000  lr: 0.0003 batch: 32 
epochs: 1 val loss: 4.75390625  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 2 train loss: 2.548828125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 2 val loss: 4.66015625  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 0 train loss: 4.34375  train f1: 0.00000  lr: 0.0003 batch: 32 
epochs: 0 val loss: 4.796875  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 1 train loss: 4.296875  train f1: 0.00000  lr: 0.0003 batch: 32 
epochs: 1 val loss: 4.75390625  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 2 train loss: 2.548828125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 2 val loss: 4.66015625  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 3 train loss: 0.87255859375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 3 val loss: 4.453125  val f1:0.14286  lr: 0.0003 batch: 32
epochs: 4 train loss: 0.396240234375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 4 val loss: 4.2265625  val f1:0.14286  lr: 0.0003 batch: 32
epochs: 5 train loss: 0.0960693359375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 5 val loss: 4.0078125  val f1:0.14286  lr: 0.0003 batch: 32
epochs: 6 train loss: 0.033966064453125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 6 val loss: 3.830078125  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 7 train loss: 0.0136566162109375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 7 val loss: 3.7265625  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 8 train loss: 0.00677490234375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 8 val loss: 3.66796875  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 9 train loss: 0.003452301025390625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 9 val loss: 3.62890625  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 10 train loss: 0.0025768280029296875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 10 val loss: 3.623046875  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 11 train loss: 0.002773284912109375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 11 val loss: 3.625  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 12 train loss: 0.0015630722045898438  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 12 val loss: 3.640625  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 13 train loss: 0.0006480216979980469  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 13 val loss: 3.658203125  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 14 train loss: 0.0009508132934570312  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 14 val loss: 3.6875  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 15 train loss: 0.0006375312805175781  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 15 val loss: 3.740234375  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 16 train loss: 0.0005788803100585938  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 16 val loss: 3.79296875  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 17 train loss: 0.00025916099548339844  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 17 val loss: 3.79296875  val f1:0.14286  lr: 0.0003 batch: 32
epochs: 18 train loss: 0.0005283355712890625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 18 val loss: 3.826171875  val f1:0.14286  lr: 0.0003 batch: 32
epochs: 19 train loss: 0.00022470951080322266  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 19 val loss: 3.814453125  val f1:0.14286  lr: 0.0003 batch: 32
epochs: 0 train loss: 6.54296875  train f1: 0.00000  lr: 0.0003 batch: 32 
epochs: 0 val loss: 6.6328125  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 1 train loss: 6.53515625  train f1: 0.00000  lr: 0.0003 batch: 32 
epochs: 1 val loss: 6.5546875  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 2 train loss: 6.09375  train f1: 0.16667  lr: 0.0003 batch: 32 
epochs: 2 val loss: 6.484375  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 3 train loss: 5.40625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 3 val loss: 6.375  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 4 train loss: 5.11328125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 4 val loss: 6.28125  val f1:0.14286  lr: 0.0003 batch: 32
epochs: 5 train loss: 4.453125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 5 val loss: 6.16015625  val f1:0.14286  lr: 0.0003 batch: 32
epochs: 6 train loss: 4.1015625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 6 val loss: 6.03125  val f1:0.14286  lr: 0.0003 batch: 32
epochs: 7 train loss: 3.45703125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 7 val loss: 5.8984375  val f1:0.14286  lr: 0.0003 batch: 32
epochs: 8 train loss: 3.083984375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 8 val loss: 5.76953125  val f1:0.14286  lr: 0.0003 batch: 32
epochs: 9 train loss: 2.482421875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 9 val loss: 5.61328125  val f1:0.14286  lr: 0.0003 batch: 32
epochs: 10 train loss: 2.12890625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 10 val loss: 5.46875  val f1:0.14286  lr: 0.0003 batch: 32
epochs: 11 train loss: 1.6181640625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 11 val loss: 5.328125  val f1:0.14286  lr: 0.0003 batch: 32
epochs: 12 train loss: 1.1689453125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 12 val loss: 5.19921875  val f1:0.14286  lr: 0.0003 batch: 32
epochs: 13 train loss: 0.77978515625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 13 val loss: 5.12890625  val f1:0.14286  lr: 0.0003 batch: 32
epochs: 14 train loss: 0.634765625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 14 val loss: 5.12109375  val f1:0.14286  lr: 0.0003 batch: 32
epochs: 15 train loss: 0.347900390625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 15 val loss: 5.12109375  val f1:0.14286  lr: 0.0003 batch: 32
epochs: 16 train loss: 0.2076416015625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 16 val loss: 5.12890625  val f1:0.14286  lr: 0.0003 batch: 32
epochs: 17 train loss: 0.1177978515625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 17 val loss: 5.14453125  val f1:0.14286  lr: 0.0003 batch: 32
epochs: 18 train loss: 0.0855712890625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 18 val loss: 5.16015625  val f1:0.14286  lr: 0.0003 batch: 32
epochs: 19 train loss: 0.051177978515625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 19 val loss: 5.171875  val f1:0.14286  lr: 0.0003 batch: 32
epochs: 0 train loss: 0.863122041088684  train f1: 0.14367  lr: 0.0003 batch: 32 
epochs: 0 val loss: 1.3545280905330885  val f1:0.35611  lr: 0.0003 batch: 32
epochs: 1 train loss: 0.27319716455930476  train f1: 0.60471  lr: 0.0003 batch: 32 
epochs: 1 val loss: 0.9206148035386027  val f1:0.57566  lr: 0.0003 batch: 32
epochs: 2 train loss: 0.13969243494352496  train f1: 0.78726  lr: 0.0003 batch: 32 
epochs: 2 val loss: 0.6897223977481617  val f1:0.71981  lr: 0.0003 batch: 32
epochs: 3 train loss: 0.07946920751633489  train f1: 0.88770  lr: 0.0003 batch: 32 
epochs: 3 val loss: 0.6220653758329504  val f1:0.75452  lr: 0.0003 batch: 32
epochs: 4 train loss: 0.06449116851920793  train f1: 0.91683  lr: 0.0003 batch: 32 
epochs: 4 val loss: 0.6309240004595588  val f1:0.77314  lr: 0.0003 batch: 32
epochs: 5 train loss: 0.0485956092130514  train f1: 0.94922  lr: 0.0003 batch: 32 
epochs: 5 val loss: 0.6635167738970588  val f1:0.78516  lr: 0.0003 batch: 32
epochs: 6 train loss: 0.04295882560368492  train f1: 0.95074  lr: 0.0003 batch: 32 
epochs: 6 val loss: 0.6970766852883731  val f1:0.79482  lr: 0.0003 batch: 32
epochs: 7 train loss: 0.032663258531147134  train f1: 0.96163  lr: 0.0003 batch: 32 
epochs: 7 val loss: 0.9531106387867648  val f1:0.78232  lr: 0.0003 batch: 32
epochs: 8 train loss: 0.035121780975798406  train f1: 0.96219  lr: 0.0003 batch: 32 
epochs: 8 val loss: 0.8429179191589359  val f1:0.78505  lr: 0.0003 batch: 32
epochs: 9 train loss: 0.02918722772241527  train f1: 0.96370  lr: 0.0003 batch: 32 
epochs: 9 val loss: 0.8277264763327205  val f1:0.76281  lr: 0.0003 batch: 32
epochs: 10 train loss: 0.024915855424362504  train f1: 0.97352  lr: 0.0003 batch: 32 
epochs: 10 val loss: 0.7902356316061581  val f1:0.80453  lr: 0.0003 batch: 32
epochs: 11 train loss: 0.02678782832890077  train f1: 0.97337  lr: 0.0003 batch: 32 
epochs: 11 val loss: 0.748381670783548  val f1:0.80702  lr: 0.0003 batch: 32
epochs: 12 train loss: 0.027703344970569928  train f1: 0.96794  lr: 0.0003 batch: 32 
epochs: 12 val loss: 0.8788057776058421  val f1:0.79199  lr: 0.0003 batch: 32
epochs: 13 train loss: 0.02320655370293707  train f1: 0.97888  lr: 0.0003 batch: 32 
epochs: 13 val loss: 0.825386874815997  val f1:0.79817  lr: 0.0003 batch: 32
epochs: 14 train loss: 0.03482268694927568  train f1: 0.95982  lr: 0.0003 batch: 32 
epochs: 14 val loss: 0.7895198148839615  val f1:0.78814  lr: 0.0003 batch: 32
epochs: 0 train loss: 4.45703125  train f1: 0.00000  lr: 0.0003 batch: 32 
epochs: 0 val loss: 4.7890625  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 0 train loss: 4.45703125  train f1: 0.00000  lr: 0.0003 batch: 32 
epochs: 0 val loss: 4.7890625  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 1 train loss: 4.296875  train f1: 0.00000  lr: 0.0003 batch: 32 
epochs: 1 val loss: 4.75  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 2 train loss: 3.32421875  train f1: 0.33333  lr: 0.0003 batch: 32 
epochs: 2 val loss: 4.64453125  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 3 train loss: 1.447265625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 3 val loss: 4.51953125  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 4 train loss: 0.7373046875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 4 val loss: 4.3203125  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 5 train loss: 0.1728515625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 5 val loss: 4.14453125  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 6 train loss: 0.1036376953125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 6 val loss: 4.0234375  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 7 train loss: 0.028350830078125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 7 val loss: 3.908203125  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 8 train loss: 0.0158843994140625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 8 val loss: 3.83203125  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 9 train loss: 0.00582122802734375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 9 val loss: 3.767578125  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 10 train loss: 0.005084991455078125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 10 val loss: 3.75  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 11 train loss: 0.0087127685546875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 11 val loss: 3.73828125  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 12 train loss: 0.002567291259765625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 12 val loss: 3.7421875  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 13 train loss: 0.00110626220703125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 13 val loss: 3.7109375  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 14 train loss: 0.005496978759765625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 14 val loss: 3.69921875  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 15 train loss: 0.0031757354736328125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 15 val loss: 3.751953125  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 16 train loss: 0.0015878677368164062  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 16 val loss: 3.767578125  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 17 train loss: 0.0008707046508789062  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 17 val loss: 3.767578125  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 18 train loss: 0.00041222572326660156  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 18 val loss: 3.798828125  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 0 train loss: 4.390625  train f1: 0.00000  lr: 0.0003 batch: 32 
epochs: 0 val loss: 4.82421875  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 1 train loss: 4.2890625  train f1: 0.00000  lr: 0.0003 batch: 32 
epochs: 1 val loss: 4.78125  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 2 train loss: 3.13671875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 2 val loss: 4.63671875  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 3 train loss: 1.796875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 3 val loss: 4.546875  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 4 train loss: 0.564453125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 4 val loss: 4.38671875  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 5 train loss: 0.62646484375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 5 val loss: 4.24609375  val f1:0.14286  lr: 0.0003 batch: 32
epochs: 6 train loss: 0.1708984375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 6 val loss: 4.05078125  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 0 train loss: 4.4609375  train f1: 0.00000  lr: 0.0003 batch: 32 
epochs: 0 val loss: 4.7734375  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 1 train loss: 4.2890625  train f1: 0.00000  lr: 0.0003 batch: 32 
epochs: 1 val loss: 4.734375  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 2 train loss: 3.24609375  train f1: 0.66667  lr: 0.0003 batch: 32 
epochs: 2 val loss: 4.6484375  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 3 train loss: 2.220703125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 3 val loss: 4.5390625  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 4 train loss: 0.66845703125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 4 val loss: 4.40234375  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 0 train loss: 4.5703125  train f1: 0.00000  lr: 0.0003 batch: 32 
epochs: 0 val loss: 4.76953125  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 1 train loss: 4.2890625  train f1: 0.00000  lr: 0.0003 batch: 32 
epochs: 1 val loss: 4.72265625  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 0 train loss: 4.48828125  train f1: 0.00000  lr: 0.0003 batch: 32 
epochs: 0 val loss: 4.82421875  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 1 train loss: 4.2890625  train f1: 0.00000  lr: 0.0003 batch: 32 
epochs: 1 val loss: 4.7734375  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 2 train loss: 3.62109375  train f1: 0.11111  lr: 0.0003 batch: 32 
epochs: 2 val loss: 4.609375  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 3 train loss: 1.8740234375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 3 val loss: 4.44140625  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 4 train loss: 0.58642578125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 4 val loss: 4.28125  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 5 train loss: 0.259765625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 5 val loss: 4.1328125  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 6 train loss: 0.13232421875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 6 val loss: 4.01171875  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 7 train loss: 0.059051513671875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 7 val loss: 3.892578125  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 8 train loss: 0.06939697265625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 8 val loss: 3.8359375  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 0 train loss: 4.625  train f1: 0.00000  lr: 0.0003 batch: 32 
epochs: 0 val loss: 4.83984375  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 1 train loss: 4.2890625  train f1: 0.00000  lr: 0.0003 batch: 32 
epochs: 1 val loss: 4.78515625  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 2 train loss: 3.521484375  train f1: 0.33333  lr: 0.0003 batch: 32 
epochs: 2 val loss: 4.6640625  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 3 train loss: 1.705078125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 3 val loss: 4.5078125  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 4 train loss: 0.7275390625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 4 val loss: 4.390625  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 5 train loss: 0.274658203125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 5 val loss: 4.21875  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 6 train loss: 0.1158447265625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 6 val loss: 4.0390625  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 0 train loss: 4.265625  train f1: 0.00000  lr: 0.0003 batch: 32 
epochs: 0 val loss: 4.78515625  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 1 train loss: 4.2890625  train f1: 0.00000  lr: 0.0003 batch: 32 
epochs: 1 val loss: 4.73046875  val f1:0.00000  lr: 0.0003 batch: 32
epochs: 2 train loss: 3.552734375  train f1: 0.33333  lr: 0.0003 batch: 32 
epochs: 2 val loss: 4.6171875  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 3 train loss: 1.705078125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 3 val loss: 4.53515625  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 4 train loss: 0.71142578125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 4 val loss: 4.4140625  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 5 train loss: 0.45458984375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 5 val loss: 4.26171875  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 6 train loss: 0.1181640625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 6 val loss: 4.1484375  val f1:0.12500  lr: 0.0003 batch: 32
epochs: 7 train loss: 0.0621337890625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 7 val loss: 4.05859375  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 8 train loss: 0.0594482421875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 8 val loss: 3.966796875  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 9 train loss: 0.0267486572265625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 9 val loss: 3.8671875  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 10 train loss: 0.034210205078125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 10 val loss: 3.865234375  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 11 train loss: 0.005046844482421875  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 11 val loss: 3.8515625  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 12 train loss: 0.006092071533203125  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 12 val loss: 3.849609375  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 13 train loss: 0.002285003662109375  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 13 val loss: 3.8671875  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 14 train loss: 0.001468658447265625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 14 val loss: 3.884765625  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 15 train loss: 0.0017499923706054688  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 15 val loss: 3.8828125  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 16 train loss: 0.002323150634765625  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 16 val loss: 3.91015625  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 17 train loss: 0.0006375312805175781  train f1: 1.00000  lr: 0.0003 batch: 32 
epochs: 17 val loss: 3.904296875  val f1:0.11111  lr: 0.0003 batch: 32
epochs: 0 train loss: 1.0251209771455232  train f1: 0.21363  lr: 0.0003 batch: 32 
epochs: 0 val loss: 1.814438763786764  val f1:0.27725  lr: 0.0003 batch: 32
epochs: 1 train loss: 0.48951743965718286  train f1: 0.40177  lr: 0.0003 batch: 32 
epochs: 1 val loss: 1.2148581112132353  val f1:0.41737  lr: 0.0003 batch: 32
epochs: 2 train loss: 0.32950011296058757  train f1: 0.56463  lr: 0.0003 batch: 32 
epochs: 2 val loss: 1.0078627642463232  val f1:0.52343  lr: 0.0003 batch: 32
epochs: 3 train loss: 0.22381011051918154  train f1: 0.70704  lr: 0.0003 batch: 32 
epochs: 3 val loss: 0.9112010282628679  val f1:0.60782  lr: 0.0003 batch: 32
epochs: 4 train loss: 0.18879659851985187  train f1: 0.74040  lr: 0.0003 batch: 32 
epochs: 4 val loss: 0.7335653866038605  val f1:0.66934  lr: 0.0003 batch: 32
epochs: 5 train loss: 0.13913197303885846  train f1: 0.80392  lr: 0.0003 batch: 32 
epochs: 5 val loss: 0.7231499167049631  val f1:0.71544  lr: 0.0003 batch: 32
epochs: 6 train loss: 0.11509741598100805  train f1: 0.85512  lr: 0.0003 batch: 32 
epochs: 6 val loss: 0.724199182846967  val f1:0.73428  lr: 0.0003 batch: 32
epochs: 7 train loss: 0.10163265911500845  train f1: 0.87206  lr: 0.0003 batch: 32 
epochs: 7 val loss: 0.8020593979779411  val f1:0.70235  lr: 0.0003 batch: 32
epochs: 8 train loss: 0.06754379841818738  train f1: 0.91190  lr: 0.0003 batch: 32 
epochs: 8 val loss: 0.653347239774816  val f1:0.74554  lr: 0.0003 batch: 32
epochs: 9 train loss: 0.0802915487716447  train f1: 0.90673  lr: 0.0003 batch: 32 
epochs: 9 val loss: 0.7022830738740807  val f1:0.75300  lr: 0.0003 batch: 32
epochs: 10 train loss: 0.061203928136113854  train f1: 0.93506  lr: 0.0003 batch: 32 
epochs: 10 val loss: 0.744994219611673  val f1:0.76153  lr: 0.0003 batch: 32
epochs: 11 train loss: 0.06456624216108182  train f1: 0.91668  lr: 0.0003 batch: 32 
epochs: 11 val loss: 0.7186961454503679  val f1:0.74243  lr: 0.0003 batch: 32
epochs: 12 train loss: 0.05928503933237557  train f1: 0.91764  lr: 0.0003 batch: 32 
epochs: 12 val loss: 0.7152081657858456  val f1:0.75031  lr: 0.0003 batch: 32
epochs: 13 train loss: 0.05910121860788831  train f1: 0.94105  lr: 0.0003 batch: 32 
epochs: 13 val loss: 0.7357518813189341  val f1:0.74873  lr: 0.0003 batch: 32
epochs: 14 train loss: 0.06089621871265012  train f1: 0.92502  lr: 0.0003 batch: 32 
epochs: 14 val loss: 0.9172291475183821  val f1:0.72469  lr: 0.0003 batch: 32
epochs: 15 train loss: 0.049947542930716925  train f1: 0.94902  lr: 0.0003 batch: 32 
epochs: 15 val loss: 0.683048023897059  val f1:0.80189  lr: 0.0003 batch: 32
epochs: 16 train loss: 0.04465455439553333  train f1: 0.95127  lr: 0.0003 batch: 32 
epochs: 16 val loss: 0.7660648121553311  val f1:0.78860  lr: 0.0003 batch: 32
epochs: 17 train loss: 0.05244905913054054  train f1: 0.95022  lr: 0.0003 batch: 32 
epochs: 17 val loss: 0.6858179428998161  val f1:0.78835  lr: 0.0003 batch: 32
epochs: 18 train loss: 0.04472790191422649  train f1: 0.94863  lr: 0.0003 batch: 32 
epochs: 18 val loss: 0.7013406192555146  val f1:0.77584  lr: 0.0003 batch: 32
epochs: 19 train loss: 0.03983047115268993  train f1: 0.95309  lr: 0.0003 batch: 32 
epochs: 19 val loss: 0.9324771656709561  val f1:0.75063  lr: 0.0003 batch: 32
epochs: 0 train loss: 4.34375  train f1: 0.00000  lr: 0.035719286524462164 batch: 10 
epochs: 0 val loss: 4.796875  val f1:0.00000  lr: 0.035719286524462164 batch: 10
epochs: 1 train loss: 4.296875  train f1: 0.00000  lr: 0.035719286524462164 batch: 10 
epochs: 1 val loss: nan  val f1:0.00000  lr: 0.035719286524462164 batch: 10
epochs: 2 train loss: 96.625  train f1: 0.00000  lr: 0.035719286524462164 batch: 10 
epochs: 2 val loss: nan  val f1:0.00000  lr: 0.035719286524462164 batch: 10
epochs: 3 train loss: 61.34375  train f1: 0.12500  lr: 0.035719286524462164 batch: 10 
epochs: 3 val loss: nan  val f1:0.00000  lr: 0.035719286524462164 batch: 10
epochs: 4 train loss: 91.25  train f1: 0.00000  lr: 0.035719286524462164 batch: 10 
epochs: 4 val loss: nan  val f1:0.00000  lr: 0.035719286524462164 batch: 10
epochs: 5 train loss: 23.6875  train f1: 0.12500  lr: 0.035719286524462164 batch: 10 
epochs: 5 val loss: nan  val f1:0.00000  lr: 0.035719286524462164 batch: 10
epochs: 6 train loss: 60.90625  train f1: 0.20000  lr: 0.035719286524462164 batch: 10 
epochs: 6 val loss: nan  val f1:0.00000  lr: 0.035719286524462164 batch: 10
epochs: 7 train loss: 24.625  train f1: 0.12500  lr: 0.035719286524462164 batch: 10 
epochs: 7 val loss: nan  val f1:0.00000  lr: 0.035719286524462164 batch: 10
epochs: 8 train loss: 97.4375  train f1: 0.00000  lr: 0.035719286524462164 batch: 10 
epochs: 8 val loss: nan  val f1:0.00000  lr: 0.035719286524462164 batch: 10
epochs: 9 train loss: 23.625  train f1: 0.12500  lr: 0.035719286524462164 batch: 10 
epochs: 9 val loss: nan  val f1:0.00000  lr: 0.035719286524462164 batch: 10
epochs: 0 train loss: 5.01171875  train f1: 0.00000  lr: 0.0006532588947222302 batch: 18 
epochs: 0 val loss: 4.79296875  val f1:0.00000  lr: 0.0006532588947222302 batch: 18
epochs: 1 train loss: 4.96875  train f1: 0.00000  lr: 0.0006532588947222302 batch: 18 
epochs: 1 val loss: 4.7578125  val f1:0.00000  lr: 0.0006532588947222302 batch: 18
epochs: 0 train loss: 4.34375  train f1: 0.00000  lr: 2.2617499414497587e-05 batch: 6 
epochs: 0 val loss: 4.796875  val f1:0.00000  lr: 2.2617499414497587e-05 batch: 6
epochs: 0 train loss: 4.34375  train f1: 0.00000  lr: 0.008163818767877853 batch: 20 
epochs: 0 val loss: 4.796875  val f1:0.00000  lr: 0.008163818767877853 batch: 20
epochs: 0 train loss: 4.34375  train f1: 0.00000  lr: 0.05387952855485316 batch: 9 
epochs: 0 val loss: 4.796875  val f1:0.00000  lr: 0.05387952855485316 batch: 9
epochs: 1 train loss: 4.296875  train f1: 0.00000  lr: 0.05387952855485316 batch: 9 
epochs: 1 val loss: nan  val f1:0.00000  lr: 0.05387952855485316 batch: 9
epochs: 2 train loss: 97.5  train f1: 0.22222  lr: 0.05387952855485316 batch: 9 
epochs: 2 val loss: nan  val f1:0.00000  lr: 0.05387952855485316 batch: 9
epochs: 0 train loss: 4.34375  train f1: 0.00000  lr: 0.0016197610162452419 batch: 13 
epochs: 0 val loss: 4.796875  val f1:0.00000  lr: 0.0016197610162452419 batch: 13
epochs: 1 train loss: 4.296875  train f1: 0.00000  lr: 0.0016197610162452419 batch: 13 
epochs: 1 val loss: 4.8203125  val f1:0.00000  lr: 0.0016197610162452419 batch: 13
epochs: 2 train loss: 4.05078125  train f1: 0.00000  lr: 0.0016197610162452419 batch: 13 
epochs: 2 val loss: 4.8046875  val f1:0.00000  lr: 0.0016197610162452419 batch: 13
epochs: 3 train loss: 3.98046875  train f1: 0.00000  lr: 0.0016197610162452419 batch: 13 
epochs: 3 val loss: 4.77734375  val f1:0.00000  lr: 0.0016197610162452419 batch: 13
epochs: 4 train loss: 3.958984375  train f1: 0.00000  lr: 0.0016197610162452419 batch: 13 
epochs: 4 val loss: 4.7265625  val f1:0.00000  lr: 0.0016197610162452419 batch: 13
epochs: 5 train loss: 3.80078125  train f1: 0.00000  lr: 0.0016197610162452419 batch: 13 
epochs: 5 val loss: 4.6875  val f1:0.00000  lr: 0.0016197610162452419 batch: 13
epochs: 6 train loss: 3.7734375  train f1: 0.00000  lr: 0.0016197610162452419 batch: 13 
epochs: 6 val loss: 4.625  val f1:0.00000  lr: 0.0016197610162452419 batch: 13
epochs: 7 train loss: 3.55859375  train f1: 0.00000  lr: 0.0016197610162452419 batch: 13 
epochs: 7 val loss: 4.5703125  val f1:0.00000  lr: 0.0016197610162452419 batch: 13
epochs: 0 train loss: 4.34375  train f1: 0.00000  lr: 0.00018662459409642872 batch: 17 
epochs: 0 val loss: 4.796875  val f1:0.00000  lr: 0.00018662459409642872 batch: 17
epochs: 1 train loss: 4.296875  train f1: 0.00000  lr: 0.00018662459409642872 batch: 17 
epochs: 1 val loss: 4.21875  val f1:0.12500  lr: 0.00018662459409642872 batch: 17
epochs: 2 train loss: 0.258056640625  train f1: 1.00000  lr: 0.00018662459409642872 batch: 17 
epochs: 2 val loss: 3.9609375  val f1:0.12500  lr: 0.00018662459409642872 batch: 17
epochs: 3 train loss: 0.0543212890625  train f1: 1.00000  lr: 0.00018662459409642872 batch: 17 
epochs: 3 val loss: 3.927734375  val f1:0.12500  lr: 0.00018662459409642872 batch: 17
epochs: 4 train loss: 0.0023326873779296875  train f1: 1.00000  lr: 0.00018662459409642872 batch: 17 
epochs: 4 val loss: 3.984375  val f1:0.11111  lr: 0.00018662459409642872 batch: 17
epochs: 0 train loss: 4.34375  train f1: 0.00000  lr: 0.08464477502483783 batch: 11 
epochs: 0 val loss: 4.796875  val f1:0.00000  lr: 0.08464477502483783 batch: 11
epochs: 1 train loss: 4.296875  train f1: 0.00000  lr: 0.08464477502483783 batch: 11 
epochs: 1 val loss: 4.51171875  val f1:0.12500  lr: 0.08464477502483783 batch: 11
epochs: 2 train loss: 1.0107421875  train f1: 1.00000  lr: 0.08464477502483783 batch: 11 
epochs: 2 val loss: 4.47265625  val f1:0.12500  lr: 0.08464477502483783 batch: 11
epochs: 3 train loss: 0.07568359375  train f1: 1.00000  lr: 0.08464477502483783 batch: 11 
epochs: 3 val loss: 4.33984375  val f1:0.14286  lr: 0.08464477502483783 batch: 11
epochs: 4 train loss: 0.0452880859375  train f1: 1.00000  lr: 0.08464477502483783 batch: 11 
epochs: 4 val loss: 4.2109375  val f1:0.14286  lr: 0.08464477502483783 batch: 11
epochs: 5 train loss: 0.025726318359375  train f1: 1.00000  lr: 0.08464477502483783 batch: 11 
epochs: 5 val loss: 4.12109375  val f1:0.12500  lr: 0.08464477502483783 batch: 11
epochs: 6 train loss: 0.0174102783203125  train f1: 1.00000  lr: 0.08464477502483783 batch: 11 
epochs: 6 val loss: 3.96484375  val f1:0.14286  lr: 0.08464477502483783 batch: 11
epochs: 7 train loss: 0.0150299072265625  train f1: 1.00000  lr: 0.08464477502483783 batch: 11 
epochs: 7 val loss: 3.8203125  val f1:0.14286  lr: 0.08464477502483783 batch: 11
epochs: 8 train loss: 0.0141143798828125  train f1: 1.00000  lr: 0.08464477502483783 batch: 11 
epochs: 8 val loss: 3.751953125  val f1:0.12500  lr: 0.08464477502483783 batch: 11
epochs: 9 train loss: 0.01110076904296875  train f1: 1.00000  lr: 0.08464477502483783 batch: 11 
epochs: 9 val loss: 3.689453125  val f1:0.12500  lr: 0.08464477502483783 batch: 11
epochs: 0 train loss: 4.34375  train f1: 0.00000  lr: 0.002678228274316318 batch: 25 
epochs: 0 val loss: 4.796875  val f1:0.00000  lr: 0.002678228274316318 batch: 25
epochs: 1 train loss: 4.296875  train f1: 0.00000  lr: 0.002678228274316318 batch: 25 
epochs: 1 val loss: nan  val f1:0.16667  lr: 0.002678228274316318 batch: 25
epochs: 2 train loss: 4.85546875  train f1: 0.26667  lr: 0.002678228274316318 batch: 25 
epochs: 2 val loss: 10688.0  val f1:0.06667  lr: 0.002678228274316318 batch: 25
epochs: 3 train loss: 4.34765625  train f1: 0.26667  lr: 0.002678228274316318 batch: 25 
epochs: 3 val loss: 717.5  val f1:0.06667  lr: 0.002678228274316318 batch: 25
epochs: 0 train loss: 4.34375  train f1: 0.00000  lr: 2.117182825177276e-05 batch: 23 
epochs: 0 val loss: 4.796875  val f1:0.00000  lr: 2.117182825177276e-05 batch: 23
epochs: 1 train loss: 4.296875  train f1: 0.00000  lr: 2.117182825177276e-05 batch: 23 
epochs: 1 val loss: 4.828125  val f1:0.00000  lr: 2.117182825177276e-05 batch: 23
epochs: 2 train loss: 4.15234375  train f1: 0.00000  lr: 2.117182825177276e-05 batch: 23 
epochs: 2 val loss: 4.82421875  val f1:0.00000  lr: 2.117182825177276e-05 batch: 23
epochs: 3 train loss: 4.21484375  train f1: 0.00000  lr: 2.117182825177276e-05 batch: 23 
epochs: 3 val loss: 4.8203125  val f1:0.00000  lr: 2.117182825177276e-05 batch: 23
epochs: 4 train loss: 4.30078125  train f1: 0.00000  lr: 2.117182825177276e-05 batch: 23 
epochs: 4 val loss: 4.7890625  val f1:0.00000  lr: 2.117182825177276e-05 batch: 23
epochs: 5 train loss: 4.2734375  train f1: 0.00000  lr: 2.117182825177276e-05 batch: 23 
epochs: 5 val loss: 4.77734375  val f1:0.00000  lr: 2.117182825177276e-05 batch: 23
epochs: 6 train loss: 4.375  train f1: 0.00000  lr: 2.117182825177276e-05 batch: 23 
epochs: 6 val loss: 4.73828125  val f1:0.00000  lr: 2.117182825177276e-05 batch: 23
epochs: 7 train loss: 4.3125  train f1: 0.00000  lr: 2.117182825177276e-05 batch: 23 
epochs: 7 val loss: 4.703125  val f1:0.00000  lr: 2.117182825177276e-05 batch: 23
epochs: 8 train loss: 4.25  train f1: 0.00000  lr: 2.117182825177276e-05 batch: 23 
epochs: 8 val loss: 4.6796875  val f1:0.00000  lr: 2.117182825177276e-05 batch: 23
epochs: 9 train loss: 4.26171875  train f1: 0.00000  lr: 2.117182825177276e-05 batch: 23 
epochs: 9 val loss: 4.68359375  val f1:0.00000  lr: 2.117182825177276e-05 batch: 23
epochs: 10 train loss: 4.19921875  train f1: 0.00000  lr: 2.117182825177276e-05 batch: 23 
epochs: 10 val loss: 4.66015625  val f1:0.00000  lr: 2.117182825177276e-05 batch: 23
epochs: 11 train loss: 4.3125  train f1: 0.00000  lr: 2.117182825177276e-05 batch: 23 
epochs: 11 val loss: 4.64453125  val f1:0.00000  lr: 2.117182825177276e-05 batch: 23
epochs: 12 train loss: 4.2265625  train f1: 0.00000  lr: 2.117182825177276e-05 batch: 23 
epochs: 12 val loss: 4.625  val f1:0.00000  lr: 2.117182825177276e-05 batch: 23
epochs: 13 train loss: 4.25  train f1: 0.00000  lr: 2.117182825177276e-05 batch: 23 
epochs: 13 val loss: 4.6015625  val f1:0.00000  lr: 2.117182825177276e-05 batch: 23
epochs: 14 train loss: 4.1953125  train f1: 0.00000  lr: 2.117182825177276e-05 batch: 23 
epochs: 14 val loss: 4.59375  val f1:0.00000  lr: 2.117182825177276e-05 batch: 23
epochs: 15 train loss: 4.328125  train f1: 0.00000  lr: 2.117182825177276e-05 batch: 23 
epochs: 15 val loss: 4.62109375  val f1:0.00000  lr: 2.117182825177276e-05 batch: 23
epochs: 16 train loss: 4.30078125  train f1: 0.00000  lr: 2.117182825177276e-05 batch: 23 
epochs: 16 val loss: 4.6328125  val f1:0.00000  lr: 2.117182825177276e-05 batch: 23
epochs: 17 train loss: 4.2421875  train f1: 0.00000  lr: 2.117182825177276e-05 batch: 23 
epochs: 17 val loss: 4.6171875  val f1:0.00000  lr: 2.117182825177276e-05 batch: 23
epochs: 0 train loss: 4.953125  train f1: 0.00000  lr: 0.006146035321308269 batch: 26 
epochs: 0 val loss: 5.49609375  val f1:0.00000  lr: 0.006146035321308269 batch: 26
epochs: 1 train loss: 5.015625  train f1: 0.00000  lr: 0.006146035321308269 batch: 26 
epochs: 1 val loss: nan  val f1:0.00000  lr: 0.006146035321308269 batch: 26
epochs: 2 train loss: 2.056640625  train f1: 0.13333  lr: 0.006146035321308269 batch: 26 
epochs: 2 val loss: nan  val f1:0.00000  lr: 0.006146035321308269 batch: 26
epochs: 3 train loss: 3.1875  train f1: 0.16667  lr: 0.006146035321308269 batch: 26 
epochs: 3 val loss: nan  val f1:0.00000  lr: 0.006146035321308269 batch: 26
epochs: 4 train loss: 2.865234375  train f1: 0.16667  lr: 0.006146035321308269 batch: 26 
epochs: 4 val loss: nan  val f1:0.00000  lr: 0.006146035321308269 batch: 26
epochs: 5 train loss: 3.46875  train f1: 0.13333  lr: 0.006146035321308269 batch: 26 
epochs: 5 val loss: nan  val f1:0.00000  lr: 0.006146035321308269 batch: 26
epochs: 6 train loss: 38.90625  train f1: 0.25000  lr: 0.006146035321308269 batch: 26 
epochs: 6 val loss: nan  val f1:0.00000  lr: 0.006146035321308269 batch: 26
epochs: 7 train loss: 38.90625  train f1: 0.25000  lr: 0.006146035321308269 batch: 26 
epochs: 7 val loss: nan  val f1:0.00000  lr: 0.006146035321308269 batch: 26
epochs: 8 train loss: 1.3154296875  train f1: 0.55556  lr: 0.006146035321308269 batch: 26 
epochs: 8 val loss: nan  val f1:0.00000  lr: 0.006146035321308269 batch: 26
epochs: 9 train loss: 3.705078125  train f1: 0.55556  lr: 0.006146035321308269 batch: 26 
epochs: 9 val loss: nan  val f1:0.00000  lr: 0.006146035321308269 batch: 26
epochs: 10 train loss: 12.015625  train f1: 0.16667  lr: 0.006146035321308269 batch: 26 
epochs: 10 val loss: nan  val f1:0.00000  lr: 0.006146035321308269 batch: 26
epochs: 11 train loss: 12.0  train f1: 0.00000  lr: 0.006146035321308269 batch: 26 
epochs: 11 val loss: nan  val f1:0.00000  lr: 0.006146035321308269 batch: 26
epochs: 12 train loss: 5.7890625  train f1: 0.25000  lr: 0.006146035321308269 batch: 26 
epochs: 12 val loss: nan  val f1:0.00000  lr: 0.006146035321308269 batch: 26
epochs: 0 train loss: 4.7109375  train f1: 0.00000  lr: 1.94158924843284e-05 batch: 20 
epochs: 0 val loss: 4.89453125  val f1:0.00000  lr: 1.94158924843284e-05 batch: 20
epochs: 1 train loss: 4.75  train f1: 0.00000  lr: 1.94158924843284e-05 batch: 20 
epochs: 1 val loss: 4.96875  val f1:0.00000  lr: 1.94158924843284e-05 batch: 20
epochs: 2 train loss: 4.54296875  train f1: 0.00000  lr: 1.94158924843284e-05 batch: 20 
epochs: 2 val loss: 5.03515625  val f1:0.00000  lr: 1.94158924843284e-05 batch: 20
epochs: 3 train loss: 4.7109375  train f1: 0.00000  lr: 1.94158924843284e-05 batch: 20 
epochs: 3 val loss: 5.08984375  val f1:0.00000  lr: 1.94158924843284e-05 batch: 20
epochs: 4 train loss: 4.64453125  train f1: 0.00000  lr: 1.94158924843284e-05 batch: 20 
epochs: 4 val loss: 5.09765625  val f1:0.00000  lr: 1.94158924843284e-05 batch: 20
epochs: 5 train loss: 4.67578125  train f1: 0.00000  lr: 1.94158924843284e-05 batch: 20 
epochs: 5 val loss: 5.0859375  val f1:0.00000  lr: 1.94158924843284e-05 batch: 20
epochs: 6 train loss: 4.75  train f1: 0.00000  lr: 1.94158924843284e-05 batch: 20 
epochs: 6 val loss: 5.07421875  val f1:0.00000  lr: 1.94158924843284e-05 batch: 20
epochs: 7 train loss: 4.6640625  train f1: 0.00000  lr: 1.94158924843284e-05 batch: 20 
epochs: 7 val loss: 5.00390625  val f1:0.00000  lr: 1.94158924843284e-05 batch: 20
epochs: 8 train loss: 4.7265625  train f1: 0.00000  lr: 1.94158924843284e-05 batch: 20 
epochs: 8 val loss: 4.9453125  val f1:0.00000  lr: 1.94158924843284e-05 batch: 20
epochs: 9 train loss: 4.66796875  train f1: 0.00000  lr: 1.94158924843284e-05 batch: 20 
epochs: 9 val loss: 4.9140625  val f1:0.00000  lr: 1.94158924843284e-05 batch: 20
epochs: 10 train loss: 4.734375  train f1: 0.00000  lr: 1.94158924843284e-05 batch: 20 
epochs: 10 val loss: 4.89453125  val f1:0.00000  lr: 1.94158924843284e-05 batch: 20
epochs: 11 train loss: 4.6640625  train f1: 0.00000  lr: 1.94158924843284e-05 batch: 20 
epochs: 11 val loss: 4.8984375  val f1:0.00000  lr: 1.94158924843284e-05 batch: 20
epochs: 12 train loss: 4.65234375  train f1: 0.00000  lr: 1.94158924843284e-05 batch: 20 
epochs: 12 val loss: 4.8828125  val f1:0.00000  lr: 1.94158924843284e-05 batch: 20
epochs: 13 train loss: 4.51171875  train f1: 0.00000  lr: 1.94158924843284e-05 batch: 20 
epochs: 13 val loss: 4.86328125  val f1:0.00000  lr: 1.94158924843284e-05 batch: 20
epochs: 14 train loss: 4.7578125  train f1: 0.00000  lr: 1.94158924843284e-05 batch: 20 
epochs: 14 val loss: 4.84765625  val f1:0.00000  lr: 1.94158924843284e-05 batch: 20
epochs: 15 train loss: 4.72265625  train f1: 0.00000  lr: 1.94158924843284e-05 batch: 20 
epochs: 15 val loss: 4.8515625  val f1:0.00000  lr: 1.94158924843284e-05 batch: 20
epochs: 16 train loss: 4.74609375  train f1: 0.00000  lr: 1.94158924843284e-05 batch: 20 
epochs: 16 val loss: 4.828125  val f1:0.00000  lr: 1.94158924843284e-05 batch: 20
epochs: 17 train loss: 4.70703125  train f1: 0.00000  lr: 1.94158924843284e-05 batch: 20 
epochs: 17 val loss: 4.7890625  val f1:0.00000  lr: 1.94158924843284e-05 batch: 20
epochs: 18 train loss: 4.7578125  train f1: 0.00000  lr: 1.94158924843284e-05 batch: 20 
epochs: 18 val loss: 4.75390625  val f1:0.00000  lr: 1.94158924843284e-05 batch: 20
epochs: 0 train loss: 4.34375  train f1: 0.00000  lr: 0.06938806928215069 batch: 23 
epochs: 0 val loss: 4.796875  val f1:0.00000  lr: 0.06938806928215069 batch: 23
epochs: 1 train loss: 4.296875  train f1: 0.00000  lr: 0.06938806928215069 batch: 23 
epochs: 1 val loss: nan  val f1:0.00000  lr: 0.06938806928215069 batch: 23
epochs: 2 train loss: 9.53125  train f1: 0.12500  lr: 0.06938806928215069 batch: 23 
epochs: 2 val loss: nan  val f1:0.00000  lr: 0.06938806928215069 batch: 23
epochs: 3 train loss: 5.203125  train f1: 0.25000  lr: 0.06938806928215069 batch: 23 
epochs: 3 val loss: nan  val f1:0.00000  lr: 0.06938806928215069 batch: 23
epochs: 4 train loss: 4.97265625  train f1: 0.25000  lr: 0.06938806928215069 batch: 23 
epochs: 4 val loss: nan  val f1:0.00000  lr: 0.06938806928215069 batch: 23
epochs: 5 train loss: 7.42578125  train f1: 0.16667  lr: 0.06938806928215069 batch: 23 
epochs: 5 val loss: nan  val f1:0.00000  lr: 0.06938806928215069 batch: 23
epochs: 6 train loss: 6.3671875  train f1: 0.25000  lr: 0.06938806928215069 batch: 23 
epochs: 6 val loss: nan  val f1:0.00000  lr: 0.06938806928215069 batch: 23
epochs: 0 train loss: 4.86328125  train f1: 0.00000  lr: 0.0024423216828863377 batch: 23 
epochs: 0 val loss: 5.2421875  val f1:0.00000  lr: 0.0024423216828863377 batch: 23
epochs: 1 train loss: 4.9453125  train f1: 0.00000  lr: 0.0024423216828863377 batch: 23 
epochs: 1 val loss: 5.12109375  val f1:0.00000  lr: 0.0024423216828863377 batch: 23
epochs: 2 train loss: 4.73828125  train f1: 0.00000  lr: 0.0024423216828863377 batch: 23 
epochs: 2 val loss: 5.015625  val f1:0.00000  lr: 0.0024423216828863377 batch: 23
epochs: 3 train loss: 4.54296875  train f1: 0.00000  lr: 0.0024423216828863377 batch: 23 
epochs: 3 val loss: 4.89453125  val f1:0.00000  lr: 0.0024423216828863377 batch: 23
epochs: 4 train loss: 4.33984375  train f1: 0.00000  lr: 0.0024423216828863377 batch: 23 
epochs: 4 val loss: 4.78125  val f1:0.00000  lr: 0.0024423216828863377 batch: 23
epochs: 5 train loss: 4.2265625  train f1: 0.00000  lr: 0.0024423216828863377 batch: 23 
epochs: 5 val loss: 4.66796875  val f1:0.00000  lr: 0.0024423216828863377 batch: 23
epochs: 6 train loss: 3.919921875  train f1: 0.00000  lr: 0.0024423216828863377 batch: 23 
epochs: 6 val loss: 4.59375  val f1:0.00000  lr: 0.0024423216828863377 batch: 23
epochs: 7 train loss: 3.81640625  train f1: 0.16667  lr: 0.0024423216828863377 batch: 23 
epochs: 7 val loss: 4.51953125  val f1:0.00000  lr: 0.0024423216828863377 batch: 23
epochs: 8 train loss: 3.759765625  train f1: 0.16667  lr: 0.0024423216828863377 batch: 23 
epochs: 8 val loss: 4.44140625  val f1:0.00000  lr: 0.0024423216828863377 batch: 23
epochs: 9 train loss: 3.45703125  train f1: 0.40000  lr: 0.0024423216828863377 batch: 23 
epochs: 9 val loss: 4.3828125  val f1:0.00000  lr: 0.0024423216828863377 batch: 23
epochs: 10 train loss: 3.109375  train f1: 0.66667  lr: 0.0024423216828863377 batch: 23 
epochs: 10 val loss: 4.32421875  val f1:0.00000  lr: 0.0024423216828863377 batch: 23
epochs: 11 train loss: 3.326171875  train f1: 0.33333  lr: 0.0024423216828863377 batch: 23 
epochs: 11 val loss: 4.30078125  val f1:0.00000  lr: 0.0024423216828863377 batch: 23
epochs: 12 train loss: 3.099609375  train f1: 1.00000  lr: 0.0024423216828863377 batch: 23 
epochs: 12 val loss: 4.29296875  val f1:0.00000  lr: 0.0024423216828863377 batch: 23
epochs: 13 train loss: 2.966796875  train f1: 1.00000  lr: 0.0024423216828863377 batch: 23 
epochs: 13 val loss: 4.28125  val f1:0.00000  lr: 0.0024423216828863377 batch: 23
epochs: 0 train loss: 4.54296875  train f1: 0.00000  lr: 3.80680092815544e-05 batch: 19 
epochs: 0 val loss: 4.73046875  val f1:0.00000  lr: 3.80680092815544e-05 batch: 19
epochs: 1 train loss: 4.421875  train f1: 0.00000  lr: 3.80680092815544e-05 batch: 19 
epochs: 1 val loss: 4.38671875  val f1:0.00000  lr: 3.80680092815544e-05 batch: 19
epochs: 2 train loss: 1.91796875  train f1: 1.00000  lr: 3.80680092815544e-05 batch: 19 
epochs: 2 val loss: 4.203125  val f1:0.00000  lr: 3.80680092815544e-05 batch: 19
epochs: 3 train loss: 1.2998046875  train f1: 1.00000  lr: 3.80680092815544e-05 batch: 19 
epochs: 3 val loss: 4.1171875  val f1:0.12500  lr: 3.80680092815544e-05 batch: 19
epochs: 4 train loss: 0.4296875  train f1: 1.00000  lr: 3.80680092815544e-05 batch: 19 
epochs: 4 val loss: 3.94921875  val f1:0.12500  lr: 3.80680092815544e-05 batch: 19
epochs: 5 train loss: 0.2177734375  train f1: 1.00000  lr: 3.80680092815544e-05 batch: 19 
epochs: 5 val loss: 3.8671875  val f1:0.11111  lr: 3.80680092815544e-05 batch: 19
epochs: 6 train loss: 0.1661376953125  train f1: 1.00000  lr: 3.80680092815544e-05 batch: 19 
epochs: 6 val loss: 3.78515625  val f1:0.11111  lr: 3.80680092815544e-05 batch: 19
epochs: 7 train loss: 0.257080078125  train f1: 1.00000  lr: 3.80680092815544e-05 batch: 19 
epochs: 7 val loss: 3.6875  val f1:0.11111  lr: 3.80680092815544e-05 batch: 19
epochs: 0 train loss: 4.6015625  train f1: 0.00000  lr: 0.01930078656240832 batch: 16 
epochs: 0 val loss: 5.48828125  val f1:0.00000  lr: 0.01930078656240832 batch: 16
epochs: 1 train loss: 4.703125  train f1: 0.00000  lr: 0.01930078656240832 batch: 16 
epochs: 1 val loss: 5.40625  val f1:0.00000  lr: 0.01930078656240832 batch: 16
epochs: 2 train loss: 3.1171875  train f1: 0.50000  lr: 0.01930078656240832 batch: 16 
epochs: 2 val loss: 5.19921875  val f1:0.00000  lr: 0.01930078656240832 batch: 16
epochs: 3 train loss: 2.27734375  train f1: 1.00000  lr: 0.01930078656240832 batch: 16 
epochs: 3 val loss: 4.71875  val f1:0.12500  lr: 0.01930078656240832 batch: 16
epochs: 4 train loss: 1.3134765625  train f1: 1.00000  lr: 0.01930078656240832 batch: 16 
epochs: 4 val loss: 4.73046875  val f1:0.12500  lr: 0.01930078656240832 batch: 16
epochs: 5 train loss: 0.89208984375  train f1: 1.00000  lr: 0.01930078656240832 batch: 16 
epochs: 5 val loss: 4.56640625  val f1:0.12500  lr: 0.01930078656240832 batch: 16
epochs: 6 train loss: 0.6083984375  train f1: 1.00000  lr: 0.01930078656240832 batch: 16 
epochs: 6 val loss: 4.4921875  val f1:0.12500  lr: 0.01930078656240832 batch: 16
epochs: 7 train loss: 0.3720703125  train f1: 1.00000  lr: 0.01930078656240832 batch: 16 
epochs: 7 val loss: 4.4921875  val f1:0.11111  lr: 0.01930078656240832 batch: 16
epochs: 8 train loss: 0.2071533203125  train f1: 1.00000  lr: 0.01930078656240832 batch: 16 
epochs: 8 val loss: 4.390625  val f1:0.11111  lr: 0.01930078656240832 batch: 16
epochs: 9 train loss: 0.1685791015625  train f1: 1.00000  lr: 0.01930078656240832 batch: 16 
epochs: 9 val loss: 4.296875  val f1:0.11111  lr: 0.01930078656240832 batch: 16
epochs: 10 train loss: 0.1641845703125  train f1: 1.00000  lr: 0.01930078656240832 batch: 16 
epochs: 10 val loss: 4.19140625  val f1:0.11111  lr: 0.01930078656240832 batch: 16
epochs: 0 train loss: 4.4140625  train f1: 0.00000  lr: 0.009897838477004492 batch: 18 
epochs: 0 val loss: 4.8203125  val f1:0.00000  lr: 0.009897838477004492 batch: 18
epochs: 1 train loss: 4.4375  train f1: 0.00000  lr: 0.009897838477004492 batch: 18 
epochs: 1 val loss: 4.9453125  val f1:0.00000  lr: 0.009897838477004492 batch: 18
epochs: 2 train loss: 1.119140625  train f1: 0.50000  lr: 0.009897838477004492 batch: 18 
epochs: 2 val loss: 1112.0  val f1:0.16667  lr: 0.009897838477004492 batch: 18
epochs: 3 train loss: 1.796875  train f1: 0.26667  lr: 0.009897838477004492 batch: 18 
epochs: 3 val loss: 262.75  val f1:0.16667  lr: 0.009897838477004492 batch: 18
epochs: 4 train loss: 1.4462890625  train f1: 0.26667  lr: 0.009897838477004492 batch: 18 
epochs: 4 val loss: 65.4375  val f1:0.16667  lr: 0.009897838477004492 batch: 18
epochs: 5 train loss: 1.482421875  train f1: 0.26667  lr: 0.009897838477004492 batch: 18 
epochs: 5 val loss: 36.21875  val f1:0.16667  lr: 0.009897838477004492 batch: 18
epochs: 6 train loss: 2.49609375  train f1: 0.26667  lr: 0.009897838477004492 batch: 18 
epochs: 6 val loss: 194.5  val f1:0.00000  lr: 0.009897838477004492 batch: 18
epochs: 7 train loss: 0.826171875  train f1: 1.00000  lr: 0.009897838477004492 batch: 18 
epochs: 7 val loss: 2732.0  val f1:0.00000  lr: 0.009897838477004492 batch: 18
epochs: 8 train loss: 2.060546875  train f1: 0.77778  lr: 0.009897838477004492 batch: 18 
epochs: 8 val loss: 923.5  val f1:0.00000  lr: 0.009897838477004492 batch: 18
epochs: 9 train loss: 0.85400390625  train f1: 0.77778  lr: 0.009897838477004492 batch: 18 
epochs: 9 val loss: 427.5  val f1:0.00000  lr: 0.009897838477004492 batch: 18
epochs: 10 train loss: 0.471923828125  train f1: 0.77778  lr: 0.009897838477004492 batch: 18 
epochs: 10 val loss: 22384.0  val f1:0.00000  lr: 0.009897838477004492 batch: 18
epochs: 11 train loss: 1.0654296875  train f1: 0.66667  lr: 0.009897838477004492 batch: 18 
epochs: 11 val loss: nan  val f1:0.00000  lr: 0.009897838477004492 batch: 18
epochs: 0 train loss: 4.203125  train f1: 0.00000  lr: 2.9743875107771688e-05 batch: 30 
epochs: 0 val loss: 4.859375  val f1:0.12500  lr: 2.9743875107771688e-05 batch: 30
epochs: 1 train loss: 4.2890625  train f1: 0.00000  lr: 2.9743875107771688e-05 batch: 30 
epochs: 1 val loss: 4.7265625  val f1:0.12500  lr: 2.9743875107771688e-05 batch: 30
epochs: 2 train loss: 4.109375  train f1: 0.16667  lr: 2.9743875107771688e-05 batch: 30 
epochs: 2 val loss: 4.5859375  val f1:0.12500  lr: 2.9743875107771688e-05 batch: 30
epochs: 3 train loss: 3.86328125  train f1: 0.00000  lr: 2.9743875107771688e-05 batch: 30 
epochs: 3 val loss: 4.44140625  val f1:0.11111  lr: 2.9743875107771688e-05 batch: 30
epochs: 4 train loss: 3.62109375  train f1: 0.33333  lr: 2.9743875107771688e-05 batch: 30 
epochs: 4 val loss: 4.3203125  val f1:0.11111  lr: 2.9743875107771688e-05 batch: 30
epochs: 5 train loss: 3.345703125  train f1: 0.33333  lr: 2.9743875107771688e-05 batch: 30 
epochs: 5 val loss: 4.23046875  val f1:0.12500  lr: 2.9743875107771688e-05 batch: 30
epochs: 6 train loss: 3.287109375  train f1: 0.33333  lr: 2.9743875107771688e-05 batch: 30 
epochs: 6 val loss: 4.15234375  val f1:0.12500  lr: 2.9743875107771688e-05 batch: 30
epochs: 7 train loss: 2.845703125  train f1: 0.66667  lr: 2.9743875107771688e-05 batch: 30 
epochs: 7 val loss: 4.0625  val f1:0.12500  lr: 2.9743875107771688e-05 batch: 30
epochs: 8 train loss: 2.73046875  train f1: 1.00000  lr: 2.9743875107771688e-05 batch: 30 
epochs: 8 val loss: 3.982421875  val f1:0.12500  lr: 2.9743875107771688e-05 batch: 30
epochs: 9 train loss: 2.658203125  train f1: 0.66667  lr: 2.9743875107771688e-05 batch: 30 
epochs: 9 val loss: 3.927734375  val f1:0.12500  lr: 2.9743875107771688e-05 batch: 30
epochs: 10 train loss: 2.62890625  train f1: 1.00000  lr: 2.9743875107771688e-05 batch: 30 
epochs: 10 val loss: 3.880859375  val f1:0.12500  lr: 2.9743875107771688e-05 batch: 30
epochs: 11 train loss: 2.328125  train f1: 1.00000  lr: 2.9743875107771688e-05 batch: 30 
epochs: 11 val loss: 3.900390625  val f1:0.12500  lr: 2.9743875107771688e-05 batch: 30
epochs: 12 train loss: 1.927734375  train f1: 1.00000  lr: 2.9743875107771688e-05 batch: 30 
epochs: 12 val loss: 3.91015625  val f1:0.12500  lr: 2.9743875107771688e-05 batch: 30
epochs: 0 train loss: 4.78125  train f1: 0.00000  lr: 1.9586363118949208e-05 batch: 19 
epochs: 0 val loss: 5.1484375  val f1:0.00000  lr: 1.9586363118949208e-05 batch: 19
epochs: 1 train loss: 4.79296875  train f1: 0.00000  lr: 1.9586363118949208e-05 batch: 19 
epochs: 1 val loss: 5.1171875  val f1:0.00000  lr: 1.9586363118949208e-05 batch: 19
epochs: 2 train loss: 4.875  train f1: 0.00000  lr: 1.9586363118949208e-05 batch: 19 
epochs: 2 val loss: 5.09375  val f1:0.00000  lr: 1.9586363118949208e-05 batch: 19
epochs: 3 train loss: 4.765625  train f1: 0.00000  lr: 1.9586363118949208e-05 batch: 19 
epochs: 3 val loss: 5.046875  val f1:0.00000  lr: 1.9586363118949208e-05 batch: 19
epochs: 4 train loss: 4.7109375  train f1: 0.00000  lr: 1.9586363118949208e-05 batch: 19 
epochs: 4 val loss: 5.0  val f1:0.00000  lr: 1.9586363118949208e-05 batch: 19
epochs: 0 train loss: 4.484375  train f1: 0.00000  lr: 0.037831422686202924 batch: 22 
epochs: 0 val loss: 4.765625  val f1:0.00000  lr: 0.037831422686202924 batch: 22
epochs: 1 train loss: 4.671875  train f1: 0.00000  lr: 0.037831422686202924 batch: 22 
epochs: 1 val loss: nan  val f1:0.00000  lr: 0.037831422686202924 batch: 22
epochs: 2 train loss: 8.8359375  train f1: 0.10000  lr: 0.037831422686202924 batch: 22 
epochs: 2 val loss: nan  val f1:0.00000  lr: 0.037831422686202924 batch: 22
epochs: 3 train loss: 6.4609375  train f1: 0.10000  lr: 0.037831422686202924 batch: 22 
epochs: 3 val loss: inf  val f1:0.06667  lr: 0.037831422686202924 batch: 22
epochs: 4 train loss: 6.578125  train f1: 0.20000  lr: 0.037831422686202924 batch: 22 
epochs: 4 val loss: 8776.0  val f1:0.06667  lr: 0.037831422686202924 batch: 22
epochs: 5 train loss: 10.0546875  train f1: 0.00000  lr: 0.037831422686202924 batch: 22 
epochs: 5 val loss: 3008.0  val f1:0.06667  lr: 0.037831422686202924 batch: 22
epochs: 6 train loss: 6.453125  train f1: 0.10000  lr: 0.037831422686202924 batch: 22 
epochs: 6 val loss: 1398.0  val f1:0.06667  lr: 0.037831422686202924 batch: 22
epochs: 7 train loss: 7.4375  train f1: 0.10000  lr: 0.037831422686202924 batch: 22 
epochs: 7 val loss: nan  val f1:0.00000  lr: 0.037831422686202924 batch: 22
epochs: 8 train loss: 11.3515625  train f1: 0.25000  lr: 0.037831422686202924 batch: 22 
epochs: 8 val loss: nan  val f1:0.00000  lr: 0.037831422686202924 batch: 22
epochs: 9 train loss: 1.1142578125  train f1: 0.26667  lr: 0.037831422686202924 batch: 22 
epochs: 9 val loss: nan  val f1:0.00000  lr: 0.037831422686202924 batch: 22
epochs: 10 train loss: 11.078125  train f1: 0.00000  lr: 0.037831422686202924 batch: 22 
epochs: 10 val loss: nan  val f1:0.00000  lr: 0.037831422686202924 batch: 22
epochs: 11 train loss: 2.267578125  train f1: 0.55556  lr: 0.037831422686202924 batch: 22 
epochs: 11 val loss: nan  val f1:0.00000  lr: 0.037831422686202924 batch: 22
epochs: 12 train loss: 5.4609375  train f1: 0.20000  lr: 0.037831422686202924 batch: 22 
epochs: 12 val loss: nan  val f1:0.00000  lr: 0.037831422686202924 batch: 22
epochs: 13 train loss: 0.1488037109375  train f1: 1.00000  lr: 0.037831422686202924 batch: 22 
epochs: 13 val loss: nan  val f1:0.00000  lr: 0.037831422686202924 batch: 22
epochs: 14 train loss: 0.84326171875  train f1: 0.55556  lr: 0.037831422686202924 batch: 22 
epochs: 14 val loss: nan  val f1:0.00000  lr: 0.037831422686202924 batch: 22
epochs: 15 train loss: 0.09814453125  train f1: 1.00000  lr: 0.037831422686202924 batch: 22 
epochs: 15 val loss: nan  val f1:0.00000  lr: 0.037831422686202924 batch: 22
epochs: 16 train loss: 0.00012636184692382812  train f1: 1.00000  lr: 0.037831422686202924 batch: 22 
epochs: 16 val loss: nan  val f1:0.00000  lr: 0.037831422686202924 batch: 22
epochs: 17 train loss: 1.138671875  train f1: 0.77778  lr: 0.037831422686202924 batch: 22 
epochs: 17 val loss: nan  val f1:0.00000  lr: 0.037831422686202924 batch: 22
epochs: 0 train loss: 5.0703125  train f1: 0.00000  lr: 0.00013728832291769324 batch: 14 
epochs: 0 val loss: 4.69921875  val f1:0.00000  lr: 0.00013728832291769324 batch: 14
epochs: 1 train loss: 5.1484375  train f1: 0.00000  lr: 0.00013728832291769324 batch: 14 
epochs: 1 val loss: 4.640625  val f1:0.00000  lr: 0.00013728832291769324 batch: 14
epochs: 2 train loss: 4.95703125  train f1: 0.00000  lr: 0.00013728832291769324 batch: 14 
epochs: 2 val loss: 4.625  val f1:0.00000  lr: 0.00013728832291769324 batch: 14
epochs: 0 train loss: 4.34375  train f1: 0.00000  lr: 1.9223508374523378e-05 batch: 15 
epochs: 0 val loss: 4.796875  val f1:0.00000  lr: 1.9223508374523378e-05 batch: 15
epochs: 0 train loss: 4.78125  train f1: 0.00000  lr: 3.375465143633981e-05 batch: 13 
epochs: 0 val loss: 5.59375  val f1:0.00000  lr: 3.375465143633981e-05 batch: 13
epochs: 0 train loss: 5.0078125  train f1: 0.00000  lr: 1.4970900003781417e-05 batch: 20 
epochs: 0 val loss: 5.30859375  val f1:0.00000  lr: 1.4970900003781417e-05 batch: 20
epochs: 0 train loss: 4.47265625  train f1: 0.00000  lr: 0.03928946693034651 batch: 18 
epochs: 0 val loss: 4.16015625  val f1:0.11111  lr: 0.03928946693034651 batch: 18
epochs: 0 train loss: 5.2578125  train f1: 0.00000  lr: 1.4185624234324002e-05 batch: 21 
epochs: 0 val loss: 5.171875  val f1:0.00000  lr: 1.4185624234324002e-05 batch: 21
epochs: 0 train loss: 5.0  train f1: 0.00000  lr: 0.027868422953792008 batch: 29 
epochs: 0 val loss: 4.56640625  val f1:0.00000  lr: 0.027868422953792008 batch: 29
epochs: 0 train loss: 4.34375  train f1: 0.00000  lr: 0.008058108903545342 batch: 21 
epochs: 0 val loss: 4.796875  val f1:0.00000  lr: 0.008058108903545342 batch: 21
epochs: 0 train loss: 4.78125  train f1: 0.00000  lr: 0.01460286701346049 batch: 17 
epochs: 0 val loss: 5.59375  val f1:0.00000  lr: 0.01460286701346049 batch: 17
epochs: 0 train loss: 5.0078125  train f1: 0.00000  lr: 0.00012126055637907843 batch: 18 
epochs: 0 val loss: 5.30859375  val f1:0.00000  lr: 0.00012126055637907843 batch: 18
epochs: 0 train loss: 4.47265625  train f1: 0.00000  lr: 0.00026811091660750594 batch: 13 
epochs: 0 val loss: 4.16015625  val f1:0.11111  lr: 0.00026811091660750594 batch: 13
epochs: 0 train loss: 5.2578125  train f1: 0.00000  lr: 0.00014276097051537485 batch: 26 
epochs: 0 val loss: 5.171875  val f1:0.00000  lr: 0.00014276097051537485 batch: 26
epochs: 0 train loss: 5.0  train f1: 0.00000  lr: 0.0001119160410935569 batch: 32 
epochs: 0 val loss: 4.56640625  val f1:0.00000  lr: 0.0001119160410935569 batch: 32
epochs: 0 train loss: 4.8203125  train f1: 0.00000  lr: 0.00014966130609985067 batch: 28 
epochs: 0 val loss: 4.74609375  val f1:0.00000  lr: 0.00014966130609985067 batch: 28
epochs: 0 train loss: 4.48828125  train f1: 0.00000  lr: 0.0004286213699740803 batch: 13 
epochs: 0 val loss: 5.765625  val f1:0.00000  lr: 0.0004286213699740803 batch: 13
epochs: 0 train loss: 4.375  train f1: 0.00000  lr: 0.06162910688555971 batch: 30 
epochs: 0 val loss: 4.22265625  val f1:0.00000  lr: 0.06162910688555971 batch: 30
epochs: 0 train loss: 4.5859375  train f1: 0.00000  lr: 0.09817954207505156 batch: 31 
epochs: 0 val loss: 5.14453125  val f1:0.00000  lr: 0.09817954207505156 batch: 31
epochs: 0 train loss: 4.65234375  train f1: 0.00000  lr: 2.189544971660596e-05 batch: 10 
epochs: 0 val loss: 5.5390625  val f1:0.00000  lr: 2.189544971660596e-05 batch: 10
epochs: 0 train loss: 4.9609375  train f1: 0.00000  lr: 0.003107441216212963 batch: 22 
epochs: 0 val loss: 5.37109375  val f1:0.00000  lr: 0.003107441216212963 batch: 22
epochs: 0 train loss: 4.6484375  train f1: 0.00000  lr: 0.00223247932127077 batch: 23 
epochs: 0 val loss: 5.44921875  val f1:0.00000  lr: 0.00223247932127077 batch: 23
epochs: 0 train loss: 4.21484375  train f1: 0.00000  lr: 0.011060367127703122 batch: 14 
epochs: 0 val loss: 4.890625  val f1:0.00000  lr: 0.011060367127703122 batch: 14
epochs: 0 train loss: 4.125  train f1: 0.00000  lr: 0.0009734527354599777 batch: 10 
epochs: 0 val loss: 4.5625  val f1:0.00000  lr: 0.0009734527354599777 batch: 10
epochs: 0 train loss: 5.2890625  train f1: 0.00000  lr: 1.1863084721952298e-05 batch: 19 
epochs: 0 val loss: 4.91796875  val f1:0.00000  lr: 1.1863084721952298e-05 batch: 19
epochs: 0 train loss: 3.921875  train f1: 0.00000  lr: 0.013963380931533815 batch: 15 
epochs: 0 val loss: 5.796875  val f1:0.00000  lr: 0.013963380931533815 batch: 15
epochs: 0 train loss: 4.75390625  train f1: 0.00000  lr: 0.003969311935684592 batch: 8 
epochs: 0 val loss: 5.6640625  val f1:0.00000  lr: 0.003969311935684592 batch: 8
epochs: 0 train loss: 4.94921875  train f1: 0.00000  lr: 0.0006296564666939994 batch: 24 
epochs: 0 val loss: 4.70703125  val f1:0.00000  lr: 0.0006296564666939994 batch: 24
epochs: 0 train loss: 4.8125  train f1: 0.00000  lr: 4.52930241664729e-05 batch: 21 
epochs: 0 val loss: 4.9921875  val f1:0.00000  lr: 4.52930241664729e-05 batch: 21
epochs: 0 train loss: 4.4765625  train f1: 0.00000  lr: 0.028340111005469668 batch: 16 
epochs: 0 val loss: 5.4140625  val f1:0.00000  lr: 0.028340111005469668 batch: 16
epochs: 0 train loss: 4.9609375  train f1: 0.00000  lr: 0.0005526083664890837 batch: 25 
epochs: 0 val loss: 4.30859375  val f1:0.00000  lr: 0.0005526083664890837 batch: 25
epochs: 0 train loss: 4.08203125  train f1: 0.00000  lr: 3.889083835340568e-05 batch: 21 
epochs: 0 val loss: 3.7265625  val f1:0.12500  lr: 3.889083835340568e-05 batch: 21
epochs: 0 train loss: 4.375  train f1: 0.00000  lr: 4.6501646821985486e-05 batch: 16 
epochs: 0 val loss: 4.58203125  val f1:0.00000  lr: 4.6501646821985486e-05 batch: 16
epochs: 0 train loss: 4.703125  train f1: 0.00000  lr: 0.00027831347875332835 batch: 20 
epochs: 0 val loss: 4.66796875  val f1:0.00000  lr: 0.00027831347875332835 batch: 20
epochs: 0 train loss: 4.265625  train f1: 0.00000  lr: 0.0018311093964660829 batch: 12 
epochs: 0 val loss: 4.12890625  val f1:0.00000  lr: 0.0018311093964660829 batch: 12
epochs: 0 train loss: 4.27734375  train f1: 0.00000  lr: 3.4219794190365286e-05 batch: 27 
epochs: 0 val loss: 4.43359375  val f1:0.00000  lr: 3.4219794190365286e-05 batch: 27
epochs: 0 train loss: 4.90625  train f1: 0.00000  lr: 0.006405428178104098 batch: 20 
epochs: 0 val loss: 4.80078125  val f1:0.00000  lr: 0.006405428178104098 batch: 20
epochs: 0 train loss: 4.640625  train f1: 0.00000  lr: 1.067299222389805e-05 batch: 23 
epochs: 0 val loss: 5.79296875  val f1:0.00000  lr: 1.067299222389805e-05 batch: 23
epochs: 0 train loss: 4.68359375  train f1: 0.00000  lr: 7.528966022220601e-05 batch: 18 
epochs: 0 val loss: 4.7265625  val f1:0.00000  lr: 7.528966022220601e-05 batch: 18
epochs: 0 train loss: 4.8125  train f1: 0.00000  lr: 0.0002738679450579267 batch: 17 
epochs: 0 val loss: 5.04296875  val f1:0.00000  lr: 0.0002738679450579267 batch: 17
epochs: 0 train loss: 3.984375  train f1: 0.11111  lr: 1.4807051954294561e-05 batch: 22 
epochs: 0 val loss: 5.28125  val f1:0.00000  lr: 1.4807051954294561e-05 batch: 22
epochs: 0 train loss: 4.359375  train f1: 0.00000  lr: 7.581762957314342e-05 batch: 18 
epochs: 0 val loss: 4.56640625  val f1:0.00000  lr: 7.581762957314342e-05 batch: 18
epochs: 0 train loss: 4.5859375  train f1: 0.00000  lr: 0.0002508760231681438 batch: 17 
epochs: 0 val loss: 4.4140625  val f1:0.00000  lr: 0.0002508760231681438 batch: 17
epochs: 0 train loss: 5.26953125  train f1: 0.00000  lr: 3.4253874186068354e-05 batch: 20 
epochs: 0 val loss: 4.6796875  val f1:0.00000  lr: 3.4253874186068354e-05 batch: 20
epochs: 0 train loss: 4.25  train f1: 0.00000  lr: 0.03346417910505955 batch: 15 
epochs: 0 val loss: 4.43359375  val f1:0.00000  lr: 0.03346417910505955 batch: 15
epochs: 0 train loss: 4.45703125  train f1: 0.00000  lr: 0.020751327385817676 batch: 12 
epochs: 0 val loss: 5.17578125  val f1:0.00000  lr: 0.020751327385817676 batch: 12
epochs: 0 train loss: 4.66015625  train f1: 0.00000  lr: 0.0007650971226084004 batch: 25 
epochs: 0 val loss: 4.84375  val f1:0.00000  lr: 0.0007650971226084004 batch: 25
epochs: 0 train loss: 4.0546875  train f1: 0.00000  lr: 0.00012087409879421949 batch: 29 
epochs: 0 val loss: 4.375  val f1:0.00000  lr: 0.00012087409879421949 batch: 29
epochs: 0 train loss: 5.24609375  train f1: 0.00000  lr: 5.438950536236265e-05 batch: 13 
epochs: 0 val loss: 5.453125  val f1:0.00000  lr: 5.438950536236265e-05 batch: 13
epochs: 0 train loss: 4.3515625  train f1: 0.00000  lr: 0.0014410904924586202 batch: 16 
epochs: 0 val loss: 4.47265625  val f1:0.00000  lr: 0.0014410904924586202 batch: 16
epochs: 0 train loss: 4.671875  train f1: 0.00000  lr: 0.0001919079414539218 batch: 19 
epochs: 0 val loss: 4.03515625  val f1:0.08333  lr: 0.0001919079414539218 batch: 19
epochs: 0 train loss: 4.2890625  train f1: 0.00000  lr: 0.00024074234690804939 batch: 19 
epochs: 0 val loss: 4.859375  val f1:0.00000  lr: 0.00024074234690804939 batch: 19
epochs: 0 train loss: 4.859375  train f1: 0.00000  lr: 0.00039363363760907557 batch: 21 
epochs: 0 val loss: 4.38671875  val f1:0.00000  lr: 0.00039363363760907557 batch: 21
epochs: 0 train loss: 4.72265625  train f1: 0.00000  lr: 0.0014434423751487826 batch: 10 
epochs: 0 val loss: 5.3125  val f1:0.00000  lr: 0.0014434423751487826 batch: 10
epochs: 0 train loss: 5.12109375  train f1: 0.00000  lr: 2.5962428401118836e-05 batch: 27 
epochs: 0 val loss: 4.9453125  val f1:0.00000  lr: 2.5962428401118836e-05 batch: 27
epochs: 0 train loss: 4.890625  train f1: 0.00000  lr: 0.00018397293486151325 batch: 12 
epochs: 0 val loss: 4.8125  val f1:0.00000  lr: 0.00018397293486151325 batch: 12
epochs: 0 train loss: 4.99609375  train f1: 0.00000  lr: 0.0041617552339179185 batch: 12 
epochs: 0 val loss: 5.35546875  val f1:0.00000  lr: 0.0041617552339179185 batch: 12
epochs: 0 train loss: 4.83984375  train f1: 0.00000  lr: 8.913168593425959e-05 batch: 27 
epochs: 0 val loss: 4.87890625  val f1:0.00000  lr: 8.913168593425959e-05 batch: 27
epochs: 0 train loss: 4.640625  train f1: 0.00000  lr: 0.011577575577175045 batch: 22 
epochs: 0 val loss: 4.5625  val f1:0.00000  lr: 0.011577575577175045 batch: 22
epochs: 0 train loss: 4.62109375  train f1: 0.00000  lr: 1.0410448728038326e-05 batch: 23 
epochs: 0 val loss: 5.4375  val f1:0.00000  lr: 1.0410448728038326e-05 batch: 23
epochs: 0 train loss: 5.12109375  train f1: 0.00000  lr: 1.8804398844989535e-05 batch: 18 
epochs: 0 val loss: 4.76171875  val f1:0.00000  lr: 1.8804398844989535e-05 batch: 18
epochs: 0 train loss: 4.6953125  train f1: 0.00000  lr: 0.00017854246781886444 batch: 19 
epochs: 0 val loss: 5.2109375  val f1:0.00000  lr: 0.00017854246781886444 batch: 19
epochs: 0 train loss: 4.625  train f1: 0.00000  lr: 7.430220186488826e-05 batch: 18 
epochs: 0 val loss: 5.09765625  val f1:0.00000  lr: 7.430220186488826e-05 batch: 18
epochs: 0 train loss: 4.7734375  train f1: 0.00000  lr: 1.574741147624744e-05 batch: 22 
epochs: 0 val loss: 5.25  val f1:0.00000  lr: 1.574741147624744e-05 batch: 22
epochs: 0 train loss: 5.1171875  train f1: 0.00000  lr: 0.00040590662769148694 batch: 17 
epochs: 0 val loss: 5.19921875  val f1:0.00000  lr: 0.00040590662769148694 batch: 17
epochs: 0 train loss: 4.90625  train f1: 0.00000  lr: 2.8192248222174793e-05 batch: 21 
epochs: 0 val loss: 4.5546875  val f1:0.00000  lr: 2.8192248222174793e-05 batch: 21
epochs: 0 train loss: 4.46484375  train f1: 0.00000  lr: 0.00012158767563131961 batch: 24 
epochs: 0 val loss: 4.41796875  val f1:0.00000  lr: 0.00012158767563131961 batch: 24
epochs: 0 train loss: 4.546875  train f1: 0.00000  lr: 6.0060637732355644e-05 batch: 15 
epochs: 0 val loss: 5.24609375  val f1:0.00000  lr: 6.0060637732355644e-05 batch: 15
epochs: 0 train loss: 5.25  train f1: 0.00000  lr: 0.00016415753863414562 batch: 14 
epochs: 0 val loss: 4.78125  val f1:0.00000  lr: 0.00016415753863414562 batch: 14
epochs: 0 train loss: 4.296875  train f1: 0.00000  lr: 3.775128321345315e-05 batch: 20 
epochs: 0 val loss: 5.41796875  val f1:0.00000  lr: 3.775128321345315e-05 batch: 20
epochs: 0 train loss: 4.953125  train f1: 0.00000  lr: 0.03834285790593024 batch: 15 
epochs: 0 val loss: 5.11328125  val f1:0.00000  lr: 0.03834285790593024 batch: 15
epochs: 0 train loss: 4.796875  train f1: 0.00000  lr: 0.06554508273767844 batch: 11 
epochs: 0 val loss: 4.94921875  val f1:0.00000  lr: 0.06554508273767844 batch: 11
epochs: 0 train loss: 4.53125  train f1: 0.00000  lr: 0.03003241089516848 batch: 8 
epochs: 0 val loss: 4.7578125  val f1:0.00000  lr: 0.03003241089516848 batch: 8
epochs: 0 train loss: 3.970703125  train f1: 0.20000  lr: 0.01937020726163284 batch: 14 
epochs: 0 val loss: 5.60546875  val f1:0.00000  lr: 0.01937020726163284 batch: 14
epochs: 0 train loss: 4.109375  train f1: 0.16667  lr: 0.007736222616466785 batch: 11 
epochs: 0 val loss: 4.95703125  val f1:0.00000  lr: 0.007736222616466785 batch: 11
epochs: 0 train loss: 4.7421875  train f1: 0.00000  lr: 0.0010608348664287693 batch: 29 
epochs: 0 val loss: 4.9921875  val f1:0.00000  lr: 0.0010608348664287693 batch: 29
epochs: 0 train loss: 4.51171875  train f1: 0.00000  lr: 2.411989235707415e-05 batch: 9 
epochs: 0 val loss: 5.4453125  val f1:0.00000  lr: 2.411989235707415e-05 batch: 9
epochs: 0 train loss: 4.640625  train f1: 0.00000  lr: 0.002664192065561615 batch: 32 
epochs: 0 val loss: 4.1796875  val f1:0.07143  lr: 0.002664192065561615 batch: 32
epochs: 0 train loss: 5.10546875  train f1: 0.00000  lr: 0.0028249949366939515 batch: 32 
epochs: 0 val loss: 4.48828125  val f1:0.00000  lr: 0.0028249949366939515 batch: 32
epochs: 0 train loss: 4.32421875  train f1: 0.00000  lr: 0.005393731873453234 batch: 26 
epochs: 0 val loss: 4.9453125  val f1:0.00000  lr: 0.005393731873453234 batch: 26
epochs: 0 train loss: 4.3828125  train f1: 0.00000  lr: 0.010381685717276403 batch: 31 
epochs: 0 val loss: 5.39453125  val f1:0.00000  lr: 0.010381685717276403 batch: 31
epochs: 0 train loss: 4.98828125  train f1: 0.00000  lr: 0.00010136911167910074 batch: 23 
epochs: 0 val loss: 5.5703125  val f1:0.00000  lr: 0.00010136911167910074 batch: 23
epochs: 0 train loss: 5.0703125  train f1: 0.00000  lr: 1.1163356761890918e-05 batch: 22 
epochs: 0 val loss: 5.38671875  val f1:0.00000  lr: 1.1163356761890918e-05 batch: 22
epochs: 0 train loss: 5.28515625  train f1: 0.00000  lr: 1.728651176482588e-05 batch: 19 
epochs: 0 val loss: 4.65234375  val f1:0.00000  lr: 1.728651176482588e-05 batch: 19
epochs: 0 train loss: 4.91015625  train f1: 0.00000  lr: 1.832429511623612e-05 batch: 19 
epochs: 0 val loss: 4.43359375  val f1:0.00000  lr: 1.832429511623612e-05 batch: 19
epochs: 0 train loss: 4.20703125  train f1: 0.00000  lr: 7.108832051930109e-05 batch: 18 
epochs: 0 val loss: 4.09765625  val f1:0.12500  lr: 7.108832051930109e-05 batch: 18
epochs: 0 train loss: 4.25390625  train f1: 0.00000  lr: 0.00019743600469403387 batch: 18 
epochs: 0 val loss: 4.6484375  val f1:0.00000  lr: 0.00019743600469403387 batch: 18
epochs: 0 train loss: 4.4609375  train f1: 0.00000  lr: 0.00037190451961127853 batch: 17 
epochs: 0 val loss: 5.19140625  val f1:0.00000  lr: 0.00037190451961127853 batch: 17
epochs: 0 train loss: 4.8125  train f1: 0.00000  lr: 0.0017490099136237627 batch: 9 
epochs: 0 val loss: 5.27734375  val f1:0.00000  lr: 0.0017490099136237627 batch: 9
epochs: 0 train loss: 4.84765625  train f1: 0.00000  lr: 0.0001387985041514345 batch: 21 
epochs: 0 val loss: 4.9765625  val f1:0.00000  lr: 0.0001387985041514345 batch: 21
epochs: 0 train loss: 4.05859375  train f1: 0.00000  lr: 2.947143795623248e-05 batch: 24 
epochs: 0 val loss: 5.32421875  val f1:0.00000  lr: 2.947143795623248e-05 batch: 24
epochs: 0 train loss: 4.4609375  train f1: 0.00000  lr: 5.99494489219928e-05 batch: 25 
epochs: 0 val loss: 4.50390625  val f1:0.11111  lr: 5.99494489219928e-05 batch: 25
epochs: 0 train loss: 4.4140625  train f1: 0.00000  lr: 3.9134576324249874e-05 batch: 30 
epochs: 0 val loss: 5.28125  val f1:0.00000  lr: 3.9134576324249874e-05 batch: 30
epochs: 0 train loss: 4.703125  train f1: 0.00000  lr: 0.0039732270485461595 batch: 13 
epochs: 0 val loss: 5.10546875  val f1:0.00000  lr: 0.0039732270485461595 batch: 13
epochs: 0 train loss: 4.87890625  train f1: 0.00000  lr: 6.327044378902618e-05 batch: 20 
epochs: 0 val loss: 5.046875  val f1:0.00000  lr: 6.327044378902618e-05 batch: 20
epochs: 0 train loss: 4.1640625  train f1: 0.00000  lr: 0.043700104438792214 batch: 21 
epochs: 0 val loss: 5.2265625  val f1:0.00000  lr: 0.043700104438792214 batch: 21
epochs: 0 train loss: 5.4765625  train f1: 0.00000  lr: 0.09289954429940468 batch: 11 
epochs: 0 val loss: 4.6484375  val f1:0.00000  lr: 0.09289954429940468 batch: 11
epochs: 0 train loss: 4.66796875  train f1: 0.00000  lr: 0.05884165525030855 batch: 16 
epochs: 0 val loss: 5.16015625  val f1:0.00000  lr: 0.05884165525030855 batch: 16
epochs: 0 train loss: 4.09765625  train f1: 0.00000  lr: 0.03132225320582479 batch: 8 
epochs: 0 val loss: 4.67578125  val f1:0.00000  lr: 0.03132225320582479 batch: 8
epochs: 0 train loss: 4.87890625  train f1: 0.00000  lr: 0.06296750896702927 batch: 14 
epochs: 0 val loss: 5.29296875  val f1:0.00000  lr: 0.06296750896702927 batch: 14
epochs: 0 train loss: 4.38671875  train f1: 0.00000  lr: 0.015970006155284183 batch: 11 
epochs: 0 val loss: 4.9609375  val f1:0.00000  lr: 0.015970006155284183 batch: 11
epochs: 0 train loss: 4.546875  train f1: 0.00000  lr: 0.007516785831989137 batch: 11 
epochs: 0 val loss: 5.14453125  val f1:0.00000  lr: 0.007516785831989137 batch: 11
epochs: 0 train loss: 3.90234375  train f1: 0.20000  lr: 0.021073318807784375 batch: 29 
epochs: 0 val loss: 4.1875  val f1:0.00000  lr: 0.021073318807784375 batch: 29
epochs: 0 train loss: 3.85546875  train f1: 0.20000  lr: 0.0008468164886185026 batch: 28 
epochs: 0 val loss: 4.32421875  val f1:0.12500  lr: 0.0008468164886185026 batch: 28
epochs: 0 train loss: 4.734375  train f1: 0.00000  lr: 2.3633144811693343e-05 batch: 28 
epochs: 0 val loss: 4.96484375  val f1:0.00000  lr: 2.3633144811693343e-05 batch: 28
epochs: 0 train loss: 4.7890625  train f1: 0.00000  lr: 0.0013058697808030074 batch: 30 
epochs: 0 val loss: 5.59375  val f1:0.00000  lr: 0.0013058697808030074 batch: 30
epochs: 0 train loss: 4.01171875  train f1: 0.00000  lr: 0.0008522807334095304 batch: 31 
epochs: 0 val loss: 5.76171875  val f1:0.00000  lr: 0.0008522807334095304 batch: 31
epochs: 0 train loss: 4.859375  train f1: 0.00000  lr: 0.002803564937504347 batch: 31 
epochs: 0 val loss: 5.26171875  val f1:0.00000  lr: 0.002803564937504347 batch: 31
epochs: 0 train loss: 4.3125  train f1: 0.00000  lr: 0.0051659077887170725 batch: 26 
epochs: 0 val loss: 4.62109375  val f1:0.00000  lr: 0.0051659077887170725 batch: 26
